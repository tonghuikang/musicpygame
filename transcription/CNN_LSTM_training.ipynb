{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras import callbacks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_row(csv_row):  # same as generate_database.ipynb\n",
    "\n",
    "    csv_row_array = csv_row.split(\",\")\n",
    "\n",
    "    filename = csv_row_array[0]\n",
    "    time_array = [float(t) for t in csv_row_array[1::2]]\n",
    "    note_array = [int(x) for x in csv_row_array[2::2]]\n",
    "\n",
    "    assert len(time_array) == len(note_array)\n",
    "\n",
    "#     print(time_array)\n",
    "#     print(note_array)\n",
    "\n",
    "    return filename, time_array, note_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv(csv_file):\n",
    "    \n",
    "    with open(csv_file, 'r') as csvfile:\n",
    "        csv_rows = csvfile.read().split(\"\\n\")\n",
    "\n",
    "    filenames = []\n",
    "    time_arrays = []\n",
    "    note_arrays = []\n",
    "    \n",
    "    for csv_row in csv_rows:\n",
    "        filename, time_array, note_array = parse_row(csv_row)\n",
    "        \n",
    "        filenames.append(filename)\n",
    "        time_arrays.append(time_array)\n",
    "        note_arrays.append(note_array)\n",
    "    \n",
    "    return filenames,time_arrays,note_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames,time_arrays,note_arrays = parse_csv('notes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[19, 8, 20, 13]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[13, 9, 8]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[2, 18, 0]\n",
      "[]\n",
      "[13, 12, 20]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[1, 5]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[7, 12, 23, 19]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[5, 7, 9, 19]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[16, 5, 23]\n",
      "[]\n",
      "[17, 2, 9, 7]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[21, 8]\n",
      "[]\n",
      "[]\n",
      "(5, 108, 108)\n",
      "(5, 10, 24)\n"
     ]
    }
   ],
   "source": [
    "filenames_to_train = filenames # list of filenames, select at random\n",
    "\n",
    "spec_array_to_train = np.zeros((0,108,108))\n",
    "notes_arrays_to_train = np.zeros((0,10,24))\n",
    "\n",
    "for i,filename in enumerate(filenames_to_train):\n",
    "    start_pt = np.random.randint(290,3446-200)\n",
    "    \n",
    "    # this refers to detection if it falls within this range\n",
    "    time_end = 2*(start_pt+54-27)*(2**7)/44100\n",
    "    interval = 2*(27)*(2**7)/44100\n",
    "    # 2 as bpm is 120 default, 2**7 is hop length, # 44100 is sampling freq\n",
    "    \n",
    "    # This is how it overlaps\n",
    "    # ==== \n",
    "    #   ====\n",
    "    #     ====\n",
    "    \n",
    "    notes_array_to_train = np.zeros((0,24))  # array without s\n",
    "    \n",
    "    for _ in range(10):  # 10 LSTM layers, note that order is reversed\n",
    "        time_start = time_end - interval\n",
    "        \n",
    "        notes = []\n",
    "        notes_array = [0]*24\n",
    "\n",
    "        for j,t in enumerate(time_arrays[i]):\n",
    "            if t > time_end:\n",
    "                break\n",
    "            if t > time_start:\n",
    "                notes.append(note_arrays[i][j])\n",
    "\n",
    "#         print(notes)\n",
    "        for note in notes:\n",
    "            notes_array[note] = 1\n",
    "    \n",
    "        notes_array_to_train = np.append(notes_array_to_train, \n",
    "                                        [notes_array], axis=0)\n",
    "        \n",
    "        time_end = time_start\n",
    "    \n",
    "#     print(np.shape(notes_array_to_train))        \n",
    "    notes_arrays_to_train = np.append(notes_arrays_to_train, \n",
    "                                      [notes_array_to_train], axis=0)\n",
    "        \n",
    "    \n",
    "    spectrogram = np.load(\"./spectrograms/{}.npy\".format(filename))[:,start_pt:start_pt+108]\n",
    "    # print(np.shape(spectrogram))\n",
    "    spec_array_to_train = np.append(spec_array_to_train, [spectrogram], axis=0)\n",
    "    \n",
    "print(np.shape(spec_array_to_train))\n",
    "print(np.shape(notes_arrays_to_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "x_train_ori = []\n",
    "y_train_ori = []\n",
    "x_test_ori = []\n",
    "y_test_ori = []\n",
    "for i in range(516):\n",
    "    print(i)\n",
    "    file = pd.read_csv(path + \"database_{}.csv\".format(i), dtype=str)\n",
    "    array = file.values\n",
    "    array = list(array)\n",
    "    train_test = random.sample(range(100), 60)\n",
    "    \n",
    "    for j in train_test[:50]:\n",
    "        sample = list(array[j])\n",
    "        lst = []\n",
    "        for x in sample[1:11665]:\n",
    "            if type(x) == str:\n",
    "                lst.append(float(x))\n",
    "            elif type(x) == float:\n",
    "                lst.append(x)\n",
    "            else:\n",
    "                print(type(x))\n",
    "        x_train_ori.append(lst)\n",
    "        y_train_ori.append(sample[0])\n",
    "        \n",
    "    for l in train_test[50:]:\n",
    "        sample = list(array[l])\n",
    "        lst = []\n",
    "        for x in sample[1:11665]:\n",
    "            if type(x) == str:\n",
    "                lst.append(float(x))\n",
    "            elif type(x) == float:\n",
    "                lst.append(x)\n",
    "            else:\n",
    "                print(type(x))\n",
    "        x_test_ori.append(lst)\n",
    "        y_test_ori.append(sample[0])\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(x_train_ori))\n",
    "print(np.shape(x_test_ori))\n",
    "print(np.shape(y_train_ori))\n",
    "print(np.shape(y_test_ori))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_convert(array, flag=0):\n",
    "    y = []\n",
    "    for i in array: # i is the name of each sample:\n",
    "        name = i.split(\"_\")\n",
    "        name = name[1:5]\n",
    "        if flag == 0:\n",
    "            # category of appearance of a note\n",
    "            category = np.zeros(24, dtype=int)\n",
    "            for j in name:\n",
    "                if j != \"x\":\n",
    "                    num = int(j)\n",
    "                    category[num-60] = 1\n",
    "            y.append(list(category))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_1 = []\n",
    "lst_2 = []\n",
    "\n",
    "for i in y_train_ori:\n",
    "    label = \"\".join(i.split())\n",
    "    lst_1.append(label)\n",
    "for j in y_test_ori:\n",
    "    label = \"\".join(j.split())\n",
    "    lst_2.append(label)\n",
    "    \n",
    "y_train_ori = y_convert(lst_1)\n",
    "y_test_ori = y_convert(lst_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_ori = np.array(y_train_ori)\n",
    "y_test_ori = np.array(y_test_ori)\n",
    "x_train_ori = np.array(x_train_ori)\n",
    "x_test_ori = np.array(x_test_ori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_ori = x_train_ori.flatten()\n",
    "x_train_ori = x_train_ori.reshape(25800, img_row, img_col, 1)\n",
    "x_test_ori = x_test_ori.flatten()\n",
    "x_test_ori = x_test_ori.reshape(5160, img_row, img_col, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train_ori.shape)\n",
    "input_shape = (108, 108, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_E():\n",
    "    early_stop = callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=3, verbose=1, mode=\"min\")\n",
    "    \n",
    "    filepath = \"Best-weights-my_model-{epoch:03d}-{loss:4f}-{acc:4f}.hdf5\"\n",
    "    checkpoint = callbacks.ModelCheckpoint(filepath, monitor=\"val_loss\", verbose=1, save_best_only= True, mode=\"min\")\n",
    "    \n",
    "    callbacks_list = [early_stop, checkpoint]\n",
    "    \n",
    "    #build the model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #first convolutional layer\n",
    "    model.add(Conv2D(32, (3,3), input_shape=input_shape, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    \n",
    "    #second convolutional layer\n",
    "    model.add(Conv2D(32, (3,3), activation=\"relu\", border_mode=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    \n",
    "    #third convolutional layer\n",
    "    model.add(Conv2D(64, (3,3), activation=\"relu\", border_mode = \"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "\n",
    "    #forth convolutional layer\n",
    "    model.add(Conv2D(64, (3,3), activation=\"relu\", border_mode = \"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    \n",
    "    #fifth convulutional layer\n",
    "    model.add(Conv2D(64, (3,3), activation=\"relu\", border_mode = \"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    #first FC layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    #second FC layer\n",
    "    model.add(Dense(256, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    #output layer\n",
    "    model.add(Dense(24, activation=\"sigmoid\", kernel_initializer=\"uniform\"))\n",
    "    \n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_E = build_model_E()\n",
    "hist_E = classifier_E.fit(x_train_ori, y_train_ori, batch_size=32, nb_epoch=200, verbose=1, \n",
    "                          validation_data=(x_test_ori, y_test_ori))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
