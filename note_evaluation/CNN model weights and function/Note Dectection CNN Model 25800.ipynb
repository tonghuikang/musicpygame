{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras import callbacks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_row = 108\n",
    "img_col = 108\n",
    "path = \"E:/database108/\"\n",
    "batch_size = 64\n",
    "output_classes = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "x_train_ori = []\n",
    "y_train_ori = []\n",
    "x_test_ori = []\n",
    "y_test_ori = []\n",
    "for i in range(516):\n",
    "    print(i)\n",
    "    file = pd.read_csv(path + \"database_{}.csv\".format(i), dtype=str)\n",
    "    array = file.values\n",
    "    array = list(array)\n",
    "    train_test = random.sample(range(100), 60)\n",
    "    \n",
    "    for j in train_test[:50]:\n",
    "        sample = list(array[j])\n",
    "        lst = []\n",
    "        for x in sample[1:11665]:\n",
    "            if type(x) == str:\n",
    "                x = \"\".join(x.split())\n",
    "                lst.append(float(x))\n",
    "            elif type(x) == float:\n",
    "                lst.append(x)\n",
    "            else:\n",
    "                print(type(x))\n",
    "        x_train_ori.append(lst)\n",
    "        y_train_ori.append(sample[0])\n",
    "        \n",
    "    for l in train_test[50:]:\n",
    "        sample = list(array[l])\n",
    "        lst = []\n",
    "        for x in sample[1:11665]:\n",
    "            if type(x) == str:\n",
    "                x = \"\".join(x.split())\n",
    "                lst.append(float(x))\n",
    "            elif type(x) == float:\n",
    "                lst.append(x)\n",
    "            else:\n",
    "                print(type(x))\n",
    "        x_test_ori.append(lst)\n",
    "        y_test_ori.append(sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25800, 11664)\n",
      "(5160, 11664)\n",
      "(25800,)\n",
      "(5160,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(x_train_ori))\n",
    "print(np.shape(x_test_ori))\n",
    "print(np.shape(y_train_ori))\n",
    "print(np.shape(y_test_ori))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_convert(array, flag=0):\n",
    "    y = []\n",
    "    for i in array: # i is the name of each sample:\n",
    "        name = i.split(\"_\")\n",
    "        name = name[1:5]\n",
    "        if flag == 0:\n",
    "            # category of appearance of a note\n",
    "            category = np.zeros(24, dtype=int)\n",
    "            for j in name:\n",
    "                if j != \"x\":\n",
    "                    num = int(j)\n",
    "                    category[num-60] = 1\n",
    "            y.append(list(category))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_1 = []\n",
    "lst_2 = []\n",
    "\n",
    "for i in y_train_ori:\n",
    "    label = \"\".join(i.split())\n",
    "    lst_1.append(label)\n",
    "for j in y_test_ori:\n",
    "    label = \"\".join(j.split())\n",
    "    lst_2.append(label)\n",
    "    \n",
    "y_train_ori = y_convert(lst_1)\n",
    "y_test_ori = y_convert(lst_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_ori = np.array(y_train_ori)\n",
    "y_test_ori = np.array(y_test_ori)\n",
    "x_train_ori = np.array(x_train_ori)\n",
    "x_test_ori = np.array(x_test_ori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_ori = x_train_ori.flatten()\n",
    "x_train_ori = x_train_ori.reshape(25800, img_row, img_col, 1)\n",
    "x_test_ori = x_test_ori.flatten()\n",
    "x_test_ori = x_test_ori.reshape(5160, img_row, img_col, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25800, 108, 108, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_ori.shape)\n",
    "input_shape = (108, 108, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_A():\n",
    "    early_stop = callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=3, verbose=1, mode=\"min\")\n",
    "    \n",
    "    filepath = \"Best-weights-my_model-{epoch:03d}-{loss:4f}-{acc:4f}.hdf5\"\n",
    "    checkpoint = callbacks.ModelCheckpoint(filepath, monitor=\"val_loss\", verbose=1, save_best_only= True, mode=\"min\")\n",
    "    \n",
    "    callbacks_list = [early_stop, checkpoint]\n",
    "    \n",
    "    #build the model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #first convolutional layer\n",
    "    model.add(Conv2D(32, (3,3), input_shape=input_shape, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    \n",
    "    #second convolutional layer\n",
    "    model.add(Conv2D(32, (3,3), activation=\"relu\", border_mode=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    \n",
    "    #third convolutional layer\n",
    "    model.add(Conv2D(64, (3,3), activation=\"relu\", border_mode = \"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    #first FC layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    #output layer\n",
    "    model.add(Dense(24, activation=\"softmax\", kernel_initializer=\"uniform\"))\n",
    "    \n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"SGD\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25800 samples, validate on 5160 samples\n",
      "Epoch 1/50\n",
      "25800/25800 [==============================] - 18s 703us/step - loss: 8.4349 - acc: 0.3898 - val_loss: 6.5902 - val_acc: 0.3562\n",
      "Epoch 2/50\n",
      "25800/25800 [==============================] - 15s 585us/step - loss: 6.6525 - acc: 0.3131 - val_loss: 6.1062 - val_acc: 0.4052\n",
      "Epoch 3/50\n",
      "25800/25800 [==============================] - 15s 586us/step - loss: 6.3411 - acc: 0.2949 - val_loss: 5.7046 - val_acc: 0.2731\n",
      "Epoch 4/50\n",
      "25800/25800 [==============================] - 15s 585us/step - loss: 6.2017 - acc: 0.2826 - val_loss: 5.8203 - val_acc: 0.1988\n",
      "Epoch 5/50\n",
      "25800/25800 [==============================] - 15s 586us/step - loss: 6.1098 - acc: 0.2801 - val_loss: 5.5129 - val_acc: 0.3905\n",
      "Epoch 6/50\n",
      "25800/25800 [==============================] - 15s 586us/step - loss: 6.0481 - acc: 0.2753 - val_loss: 5.5198 - val_acc: 0.2151\n",
      "Epoch 7/50\n",
      "25800/25800 [==============================] - 15s 588us/step - loss: 6.0056 - acc: 0.2772 - val_loss: 5.4784 - val_acc: 0.2853\n",
      "Epoch 8/50\n",
      "25800/25800 [==============================] - 15s 587us/step - loss: 5.9637 - acc: 0.2750 - val_loss: 5.4529 - val_acc: 0.1952\n",
      "Epoch 9/50\n",
      "25800/25800 [==============================] - 15s 587us/step - loss: 5.9355 - acc: 0.2750 - val_loss: 5.4766 - val_acc: 0.3913\n",
      "Epoch 10/50\n",
      "25800/25800 [==============================] - 15s 587us/step - loss: 5.9037 - acc: 0.2769 - val_loss: 5.4379 - val_acc: 0.3727\n",
      "Epoch 11/50\n",
      "25800/25800 [==============================] - 15s 587us/step - loss: 5.8847 - acc: 0.2763 - val_loss: 5.4035 - val_acc: 0.2816\n",
      "Epoch 12/50\n",
      "25800/25800 [==============================] - 15s 589us/step - loss: 5.8579 - acc: 0.2740 - val_loss: 5.4952 - val_acc: 0.2089\n",
      "Epoch 13/50\n",
      "25800/25800 [==============================] - 15s 593us/step - loss: 5.8412 - acc: 0.2769 - val_loss: 5.4808 - val_acc: 0.1855\n",
      "Epoch 14/50\n",
      "25800/25800 [==============================] - 15s 589us/step - loss: 5.8268 - acc: 0.2729 - val_loss: 5.4066 - val_acc: 0.3719\n",
      "Epoch 15/50\n",
      "25800/25800 [==============================] - 15s 588us/step - loss: 5.8150 - acc: 0.2705 - val_loss: 5.3639 - val_acc: 0.2831\n",
      "Epoch 16/50\n",
      "25800/25800 [==============================] - 15s 589us/step - loss: 5.8023 - acc: 0.2697 - val_loss: 5.4437 - val_acc: 0.2442\n",
      "Epoch 17/50\n",
      "25800/25800 [==============================] - 15s 588us/step - loss: 5.7873 - acc: 0.2726 - val_loss: 5.4029 - val_acc: 0.1574\n",
      "Epoch 18/50\n",
      "25800/25800 [==============================] - 15s 590us/step - loss: 5.7811 - acc: 0.2679 - val_loss: 5.3638 - val_acc: 0.2444\n",
      "Epoch 19/50\n",
      "25800/25800 [==============================] - 15s 589us/step - loss: 5.7653 - acc: 0.2731 - val_loss: 5.3404 - val_acc: 0.3424\n",
      "Epoch 20/50\n",
      "25800/25800 [==============================] - 15s 589us/step - loss: 5.7633 - acc: 0.2709 - val_loss: 5.3384 - val_acc: 0.1992\n",
      "Epoch 21/50\n",
      "25800/25800 [==============================] - 15s 588us/step - loss: 5.7496 - acc: 0.2719 - val_loss: 5.3965 - val_acc: 0.2591\n",
      "Epoch 22/50\n",
      "25800/25800 [==============================] - 15s 588us/step - loss: 5.7372 - acc: 0.2693 - val_loss: 5.3917 - val_acc: 0.2424\n",
      "Epoch 23/50\n",
      "25800/25800 [==============================] - 15s 588us/step - loss: 5.7334 - acc: 0.2686 - val_loss: 5.3345 - val_acc: 0.3839\n",
      "Epoch 24/50\n",
      "25800/25800 [==============================] - 15s 588us/step - loss: 5.7247 - acc: 0.2714 - val_loss: 5.3096 - val_acc: 0.3050\n",
      "Epoch 25/50\n",
      "25800/25800 [==============================] - 15s 588us/step - loss: 5.7177 - acc: 0.2716 - val_loss: 5.3644 - val_acc: 0.3510\n",
      "Epoch 26/50\n",
      "25800/25800 [==============================] - 15s 588us/step - loss: 5.7123 - acc: 0.2699 - val_loss: 5.3651 - val_acc: 0.2362\n",
      "Epoch 27/50\n",
      "25800/25800 [==============================] - 15s 588us/step - loss: 5.7062 - acc: 0.2689 - val_loss: 5.3336 - val_acc: 0.1426\n",
      "Epoch 28/50\n",
      "25800/25800 [==============================] - 15s 589us/step - loss: 5.6972 - acc: 0.2685 - val_loss: 5.3272 - val_acc: 0.3138\n",
      "Epoch 29/50\n",
      "25800/25800 [==============================] - 15s 588us/step - loss: 5.6919 - acc: 0.2697 - val_loss: 5.3240 - val_acc: 0.2256\n",
      "Epoch 30/50\n",
      "25800/25800 [==============================] - 15s 590us/step - loss: 5.6874 - acc: 0.2694 - val_loss: 5.3326 - val_acc: 0.2318\n",
      "Epoch 31/50\n",
      "25800/25800 [==============================] - 15s 589us/step - loss: 5.6828 - acc: 0.2678 - val_loss: 5.3130 - val_acc: 0.1866\n",
      "Epoch 32/50\n",
      "25800/25800 [==============================] - 15s 589us/step - loss: 5.6776 - acc: 0.2697 - val_loss: 5.4110 - val_acc: 0.2415\n",
      "Epoch 33/50\n",
      "25800/25800 [==============================] - 15s 589us/step - loss: 5.6730 - acc: 0.2666 - val_loss: 5.3164 - val_acc: 0.2136\n",
      "Epoch 34/50\n",
      "25800/25800 [==============================] - 15s 589us/step - loss: 5.6685 - acc: 0.2716 - val_loss: 5.3283 - val_acc: 0.2360\n",
      "Epoch 35/50\n",
      "25800/25800 [==============================] - 15s 589us/step - loss: 5.6622 - acc: 0.2661 - val_loss: 5.3324 - val_acc: 0.3494\n",
      "Epoch 36/50\n",
      "25800/25800 [==============================] - 15s 589us/step - loss: 5.6579 - acc: 0.2659 - val_loss: 5.3242 - val_acc: 0.2758\n",
      "Epoch 37/50\n",
      "25800/25800 [==============================] - 15s 589us/step - loss: 5.6535 - acc: 0.2684 - val_loss: 5.2815 - val_acc: 0.2692\n",
      "Epoch 38/50\n",
      "25800/25800 [==============================] - 15s 589us/step - loss: 5.6494 - acc: 0.2651 - val_loss: 5.3206 - val_acc: 0.3246\n",
      "Epoch 39/50\n",
      "25800/25800 [==============================] - 15s 589us/step - loss: 5.6441 - acc: 0.2693 - val_loss: 5.3138 - val_acc: 0.2341\n",
      "Epoch 40/50\n",
      "25800/25800 [==============================] - 15s 589us/step - loss: 5.6388 - acc: 0.2715 - val_loss: 5.2914 - val_acc: 0.2076\n",
      "Epoch 41/50\n",
      "25800/25800 [==============================] - 15s 589us/step - loss: 5.6354 - acc: 0.2667 - val_loss: 5.2874 - val_acc: 0.3411\n",
      "Epoch 42/50\n",
      "25800/25800 [==============================] - 15s 589us/step - loss: 5.6315 - acc: 0.2662 - val_loss: 5.3704 - val_acc: 0.2405\n",
      "Epoch 43/50\n",
      "25800/25800 [==============================] - 15s 589us/step - loss: 5.6272 - acc: 0.2673 - val_loss: 5.2870 - val_acc: 0.1498\n",
      "Epoch 44/50\n",
      "25800/25800 [==============================] - 15s 589us/step - loss: 5.6236 - acc: 0.2703 - val_loss: 5.3336 - val_acc: 0.2872\n",
      "Epoch 45/50\n",
      "25800/25800 [==============================] - 15s 588us/step - loss: 5.6167 - acc: 0.2685 - val_loss: 5.2778 - val_acc: 0.2335\n",
      "Epoch 46/50\n",
      "25800/25800 [==============================] - 15s 588us/step - loss: 5.6159 - acc: 0.2661 - val_loss: 5.2778 - val_acc: 0.1890\n",
      "Epoch 47/50\n",
      "25800/25800 [==============================] - 15s 588us/step - loss: 5.6124 - acc: 0.2666 - val_loss: 5.3020 - val_acc: 0.2843\n",
      "Epoch 48/50\n",
      "25800/25800 [==============================] - 15s 588us/step - loss: 5.6110 - acc: 0.2665 - val_loss: 5.2589 - val_acc: 0.2620\n",
      "Epoch 49/50\n",
      "25800/25800 [==============================] - 15s 586us/step - loss: 5.6050 - acc: 0.2691 - val_loss: 5.3339 - val_acc: 0.1525\n",
      "Epoch 50/50\n",
      "25800/25800 [==============================] - 15s 587us/step - loss: 5.6014 - acc: 0.2676 - val_loss: 5.2604 - val_acc: 0.1583\n"
     ]
    }
   ],
   "source": [
    "# randomly taking 10320 data samples\n",
    "classifier_A = build_model_A()\n",
    "hist_A = classifier_A.fit(x_train_ori, y_train_ori, batch_size=128, nb_epoch=50, verbose=1, \n",
    "                      validation_data=(x_test_ori, y_test_ori))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"weight-25800-CNN_A.hdf5\"\n",
    "classifier_A.save_weights(fname, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_B():\n",
    "    early_stop = callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=3, verbose=1, mode=\"min\")\n",
    "    \n",
    "    filepath = \"Best-weights-my_model-{epoch:03d}-{loss:4f}-{acc:4f}.hdf5\"\n",
    "    checkpoint = callbacks.ModelCheckpoint(filepath, monitor=\"val_loss\", verbose=1, save_best_only= True, mode=\"min\")\n",
    "    \n",
    "    callbacks_list = [early_stop, checkpoint]\n",
    "    \n",
    "    #build the model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #first convolutional layer\n",
    "    model.add(Conv2D(32, (3,3), input_shape=input_shape, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    \n",
    "    #second convolutional layer\n",
    "    model.add(Conv2D(32, (3,3), activation=\"relu\", border_mode=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    \n",
    "    #third convolutional layer\n",
    "    model.add(Conv2D(64, (3,3), activation=\"relu\", border_mode = \"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    \n",
    "    #forth convolutional layer\n",
    "    model.add(Conv2D(64, (3,3), activation=\"relu\", border_mode = \"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    #first FC layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "    model.add(BatchNormalization())\n",
    "        \n",
    "    #second FC layer\n",
    "    model.add(Dense(256, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    #output layer\n",
    "    model.add(Dense(24, activation=\"softmax\", kernel_initializer=\"uniform\"))\n",
    "    \n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:28: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25800 samples, validate on 5160 samples\n",
      "Epoch 1/40\n",
      "25800/25800 [==============================] - 19s 729us/step - loss: 6.7204 - acc: 0.2876 - val_loss: 5.5816 - val_acc: 0.1934\n",
      "Epoch 2/40\n",
      "25800/25800 [==============================] - 17s 675us/step - loss: 5.6990 - acc: 0.2688 - val_loss: 5.2922 - val_acc: 0.2711\n",
      "Epoch 3/40\n",
      "25800/25800 [==============================] - 17s 676us/step - loss: 5.5079 - acc: 0.2696 - val_loss: 5.2291 - val_acc: 0.2826\n",
      "Epoch 4/40\n",
      "25800/25800 [==============================] - 17s 675us/step - loss: 5.4197 - acc: 0.2674 - val_loss: 5.1835 - val_acc: 0.3240\n",
      "Epoch 5/40\n",
      "25800/25800 [==============================] - 17s 676us/step - loss: 5.3545 - acc: 0.2689 - val_loss: 5.1971 - val_acc: 0.2269\n",
      "Epoch 6/40\n",
      "25800/25800 [==============================] - 17s 676us/step - loss: 5.3095 - acc: 0.2689 - val_loss: 5.1593 - val_acc: 0.3172\n",
      "Epoch 7/40\n",
      "25800/25800 [==============================] - 17s 676us/step - loss: 5.2653 - acc: 0.2674 - val_loss: 5.1418 - val_acc: 0.2132\n",
      "Epoch 8/40\n",
      "25800/25800 [==============================] - 17s 676us/step - loss: 5.2319 - acc: 0.2686 - val_loss: 5.1342 - val_acc: 0.2614\n",
      "Epoch 9/40\n",
      "25800/25800 [==============================] - 17s 676us/step - loss: 5.2085 - acc: 0.2651 - val_loss: 5.1412 - val_acc: 0.2273\n",
      "Epoch 10/40\n",
      "25800/25800 [==============================] - 17s 677us/step - loss: 5.2018 - acc: 0.2662 - val_loss: 5.1329 - val_acc: 0.2684\n",
      "Epoch 11/40\n",
      "25800/25800 [==============================] - 18s 688us/step - loss: 5.1861 - acc: 0.2648 - val_loss: 5.1330 - val_acc: 0.2984\n",
      "Epoch 12/40\n",
      "25800/25800 [==============================] - 18s 687us/step - loss: 5.1909 - acc: 0.2676 - val_loss: 5.1234 - val_acc: 0.3403\n",
      "Epoch 13/40\n",
      "25800/25800 [==============================] - 18s 687us/step - loss: 5.1764 - acc: 0.2650 - val_loss: 5.1321 - val_acc: 0.2909\n",
      "Epoch 14/40\n",
      "25800/25800 [==============================] - 18s 686us/step - loss: 5.1886 - acc: 0.2672 - val_loss: 5.1297 - val_acc: 0.2002\n",
      "Epoch 15/40\n",
      "25800/25800 [==============================] - 17s 678us/step - loss: 5.1703 - acc: 0.2621 - val_loss: 5.1247 - val_acc: 0.3335\n",
      "Epoch 16/40\n",
      "25800/25800 [==============================] - 18s 679us/step - loss: 5.1724 - acc: 0.2637 - val_loss: 5.1252 - val_acc: 0.2692\n",
      "Epoch 17/40\n",
      "25800/25800 [==============================] - 17s 675us/step - loss: 5.1768 - acc: 0.2657 - val_loss: 5.1213 - val_acc: 0.3775\n",
      "Epoch 18/40\n",
      "25800/25800 [==============================] - 17s 676us/step - loss: 5.1701 - acc: 0.2638 - val_loss: 5.1230 - val_acc: 0.2779\n",
      "Epoch 19/40\n",
      "25800/25800 [==============================] - 17s 677us/step - loss: 5.1664 - acc: 0.2641 - val_loss: 5.1240 - val_acc: 0.2238\n",
      "Epoch 20/40\n",
      "25800/25800 [==============================] - 17s 676us/step - loss: 5.1636 - acc: 0.2703 - val_loss: 5.1275 - val_acc: 0.1986\n",
      "Epoch 21/40\n",
      "25800/25800 [==============================] - 18s 681us/step - loss: 5.1592 - acc: 0.2632 - val_loss: 5.1237 - val_acc: 0.3202\n",
      "Epoch 22/40\n",
      "25800/25800 [==============================] - 18s 681us/step - loss: 5.1579 - acc: 0.2679 - val_loss: 5.1200 - val_acc: 0.2184\n",
      "Epoch 23/40\n",
      "25800/25800 [==============================] - 18s 681us/step - loss: 5.1593 - acc: 0.2613 - val_loss: 5.1185 - val_acc: 0.1979\n",
      "Epoch 24/40\n",
      "25800/25800 [==============================] - 18s 680us/step - loss: 5.1570 - acc: 0.2637 - val_loss: 5.1218 - val_acc: 0.3120\n",
      "Epoch 25/40\n",
      "25800/25800 [==============================] - 18s 680us/step - loss: 5.1607 - acc: 0.2678 - val_loss: 5.1212 - val_acc: 0.1820\n",
      "Epoch 26/40\n",
      "25800/25800 [==============================] - 18s 680us/step - loss: 5.1623 - acc: 0.2656 - val_loss: 5.1217 - val_acc: 0.3318\n",
      "Epoch 27/40\n",
      "25800/25800 [==============================] - 18s 680us/step - loss: 5.1602 - acc: 0.2679 - val_loss: 5.1296 - val_acc: 0.1756\n",
      "Epoch 28/40\n",
      "25800/25800 [==============================] - 18s 681us/step - loss: 5.1641 - acc: 0.2662 - val_loss: 5.1316 - val_acc: 0.2696\n",
      "Epoch 29/40\n",
      "25800/25800 [==============================] - 18s 680us/step - loss: 5.1620 - acc: 0.2643 - val_loss: 5.1141 - val_acc: 0.1682\n",
      "Epoch 30/40\n",
      "25800/25800 [==============================] - 18s 680us/step - loss: 5.1539 - acc: 0.2618 - val_loss: 5.1219 - val_acc: 0.2804\n",
      "Epoch 31/40\n",
      "25800/25800 [==============================] - 18s 680us/step - loss: 5.1597 - acc: 0.2648 - val_loss: 5.1201 - val_acc: 0.1547\n",
      "Epoch 32/40\n",
      "25800/25800 [==============================] - 18s 680us/step - loss: 5.1556 - acc: 0.2645 - val_loss: 5.1192 - val_acc: 0.4056\n",
      "Epoch 33/40\n",
      "25800/25800 [==============================] - 18s 680us/step - loss: 5.1529 - acc: 0.2659 - val_loss: 5.1198 - val_acc: 0.3477\n",
      "Epoch 34/40\n",
      "25800/25800 [==============================] - 18s 680us/step - loss: 5.1499 - acc: 0.2694 - val_loss: 5.1166 - val_acc: 0.4523\n",
      "Epoch 35/40\n",
      "25800/25800 [==============================] - 18s 680us/step - loss: 5.1543 - acc: 0.2621 - val_loss: 5.1253 - val_acc: 0.1331\n",
      "Epoch 36/40\n",
      "25800/25800 [==============================] - 18s 680us/step - loss: 5.1508 - acc: 0.2638 - val_loss: 5.1212 - val_acc: 0.3149\n",
      "Epoch 37/40\n",
      "25800/25800 [==============================] - 18s 680us/step - loss: 5.1516 - acc: 0.2691 - val_loss: 5.1193 - val_acc: 0.2884\n",
      "Epoch 38/40\n",
      "25800/25800 [==============================] - 18s 681us/step - loss: 5.1566 - acc: 0.2669 - val_loss: 5.1165 - val_acc: 0.2583\n",
      "Epoch 39/40\n",
      "25800/25800 [==============================] - 18s 681us/step - loss: 5.1528 - acc: 0.2629 - val_loss: 5.1237 - val_acc: 0.4984\n",
      "Epoch 40/40\n",
      "25800/25800 [==============================] - 18s 681us/step - loss: 5.1493 - acc: 0.2666 - val_loss: 5.1244 - val_acc: 0.2572\n"
     ]
    }
   ],
   "source": [
    "# randomly taking 25800 data samples\n",
    "classifier_B = build_model_B()\n",
    "hist_B = classifier_B.fit(x_train_ori, y_train_ori, batch_size=64, nb_epoch=40, verbose=1,\n",
    "                          validation_data=(x_test_ori, y_test_ori))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"weight-25800-CNN_B.hdf5\"\n",
    "classifier_B.save_weights(fname, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_C():   \n",
    "    early_stop = callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=3, verbose=1, mode=\"min\")\n",
    "    \n",
    "    filepath = \"Best-weights-my_model-{epoch:03d}-{loss:4f}-{acc:4f}.hdf5\"\n",
    "    checkpoint = callbacks.ModelCheckpoint(filepath, monitor=\"val_loss\", verbose=1, save_best_only= True, mode=\"min\")\n",
    "    \n",
    "    callbacks_list = [early_stop, checkpoint]\n",
    "    \n",
    "    #build the model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #first convolutional layer\n",
    "    model.add(Conv2D(32, (3,3), input_shape=input_shape, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    \n",
    "    #second convolutional layer\n",
    "    model.add(Conv2D(32, (3,3), activation=\"relu\", border_mode=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    #first FC layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    #output layer\n",
    "    model.add(Dense(24, activation=\"softmax\", kernel_initializer=\"uniform\"))\n",
    "    \n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"SGD\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25800 samples, validate on 5160 samples\n",
      "Epoch 1/40\n",
      "25800/25800 [==============================] - 17s 650us/step - loss: 7.1172 - acc: 0.3403 - val_loss: 6.2741 - val_acc: 0.2029\n",
      "Epoch 2/40\n",
      "25800/25800 [==============================] - 16s 613us/step - loss: 6.0362 - acc: 0.2770 - val_loss: 5.5431 - val_acc: 0.1828\n",
      "Epoch 3/40\n",
      "25800/25800 [==============================] - 16s 614us/step - loss: 5.9119 - acc: 0.2769 - val_loss: 5.4659 - val_acc: 0.2506\n",
      "Epoch 4/40\n",
      "25800/25800 [==============================] - 16s 614us/step - loss: 5.8348 - acc: 0.2711 - val_loss: 5.4579 - val_acc: 0.2500\n",
      "Epoch 5/40\n",
      "25800/25800 [==============================] - 16s 614us/step - loss: 5.7952 - acc: 0.2726 - val_loss: 5.4811 - val_acc: 0.2072\n",
      "Epoch 6/40\n",
      "25800/25800 [==============================] - 16s 614us/step - loss: 5.7616 - acc: 0.2670 - val_loss: 5.4535 - val_acc: 0.3351\n",
      "Epoch 7/40\n",
      "25800/25800 [==============================] - 16s 615us/step - loss: 5.7354 - acc: 0.2685 - val_loss: 5.4422 - val_acc: 0.1781\n",
      "Epoch 8/40\n",
      "25800/25800 [==============================] - 16s 615us/step - loss: 5.7137 - acc: 0.2652 - val_loss: 5.4036 - val_acc: 0.2252\n",
      "Epoch 9/40\n",
      "25800/25800 [==============================] - 16s 615us/step - loss: 5.6971 - acc: 0.2688 - val_loss: 5.3889 - val_acc: 0.4318\n",
      "Epoch 10/40\n",
      "25800/25800 [==============================] - 16s 615us/step - loss: 5.6784 - acc: 0.2677 - val_loss: 5.3484 - val_acc: 0.3703\n",
      "Epoch 11/40\n",
      "25800/25800 [==============================] - 16s 616us/step - loss: 5.6641 - acc: 0.2706 - val_loss: 5.3477 - val_acc: 0.3027\n",
      "Epoch 12/40\n",
      "25800/25800 [==============================] - 16s 616us/step - loss: 5.6492 - acc: 0.2680 - val_loss: 5.4001 - val_acc: 0.1859\n",
      "Epoch 13/40\n",
      "25800/25800 [==============================] - 16s 615us/step - loss: 5.6401 - acc: 0.2666 - val_loss: 5.4118 - val_acc: 0.1130\n",
      "Epoch 14/40\n",
      "25800/25800 [==============================] - 16s 616us/step - loss: 5.6318 - acc: 0.2671 - val_loss: 5.3390 - val_acc: 0.3337\n",
      "Epoch 15/40\n",
      "25800/25800 [==============================] - 16s 616us/step - loss: 5.6195 - acc: 0.2682 - val_loss: 5.3231 - val_acc: 0.3442\n",
      "Epoch 16/40\n",
      "25800/25800 [==============================] - 16s 616us/step - loss: 5.6109 - acc: 0.2684 - val_loss: 5.3225 - val_acc: 0.1661\n",
      "Epoch 17/40\n",
      "25800/25800 [==============================] - 16s 616us/step - loss: 5.6004 - acc: 0.2649 - val_loss: 5.3267 - val_acc: 0.2531\n",
      "Epoch 18/40\n",
      "25800/25800 [==============================] - 16s 615us/step - loss: 5.5907 - acc: 0.2676 - val_loss: 5.3375 - val_acc: 0.2897\n",
      "Epoch 19/40\n",
      "25800/25800 [==============================] - 16s 615us/step - loss: 5.5828 - acc: 0.2660 - val_loss: 5.2949 - val_acc: 0.3008\n",
      "Epoch 20/40\n",
      "25800/25800 [==============================] - 16s 615us/step - loss: 5.5739 - acc: 0.2670 - val_loss: 5.3639 - val_acc: 0.2622\n",
      "Epoch 21/40\n",
      "25800/25800 [==============================] - 16s 615us/step - loss: 5.5659 - acc: 0.2694 - val_loss: 5.2816 - val_acc: 0.4074\n",
      "Epoch 22/40\n",
      "25800/25800 [==============================] - 16s 615us/step - loss: 5.5568 - acc: 0.2691 - val_loss: 5.2858 - val_acc: 0.4101\n",
      "Epoch 23/40\n",
      "25800/25800 [==============================] - 16s 615us/step - loss: 5.5532 - acc: 0.2693 - val_loss: 5.2742 - val_acc: 0.2376\n",
      "Epoch 24/40\n",
      "25800/25800 [==============================] - 16s 616us/step - loss: 5.5452 - acc: 0.2646 - val_loss: 5.3612 - val_acc: 0.1190\n",
      "Epoch 25/40\n",
      "25800/25800 [==============================] - 16s 616us/step - loss: 5.5401 - acc: 0.2653 - val_loss: 5.2790 - val_acc: 0.2486\n",
      "Epoch 26/40\n",
      "25800/25800 [==============================] - 16s 616us/step - loss: 5.5300 - acc: 0.2662 - val_loss: 5.3272 - val_acc: 0.1409\n",
      "Epoch 27/40\n",
      "25800/25800 [==============================] - 16s 616us/step - loss: 5.5245 - acc: 0.2661 - val_loss: 5.2638 - val_acc: 0.3866\n",
      "Epoch 28/40\n",
      "25800/25800 [==============================] - 16s 616us/step - loss: 5.5165 - acc: 0.2661 - val_loss: 5.2690 - val_acc: 0.3118\n",
      "Epoch 29/40\n",
      "25800/25800 [==============================] - 16s 616us/step - loss: 5.5103 - acc: 0.2699 - val_loss: 5.2662 - val_acc: 0.3058\n",
      "Epoch 30/40\n",
      "25800/25800 [==============================] - 16s 616us/step - loss: 5.5063 - acc: 0.2677 - val_loss: 5.2886 - val_acc: 0.1973\n",
      "Epoch 31/40\n",
      "25800/25800 [==============================] - 16s 617us/step - loss: 5.5001 - acc: 0.2635 - val_loss: 5.3806 - val_acc: 0.0855\n",
      "Epoch 32/40\n",
      "25800/25800 [==============================] - 16s 616us/step - loss: 5.4898 - acc: 0.2663 - val_loss: 5.2476 - val_acc: 0.3300\n",
      "Epoch 33/40\n",
      "25800/25800 [==============================] - 16s 616us/step - loss: 5.4861 - acc: 0.2661 - val_loss: 5.2792 - val_acc: 0.2516\n",
      "Epoch 34/40\n",
      "25800/25800 [==============================] - 16s 616us/step - loss: 5.4749 - acc: 0.2624 - val_loss: 5.2714 - val_acc: 0.2512\n",
      "Epoch 35/40\n",
      "25800/25800 [==============================] - 16s 616us/step - loss: 5.4728 - acc: 0.2636 - val_loss: 5.3050 - val_acc: 0.2424\n",
      "Epoch 36/40\n",
      "25800/25800 [==============================] - 16s 617us/step - loss: 5.4678 - acc: 0.2650 - val_loss: 5.2433 - val_acc: 0.1785\n",
      "Epoch 37/40\n",
      "25800/25800 [==============================] - 16s 616us/step - loss: 5.4615 - acc: 0.2686 - val_loss: 5.2540 - val_acc: 0.2558\n",
      "Epoch 38/40\n",
      "25800/25800 [==============================] - 16s 616us/step - loss: 5.4548 - acc: 0.2647 - val_loss: 5.2620 - val_acc: 0.2279\n",
      "Epoch 39/40\n",
      "25800/25800 [==============================] - 16s 616us/step - loss: 5.4512 - acc: 0.2636 - val_loss: 5.2736 - val_acc: 0.4355\n",
      "Epoch 40/40\n",
      "25800/25800 [==============================] - 16s 616us/step - loss: 5.4471 - acc: 0.2645 - val_loss: 5.3483 - val_acc: 0.2050\n"
     ]
    }
   ],
   "source": [
    "# randomly taking 25800 data samples\n",
    "classifier_C = build_model_C()\n",
    "hist_C = classifier_C.fit(x_train_ori, y_train_ori, batch_size=64, nb_epoch=40, verbose=1, \n",
    "                          validation_data=(x_test_ori, y_test_ori))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"weight-25800-CNN_C.hdf5\"\n",
    "classifier_C.save_weights(fname, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_D():\n",
    "    early_stop = callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=3, verbose=1, mode=\"min\")\n",
    "    \n",
    "    filepath = \"Best-weights-my_model-{epoch:03d}-{loss:4f}-{acc:4f}.hdf5\"\n",
    "    checkpoint = callbacks.ModelCheckpoint(filepath, monitor=\"val_loss\", verbose=1, save_best_only= True, mode=\"min\")\n",
    "    \n",
    "    callbacks_list = [early_stop, checkpoint]\n",
    "    \n",
    "    #build the model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #first convolutional layer\n",
    "    model.add(Conv2D(32, (3,3), input_shape=input_shape, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    \n",
    "    #second convolutional layer\n",
    "    model.add(Conv2D(32, (3,3), activation=\"relu\", border_mode=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    \n",
    "    #third convolutional layer\n",
    "    model.add(Conv2D(64, (3,3), activation=\"relu\", border_mode = \"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    \n",
    "    #forth convolutional layer\n",
    "    model.add(Conv2D(64, (3,3), activation=\"relu\", border_mode = \"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    #first FC layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "    model.add(BatchNormalization())\n",
    "        \n",
    "    #second FC layer\n",
    "    model.add(Dense(128, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    #third FC layer\n",
    "    model.add(Dense(256, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    #output layer\n",
    "    model.add(Dense(24, activation=\"softmax\", kernel_initializer=\"uniform\"))\n",
    "    \n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:28: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25800 samples, validate on 5160 samples\n",
      "Epoch 1/40\n",
      "25800/25800 [==============================] - 23s 894us/step - loss: 6.6334 - acc: 0.2863 - val_loss: 5.4716 - val_acc: 0.2312\n",
      "Epoch 2/40\n",
      "25800/25800 [==============================] - 22s 833us/step - loss: 5.6482 - acc: 0.2716 - val_loss: 5.2915 - val_acc: 0.2374\n",
      "Epoch 3/40\n",
      "25800/25800 [==============================] - 22s 834us/step - loss: 5.4960 - acc: 0.2683 - val_loss: 5.2423 - val_acc: 0.2845\n",
      "Epoch 4/40\n",
      "25800/25800 [==============================] - 21s 833us/step - loss: 5.3981 - acc: 0.2696 - val_loss: 5.2070 - val_acc: 0.2981\n",
      "Epoch 5/40\n",
      "25800/25800 [==============================] - 22s 835us/step - loss: 5.3328 - acc: 0.2654 - val_loss: 5.1714 - val_acc: 0.3105\n",
      "Epoch 6/40\n",
      "25800/25800 [==============================] - 22s 834us/step - loss: 5.2769 - acc: 0.2626 - val_loss: 5.1535 - val_acc: 0.3494\n",
      "Epoch 7/40\n",
      "25800/25800 [==============================] - 22s 834us/step - loss: 5.2466 - acc: 0.2690 - val_loss: 5.1474 - val_acc: 0.3318\n",
      "Epoch 8/40\n",
      "25800/25800 [==============================] - 22s 835us/step - loss: 5.2344 - acc: 0.2656 - val_loss: 5.1668 - val_acc: 0.3045\n",
      "Epoch 9/40\n",
      "25800/25800 [==============================] - 22s 836us/step - loss: 5.2283 - acc: 0.2653 - val_loss: 5.1587 - val_acc: 0.2180\n",
      "Epoch 10/40\n",
      "25800/25800 [==============================] - 22s 835us/step - loss: 5.2185 - acc: 0.2699 - val_loss: 5.1425 - val_acc: 0.3886\n",
      "Epoch 11/40\n",
      "25800/25800 [==============================] - 22s 835us/step - loss: 5.2116 - acc: 0.2667 - val_loss: 5.1345 - val_acc: 0.3403\n",
      "Epoch 12/40\n",
      "25800/25800 [==============================] - 22s 835us/step - loss: 5.2090 - acc: 0.2722 - val_loss: 5.2016 - val_acc: 0.1833\n",
      "Epoch 13/40\n",
      "25800/25800 [==============================] - 22s 834us/step - loss: 5.2088 - acc: 0.2669 - val_loss: 5.1371 - val_acc: 0.1560\n",
      "Epoch 14/40\n",
      "25800/25800 [==============================] - 22s 834us/step - loss: 5.2091 - acc: 0.2647 - val_loss: 5.1332 - val_acc: 0.2483\n",
      "Epoch 15/40\n",
      "25800/25800 [==============================] - 21s 833us/step - loss: 5.2013 - acc: 0.2619 - val_loss: 5.1273 - val_acc: 0.3279\n",
      "Epoch 16/40\n",
      "25800/25800 [==============================] - 21s 833us/step - loss: 5.1961 - acc: 0.2641 - val_loss: 5.1380 - val_acc: 0.2626\n",
      "Epoch 17/40\n",
      "25800/25800 [==============================] - 21s 832us/step - loss: 5.2005 - acc: 0.2696 - val_loss: 5.1640 - val_acc: 0.2390\n",
      "Epoch 18/40\n",
      "25800/25800 [==============================] - 22s 835us/step - loss: 5.1963 - acc: 0.2650 - val_loss: 5.1284 - val_acc: 0.4124\n",
      "Epoch 19/40\n",
      "25800/25800 [==============================] - 21s 832us/step - loss: 5.1912 - acc: 0.2712 - val_loss: 5.1299 - val_acc: 0.3149\n",
      "Epoch 20/40\n",
      "25800/25800 [==============================] - 21s 833us/step - loss: 5.1911 - acc: 0.2694 - val_loss: 5.1289 - val_acc: 0.3004\n",
      "Epoch 21/40\n",
      "25800/25800 [==============================] - 22s 835us/step - loss: 5.1915 - acc: 0.2720 - val_loss: 5.1284 - val_acc: 0.2998\n",
      "Epoch 22/40\n",
      "25800/25800 [==============================] - 21s 833us/step - loss: 5.1888 - acc: 0.2674 - val_loss: 5.1365 - val_acc: 0.4698\n",
      "Epoch 23/40\n",
      "25800/25800 [==============================] - 22s 834us/step - loss: 5.1880 - acc: 0.2686 - val_loss: 5.1305 - val_acc: 0.1349\n",
      "Epoch 24/40\n",
      "25800/25800 [==============================] - 22s 834us/step - loss: 5.1886 - acc: 0.2675 - val_loss: 5.1232 - val_acc: 0.2924\n",
      "Epoch 25/40\n",
      "25800/25800 [==============================] - 22s 834us/step - loss: 5.1827 - acc: 0.2734 - val_loss: 5.1279 - val_acc: 0.1521\n",
      "Epoch 26/40\n",
      "25800/25800 [==============================] - 22s 834us/step - loss: 5.1905 - acc: 0.2669 - val_loss: 5.1259 - val_acc: 0.2310\n",
      "Epoch 27/40\n",
      "25800/25800 [==============================] - 22s 834us/step - loss: 5.1847 - acc: 0.2660 - val_loss: 5.1314 - val_acc: 0.1839\n",
      "Epoch 28/40\n",
      "25800/25800 [==============================] - 22s 834us/step - loss: 5.1820 - acc: 0.2686 - val_loss: 5.1263 - val_acc: 0.3576\n",
      "Epoch 29/40\n",
      "25800/25800 [==============================] - 21s 833us/step - loss: 5.1800 - acc: 0.2669 - val_loss: 5.1232 - val_acc: 0.3023\n",
      "Epoch 30/40\n",
      "25800/25800 [==============================] - 22s 833us/step - loss: 5.1818 - acc: 0.2654 - val_loss: 5.1267 - val_acc: 0.2777\n",
      "Epoch 31/40\n",
      "25800/25800 [==============================] - 22s 836us/step - loss: 5.1833 - acc: 0.2693 - val_loss: 5.1261 - val_acc: 0.2390\n",
      "Epoch 32/40\n",
      "25800/25800 [==============================] - 22s 834us/step - loss: 5.1783 - acc: 0.2705 - val_loss: 5.1245 - val_acc: 0.2409\n",
      "Epoch 33/40\n",
      "25800/25800 [==============================] - 21s 833us/step - loss: 5.1798 - acc: 0.2705 - val_loss: 5.1275 - val_acc: 0.2283\n",
      "Epoch 34/40\n",
      "25800/25800 [==============================] - 22s 834us/step - loss: 5.1786 - acc: 0.2684 - val_loss: 5.1241 - val_acc: 0.3640\n",
      "Epoch 35/40\n",
      "25800/25800 [==============================] - 22s 834us/step - loss: 5.1758 - acc: 0.2692 - val_loss: 5.1308 - val_acc: 0.3089\n",
      "Epoch 36/40\n",
      "25800/25800 [==============================] - 22s 835us/step - loss: 5.1763 - acc: 0.2667 - val_loss: 5.1246 - val_acc: 0.2740\n",
      "Epoch 37/40\n",
      "25800/25800 [==============================] - 22s 836us/step - loss: 5.1761 - acc: 0.2634 - val_loss: 5.1183 - val_acc: 0.3680\n",
      "Epoch 38/40\n",
      "25800/25800 [==============================] - 22s 837us/step - loss: 5.1735 - acc: 0.2682 - val_loss: 5.1274 - val_acc: 0.2502\n",
      "Epoch 39/40\n",
      "25800/25800 [==============================] - 22s 835us/step - loss: 5.1732 - acc: 0.2663 - val_loss: 5.1193 - val_acc: 0.3359\n",
      "Epoch 40/40\n",
      "25800/25800 [==============================] - 22s 835us/step - loss: 5.1742 - acc: 0.2738 - val_loss: 5.1259 - val_acc: 0.2448\n"
     ]
    }
   ],
   "source": [
    "classifier_D = build_model_D()\n",
    "hist_D = classifier_D.fit(x_train_ori, y_train_ori, batch_size=32, nb_epoch=40, verbose=1, \n",
    "                          validation_data=(x_test_ori, y_test_ori))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"weight-25800-CNN_D.hdf5\"\n",
    "classifier_D.save_weights(fname, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:28: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25800 samples, validate on 5160 samples\n",
      "Epoch 1/200\n",
      "25800/25800 [==============================] - 22s 869us/step - loss: 6.5958 - acc: 0.2880 - val_loss: 5.4642 - val_acc: 0.2196\n",
      "Epoch 2/200\n",
      "25800/25800 [==============================] - 21s 802us/step - loss: 5.6624 - acc: 0.2705 - val_loss: 5.2693 - val_acc: 0.1919\n",
      "Epoch 3/200\n",
      "25800/25800 [==============================] - 21s 804us/step - loss: 5.4873 - acc: 0.2710 - val_loss: 5.2168 - val_acc: 0.2242\n",
      "Epoch 4/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.3961 - acc: 0.2667 - val_loss: 5.1821 - val_acc: 0.2198\n",
      "Epoch 5/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.3269 - acc: 0.2678 - val_loss: 5.1655 - val_acc: 0.3471\n",
      "Epoch 6/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.2831 - acc: 0.2677 - val_loss: 5.1456 - val_acc: 0.2266\n",
      "Epoch 7/200\n",
      "25800/25800 [==============================] - 21s 803us/step - loss: 5.2505 - acc: 0.2633 - val_loss: 5.1503 - val_acc: 0.1812\n",
      "Epoch 8/200\n",
      "25800/25800 [==============================] - 21s 804us/step - loss: 5.2403 - acc: 0.2668 - val_loss: 5.1732 - val_acc: 0.1622\n",
      "Epoch 9/200\n",
      "25800/25800 [==============================] - 21s 804us/step - loss: 5.2289 - acc: 0.2660 - val_loss: 5.1457 - val_acc: 0.2963\n",
      "Epoch 10/200\n",
      "25800/25800 [==============================] - 21s 804us/step - loss: 5.2225 - acc: 0.2695 - val_loss: 5.1483 - val_acc: 0.2442\n",
      "Epoch 11/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.2172 - acc: 0.2633 - val_loss: 5.1390 - val_acc: 0.3578\n",
      "Epoch 12/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.2121 - acc: 0.2658 - val_loss: 5.1286 - val_acc: 0.2688\n",
      "Epoch 13/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.2055 - acc: 0.2652 - val_loss: 5.1349 - val_acc: 0.1921\n",
      "Epoch 14/200\n",
      "25800/25800 [==============================] - 21s 804us/step - loss: 5.2065 - acc: 0.2643 - val_loss: 5.1315 - val_acc: 0.4300\n",
      "Epoch 15/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.2019 - acc: 0.2691 - val_loss: 5.1298 - val_acc: 0.3717\n",
      "Epoch 16/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.2001 - acc: 0.2674 - val_loss: 5.1258 - val_acc: 0.2657\n",
      "Epoch 17/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.1973 - acc: 0.2728 - val_loss: 5.1318 - val_acc: 0.1560\n",
      "Epoch 18/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.1943 - acc: 0.2633 - val_loss: 5.1457 - val_acc: 0.1380\n",
      "Epoch 19/200\n",
      "25800/25800 [==============================] - 21s 804us/step - loss: 5.1967 - acc: 0.2661 - val_loss: 5.1374 - val_acc: 0.3136\n",
      "Epoch 20/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.1948 - acc: 0.2667 - val_loss: 5.1336 - val_acc: 0.1721\n",
      "Epoch 21/200\n",
      "25800/25800 [==============================] - 21s 804us/step - loss: 5.1876 - acc: 0.2658 - val_loss: 5.1273 - val_acc: 0.2564\n",
      "Epoch 22/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.1902 - acc: 0.2649 - val_loss: 5.1293 - val_acc: 0.3977\n",
      "Epoch 23/200\n",
      "25800/25800 [==============================] - 21s 804us/step - loss: 5.1872 - acc: 0.2687 - val_loss: 5.1258 - val_acc: 0.2719\n",
      "Epoch 24/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.1867 - acc: 0.2669 - val_loss: 5.1288 - val_acc: 0.2570\n",
      "Epoch 25/200\n",
      "25800/25800 [==============================] - 21s 804us/step - loss: 5.1850 - acc: 0.2713 - val_loss: 5.1286 - val_acc: 0.1649\n",
      "Epoch 26/200\n",
      "25800/25800 [==============================] - 21s 804us/step - loss: 5.1824 - acc: 0.2709 - val_loss: 5.1249 - val_acc: 0.2938\n",
      "Epoch 27/200\n",
      "25800/25800 [==============================] - 21s 804us/step - loss: 5.1807 - acc: 0.2648 - val_loss: 5.1257 - val_acc: 0.1516\n",
      "Epoch 28/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.1825 - acc: 0.2698 - val_loss: 5.1248 - val_acc: 0.2269\n",
      "Epoch 29/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.1818 - acc: 0.2696 - val_loss: 5.1252 - val_acc: 0.2909\n",
      "Epoch 30/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.1794 - acc: 0.2668 - val_loss: 5.1287 - val_acc: 0.1748\n",
      "Epoch 31/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.1779 - acc: 0.2684 - val_loss: 5.1260 - val_acc: 0.1477\n",
      "Epoch 32/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1794 - acc: 0.2660 - val_loss: 5.1232 - val_acc: 0.3521\n",
      "Epoch 33/200\n",
      "25800/25800 [==============================] - 21s 804us/step - loss: 5.1775 - acc: 0.2662 - val_loss: 5.1257 - val_acc: 0.2727\n",
      "Epoch 34/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.1811 - acc: 0.2691 - val_loss: 5.1247 - val_acc: 0.3583\n",
      "Epoch 35/200\n",
      "25800/25800 [==============================] - 21s 804us/step - loss: 5.1770 - acc: 0.2674 - val_loss: 5.1244 - val_acc: 0.4182\n",
      "Epoch 36/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.1760 - acc: 0.2706 - val_loss: 5.1238 - val_acc: 0.3413\n",
      "Epoch 37/200\n",
      "25800/25800 [==============================] - 21s 804us/step - loss: 5.1767 - acc: 0.2692 - val_loss: 5.1232 - val_acc: 0.2773\n",
      "Epoch 38/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.1746 - acc: 0.2683 - val_loss: 5.1264 - val_acc: 0.2636\n",
      "Epoch 39/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.1741 - acc: 0.2697 - val_loss: 5.1202 - val_acc: 0.1366\n",
      "Epoch 40/200\n",
      "25800/25800 [==============================] - 21s 804us/step - loss: 5.1740 - acc: 0.2692 - val_loss: 5.1209 - val_acc: 0.2366\n",
      "Epoch 41/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.1728 - acc: 0.2670 - val_loss: 5.1305 - val_acc: 0.3694\n",
      "Epoch 42/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.1715 - acc: 0.2668 - val_loss: 5.1197 - val_acc: 0.2167\n",
      "Epoch 43/200\n",
      "25800/25800 [==============================] - 21s 804us/step - loss: 5.1695 - acc: 0.2656 - val_loss: 5.1225 - val_acc: 0.1872\n",
      "Epoch 44/200\n",
      "25800/25800 [==============================] - 21s 806us/step - loss: 5.1707 - acc: 0.2648 - val_loss: 5.1251 - val_acc: 0.3376\n",
      "Epoch 45/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.1715 - acc: 0.2700 - val_loss: 5.1263 - val_acc: 0.2864\n",
      "Epoch 46/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.1699 - acc: 0.2662 - val_loss: 5.1200 - val_acc: 0.2539\n",
      "Epoch 47/200\n",
      "25800/25800 [==============================] - 21s 806us/step - loss: 5.1699 - acc: 0.2682 - val_loss: 5.1168 - val_acc: 0.3025\n",
      "Epoch 48/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1684 - acc: 0.2671 - val_loss: 5.1200 - val_acc: 0.2599\n",
      "Epoch 49/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1682 - acc: 0.2681 - val_loss: 5.1220 - val_acc: 0.2818\n",
      "Epoch 50/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1689 - acc: 0.2679 - val_loss: 5.1237 - val_acc: 0.2680\n",
      "Epoch 51/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1673 - acc: 0.2751 - val_loss: 5.1252 - val_acc: 0.1888\n",
      "Epoch 52/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1685 - acc: 0.2673 - val_loss: 5.1432 - val_acc: 0.4917\n",
      "Epoch 53/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1672 - acc: 0.2679 - val_loss: 5.1187 - val_acc: 0.3812\n",
      "Epoch 54/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1651 - acc: 0.2666 - val_loss: 5.1186 - val_acc: 0.2824\n",
      "Epoch 55/200\n",
      "25800/25800 [==============================] - 21s 806us/step - loss: 5.1671 - acc: 0.2693 - val_loss: 5.1246 - val_acc: 0.3349\n",
      "Epoch 56/200\n",
      "25800/25800 [==============================] - 21s 806us/step - loss: 5.1653 - acc: 0.2699 - val_loss: 5.1180 - val_acc: 0.2926\n",
      "Epoch 57/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1639 - acc: 0.2672 - val_loss: 5.1189 - val_acc: 0.1899\n",
      "Epoch 58/200\n",
      "25800/25800 [==============================] - 21s 806us/step - loss: 5.1665 - acc: 0.2701 - val_loss: 5.1174 - val_acc: 0.3578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/200\n",
      "25800/25800 [==============================] - 21s 804us/step - loss: 5.1659 - acc: 0.2660 - val_loss: 5.1209 - val_acc: 0.3397\n",
      "Epoch 60/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.1648 - acc: 0.2704 - val_loss: 5.1183 - val_acc: 0.2469\n",
      "Epoch 61/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.1632 - acc: 0.2698 - val_loss: 5.1185 - val_acc: 0.1393\n",
      "Epoch 62/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.1638 - acc: 0.2695 - val_loss: 5.1169 - val_acc: 0.2529\n",
      "Epoch 63/200\n",
      "25800/25800 [==============================] - 21s 806us/step - loss: 5.1632 - acc: 0.2684 - val_loss: 5.1176 - val_acc: 0.2899\n",
      "Epoch 64/200\n",
      "25800/25800 [==============================] - 21s 806us/step - loss: 5.1641 - acc: 0.2673 - val_loss: 5.1209 - val_acc: 0.2068\n",
      "Epoch 65/200\n",
      "25800/25800 [==============================] - 21s 806us/step - loss: 5.1636 - acc: 0.2721 - val_loss: 5.1195 - val_acc: 0.3624\n",
      "Epoch 66/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1618 - acc: 0.2657 - val_loss: 5.1192 - val_acc: 0.2866\n",
      "Epoch 67/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1636 - acc: 0.2713 - val_loss: 5.1217 - val_acc: 0.2994\n",
      "Epoch 68/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1641 - acc: 0.2709 - val_loss: 5.1173 - val_acc: 0.1793\n",
      "Epoch 69/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1611 - acc: 0.2664 - val_loss: 5.1187 - val_acc: 0.1401\n",
      "Epoch 70/200\n",
      "25800/25800 [==============================] - 21s 806us/step - loss: 5.1630 - acc: 0.2728 - val_loss: 5.1194 - val_acc: 0.1610\n",
      "Epoch 71/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1627 - acc: 0.2685 - val_loss: 5.1154 - val_acc: 0.1690\n",
      "Epoch 72/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1605 - acc: 0.2711 - val_loss: 5.1163 - val_acc: 0.2973\n",
      "Epoch 73/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1611 - acc: 0.2710 - val_loss: 5.1137 - val_acc: 0.3316\n",
      "Epoch 74/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1594 - acc: 0.2727 - val_loss: 5.1412 - val_acc: 0.2744\n",
      "Epoch 75/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1626 - acc: 0.2665 - val_loss: 5.1149 - val_acc: 0.3184\n",
      "Epoch 76/200\n",
      "25800/25800 [==============================] - 21s 815us/step - loss: 5.1603 - acc: 0.2687 - val_loss: 5.1147 - val_acc: 0.2975\n",
      "Epoch 77/200\n",
      "25800/25800 [==============================] - 21s 820us/step - loss: 5.1604 - acc: 0.2683 - val_loss: 5.1166 - val_acc: 0.1316\n",
      "Epoch 78/200\n",
      "25800/25800 [==============================] - 21s 816us/step - loss: 5.1601 - acc: 0.2686 - val_loss: 5.1190 - val_acc: 0.1837\n",
      "Epoch 79/200\n",
      "25800/25800 [==============================] - 21s 821us/step - loss: 5.1581 - acc: 0.2695 - val_loss: 5.1181 - val_acc: 0.3118\n",
      "Epoch 80/200\n",
      "25800/25800 [==============================] - 21s 811us/step - loss: 5.1584 - acc: 0.2706 - val_loss: 5.1149 - val_acc: 0.3141\n",
      "Epoch 81/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1590 - acc: 0.2662 - val_loss: 5.1152 - val_acc: 0.2849\n",
      "Epoch 82/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1578 - acc: 0.2703 - val_loss: 5.1163 - val_acc: 0.3099\n",
      "Epoch 83/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1586 - acc: 0.2718 - val_loss: 5.1180 - val_acc: 0.3463\n",
      "Epoch 84/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1589 - acc: 0.2712 - val_loss: 5.1144 - val_acc: 0.2269\n",
      "Epoch 85/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1582 - acc: 0.2693 - val_loss: 5.1159 - val_acc: 0.4136\n",
      "Epoch 86/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1578 - acc: 0.2692 - val_loss: 5.1149 - val_acc: 0.3591\n",
      "Epoch 87/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1588 - acc: 0.2682 - val_loss: 5.1190 - val_acc: 0.2901\n",
      "Epoch 88/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1573 - acc: 0.2671 - val_loss: 5.1154 - val_acc: 0.4521\n",
      "Epoch 89/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1582 - acc: 0.2703 - val_loss: 5.1176 - val_acc: 0.2331\n",
      "Epoch 90/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1565 - acc: 0.2686 - val_loss: 5.1189 - val_acc: 0.3994\n",
      "Epoch 91/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1566 - acc: 0.2672 - val_loss: 5.1168 - val_acc: 0.4488\n",
      "Epoch 92/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1576 - acc: 0.2679 - val_loss: 5.1159 - val_acc: 0.3184\n",
      "Epoch 93/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1567 - acc: 0.2716 - val_loss: 5.1215 - val_acc: 0.3213\n",
      "Epoch 94/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1552 - acc: 0.2691 - val_loss: 5.1134 - val_acc: 0.2477\n",
      "Epoch 95/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1566 - acc: 0.2726 - val_loss: 5.1140 - val_acc: 0.3186\n",
      "Epoch 96/200\n",
      "25800/25800 [==============================] - 21s 809us/step - loss: 5.1553 - acc: 0.2663 - val_loss: 5.1177 - val_acc: 0.3475\n",
      "Epoch 97/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1555 - acc: 0.2684 - val_loss: 5.1156 - val_acc: 0.2291\n",
      "Epoch 98/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1561 - acc: 0.2703 - val_loss: 5.1133 - val_acc: 0.2186\n",
      "Epoch 99/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1547 - acc: 0.2660 - val_loss: 5.1184 - val_acc: 0.2374\n",
      "Epoch 100/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1555 - acc: 0.2671 - val_loss: 5.1192 - val_acc: 0.2422\n",
      "Epoch 101/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1577 - acc: 0.2673 - val_loss: 5.1149 - val_acc: 0.2955\n",
      "Epoch 102/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1557 - acc: 0.2710 - val_loss: 5.1142 - val_acc: 0.3930\n",
      "Epoch 103/200\n",
      "25800/25800 [==============================] - 21s 806us/step - loss: 5.1544 - acc: 0.2705 - val_loss: 5.1224 - val_acc: 0.1572\n",
      "Epoch 104/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1549 - acc: 0.2684 - val_loss: 5.1150 - val_acc: 0.2337\n",
      "Epoch 105/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1563 - acc: 0.2645 - val_loss: 5.1142 - val_acc: 0.1841\n",
      "Epoch 106/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1541 - acc: 0.2678 - val_loss: 5.1146 - val_acc: 0.1853\n",
      "Epoch 107/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1544 - acc: 0.2710 - val_loss: 5.1187 - val_acc: 0.2702\n",
      "Epoch 108/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1546 - acc: 0.2678 - val_loss: 5.1156 - val_acc: 0.2550\n",
      "Epoch 109/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1540 - acc: 0.2647 - val_loss: 5.1152 - val_acc: 0.3636\n",
      "Epoch 110/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1546 - acc: 0.2699 - val_loss: 5.1145 - val_acc: 0.2078\n",
      "Epoch 111/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1539 - acc: 0.2705 - val_loss: 5.1174 - val_acc: 0.2603\n",
      "Epoch 112/200\n",
      "25800/25800 [==============================] - 21s 806us/step - loss: 5.1543 - acc: 0.2678 - val_loss: 5.1120 - val_acc: 0.2971\n",
      "Epoch 113/200\n",
      "25800/25800 [==============================] - 21s 806us/step - loss: 5.1536 - acc: 0.2709 - val_loss: 5.1121 - val_acc: 0.1585\n",
      "Epoch 114/200\n",
      "25800/25800 [==============================] - 21s 806us/step - loss: 5.1546 - acc: 0.2672 - val_loss: 5.1149 - val_acc: 0.1068\n",
      "Epoch 115/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1532 - acc: 0.2664 - val_loss: 5.1163 - val_acc: 0.2876\n",
      "Epoch 116/200\n",
      "25800/25800 [==============================] - 21s 806us/step - loss: 5.1528 - acc: 0.2672 - val_loss: 5.1138 - val_acc: 0.2492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/200\n",
      "25800/25800 [==============================] - 21s 804us/step - loss: 5.1524 - acc: 0.2665 - val_loss: 5.1158 - val_acc: 0.4095\n",
      "Epoch 118/200\n",
      "25800/25800 [==============================] - 21s 804us/step - loss: 5.1537 - acc: 0.2675 - val_loss: 5.1186 - val_acc: 0.2508\n",
      "Epoch 119/200\n",
      "25800/25800 [==============================] - 21s 804us/step - loss: 5.1546 - acc: 0.2679 - val_loss: 5.1167 - val_acc: 0.2446\n",
      "Epoch 120/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.1542 - acc: 0.2692 - val_loss: 5.1135 - val_acc: 0.2076\n",
      "Epoch 121/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.1540 - acc: 0.2694 - val_loss: 5.1140 - val_acc: 0.3634\n",
      "Epoch 122/200\n",
      "25800/25800 [==============================] - 21s 804us/step - loss: 5.1537 - acc: 0.2683 - val_loss: 5.1148 - val_acc: 0.2008\n",
      "Epoch 123/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.1515 - acc: 0.2686 - val_loss: 5.1136 - val_acc: 0.2568\n",
      "Epoch 124/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.1514 - acc: 0.2736 - val_loss: 5.1135 - val_acc: 0.2300\n",
      "Epoch 125/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1514 - acc: 0.2724 - val_loss: 5.1160 - val_acc: 0.1661\n",
      "Epoch 126/200\n",
      "25800/25800 [==============================] - 21s 806us/step - loss: 5.1529 - acc: 0.2711 - val_loss: 5.1130 - val_acc: 0.2107\n",
      "Epoch 127/200\n",
      "25800/25800 [==============================] - 21s 806us/step - loss: 5.1523 - acc: 0.2676 - val_loss: 5.1145 - val_acc: 0.2880\n",
      "Epoch 128/200\n",
      "25800/25800 [==============================] - 21s 806us/step - loss: 5.1528 - acc: 0.2672 - val_loss: 5.1149 - val_acc: 0.2769\n",
      "Epoch 129/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1518 - acc: 0.2690 - val_loss: 5.1124 - val_acc: 0.3628\n",
      "Epoch 130/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1522 - acc: 0.2681 - val_loss: 5.1168 - val_acc: 0.3227\n",
      "Epoch 131/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.1525 - acc: 0.2695 - val_loss: 5.1145 - val_acc: 0.2417\n",
      "Epoch 132/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1536 - acc: 0.2671 - val_loss: 5.1140 - val_acc: 0.3440\n",
      "Epoch 133/200\n",
      "25800/25800 [==============================] - 21s 806us/step - loss: 5.1504 - acc: 0.2731 - val_loss: 5.1143 - val_acc: 0.2911\n",
      "Epoch 134/200\n",
      "25800/25800 [==============================] - 21s 806us/step - loss: 5.1509 - acc: 0.2688 - val_loss: 5.1128 - val_acc: 0.2306\n",
      "Epoch 135/200\n",
      "25800/25800 [==============================] - 21s 806us/step - loss: 5.1511 - acc: 0.2740 - val_loss: 5.1121 - val_acc: 0.2723\n",
      "Epoch 136/200\n",
      "25800/25800 [==============================] - 21s 806us/step - loss: 5.1514 - acc: 0.2687 - val_loss: 5.1152 - val_acc: 0.3370\n",
      "Epoch 137/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1518 - acc: 0.2717 - val_loss: 5.1119 - val_acc: 0.2227\n",
      "Epoch 138/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1516 - acc: 0.2727 - val_loss: 5.1149 - val_acc: 0.2709\n",
      "Epoch 139/200\n",
      "25800/25800 [==============================] - 21s 806us/step - loss: 5.1514 - acc: 0.2644 - val_loss: 5.1156 - val_acc: 0.4138\n",
      "Epoch 140/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1501 - acc: 0.2702 - val_loss: 5.1157 - val_acc: 0.3655\n",
      "Epoch 141/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1510 - acc: 0.2694 - val_loss: 5.1169 - val_acc: 0.3374\n",
      "Epoch 142/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1508 - acc: 0.2684 - val_loss: 5.1175 - val_acc: 0.2471\n",
      "Epoch 143/200\n",
      "25800/25800 [==============================] - 21s 806us/step - loss: 5.1519 - acc: 0.2711 - val_loss: 5.1121 - val_acc: 0.3027\n",
      "Epoch 144/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1511 - acc: 0.2686 - val_loss: 5.1131 - val_acc: 0.1767\n",
      "Epoch 145/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1502 - acc: 0.2667 - val_loss: 5.1160 - val_acc: 0.2909\n",
      "Epoch 146/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1500 - acc: 0.2702 - val_loss: 5.1152 - val_acc: 0.3012\n",
      "Epoch 147/200\n",
      "25800/25800 [==============================] - 21s 809us/step - loss: 5.1501 - acc: 0.2676 - val_loss: 5.1139 - val_acc: 0.3510\n",
      "Epoch 148/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1502 - acc: 0.2696 - val_loss: 5.1122 - val_acc: 0.2481\n",
      "Epoch 149/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1510 - acc: 0.2702 - val_loss: 5.1115 - val_acc: 0.3618\n",
      "Epoch 150/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1502 - acc: 0.2681 - val_loss: 5.1137 - val_acc: 0.1616\n",
      "Epoch 151/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1514 - acc: 0.2699 - val_loss: 5.1165 - val_acc: 0.2300\n",
      "Epoch 152/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1507 - acc: 0.2709 - val_loss: 5.1138 - val_acc: 0.3105\n",
      "Epoch 153/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1489 - acc: 0.2689 - val_loss: 5.1167 - val_acc: 0.2868\n",
      "Epoch 154/200\n",
      "25800/25800 [==============================] - 21s 809us/step - loss: 5.1494 - acc: 0.2688 - val_loss: 5.1117 - val_acc: 0.3161\n",
      "Epoch 155/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1495 - acc: 0.2708 - val_loss: 5.1147 - val_acc: 0.2238\n",
      "Epoch 156/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1500 - acc: 0.2658 - val_loss: 5.1134 - val_acc: 0.1368\n",
      "Epoch 157/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1499 - acc: 0.2651 - val_loss: 5.1131 - val_acc: 0.1663\n",
      "Epoch 158/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1505 - acc: 0.2651 - val_loss: 5.1135 - val_acc: 0.2099\n",
      "Epoch 159/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1484 - acc: 0.2710 - val_loss: 5.1136 - val_acc: 0.3153\n",
      "Epoch 160/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1491 - acc: 0.2661 - val_loss: 5.1147 - val_acc: 0.2184\n",
      "Epoch 161/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1500 - acc: 0.2698 - val_loss: 5.1125 - val_acc: 0.1422\n",
      "Epoch 162/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1492 - acc: 0.2689 - val_loss: 5.1126 - val_acc: 0.2229\n",
      "Epoch 163/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1497 - acc: 0.2617 - val_loss: 5.1131 - val_acc: 0.3924\n",
      "Epoch 164/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1504 - acc: 0.2709 - val_loss: 5.1131 - val_acc: 0.2901\n",
      "Epoch 165/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1494 - acc: 0.2707 - val_loss: 5.1153 - val_acc: 0.1374\n",
      "Epoch 166/200\n",
      "25800/25800 [==============================] - 21s 806us/step - loss: 5.1492 - acc: 0.2662 - val_loss: 5.1152 - val_acc: 0.3775\n",
      "Epoch 167/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.1498 - acc: 0.2671 - val_loss: 5.1147 - val_acc: 0.1826\n",
      "Epoch 168/200\n",
      "25800/25800 [==============================] - 21s 806us/step - loss: 5.1487 - acc: 0.2717 - val_loss: 5.1126 - val_acc: 0.3039\n",
      "Epoch 169/200\n",
      "25800/25800 [==============================] - 21s 806us/step - loss: 5.1477 - acc: 0.2672 - val_loss: 5.1160 - val_acc: 0.2798\n",
      "Epoch 170/200\n",
      "25800/25800 [==============================] - 21s 806us/step - loss: 5.1493 - acc: 0.2700 - val_loss: 5.1127 - val_acc: 0.2112\n",
      "Epoch 171/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.1495 - acc: 0.2647 - val_loss: 5.1164 - val_acc: 0.2386\n",
      "Epoch 172/200\n",
      "25800/25800 [==============================] - 21s 806us/step - loss: 5.1487 - acc: 0.2625 - val_loss: 5.1123 - val_acc: 0.1913\n",
      "Epoch 173/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.1483 - acc: 0.2709 - val_loss: 5.1161 - val_acc: 0.2182\n",
      "Epoch 174/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.1487 - acc: 0.2670 - val_loss: 5.1156 - val_acc: 0.2868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/200\n",
      "25800/25800 [==============================] - 21s 804us/step - loss: 5.1485 - acc: 0.2683 - val_loss: 5.1125 - val_acc: 0.1477\n",
      "Epoch 176/200\n",
      "25800/25800 [==============================] - 21s 804us/step - loss: 5.1487 - acc: 0.2719 - val_loss: 5.1106 - val_acc: 0.2868\n",
      "Epoch 177/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.1472 - acc: 0.2667 - val_loss: 5.1146 - val_acc: 0.2114\n",
      "Epoch 178/200\n",
      "25800/25800 [==============================] - 21s 806us/step - loss: 5.1476 - acc: 0.2666 - val_loss: 5.1122 - val_acc: 0.2112\n",
      "Epoch 179/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.1486 - acc: 0.2734 - val_loss: 5.1122 - val_acc: 0.2318\n",
      "Epoch 180/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.1487 - acc: 0.2692 - val_loss: 5.1156 - val_acc: 0.1417\n",
      "Epoch 181/200\n",
      "25800/25800 [==============================] - 21s 806us/step - loss: 5.1476 - acc: 0.2642 - val_loss: 5.1107 - val_acc: 0.2802\n",
      "Epoch 182/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.1478 - acc: 0.2696 - val_loss: 5.1132 - val_acc: 0.2845\n",
      "Epoch 183/200\n",
      "25800/25800 [==============================] - 21s 806us/step - loss: 5.1473 - acc: 0.2640 - val_loss: 5.1125 - val_acc: 0.1614\n",
      "Epoch 184/200\n",
      "25800/25800 [==============================] - 21s 806us/step - loss: 5.1483 - acc: 0.2681 - val_loss: 5.1131 - val_acc: 0.2279\n",
      "Epoch 185/200\n",
      "25800/25800 [==============================] - 21s 805us/step - loss: 5.1467 - acc: 0.2676 - val_loss: 5.1122 - val_acc: 0.3386\n",
      "Epoch 186/200\n",
      "25800/25800 [==============================] - 21s 806us/step - loss: 5.1472 - acc: 0.2678 - val_loss: 5.1135 - val_acc: 0.2661\n",
      "Epoch 187/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1475 - acc: 0.2683 - val_loss: 5.1125 - val_acc: 0.1568\n",
      "Epoch 188/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1472 - acc: 0.2678 - val_loss: 5.1139 - val_acc: 0.3021\n",
      "Epoch 189/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1483 - acc: 0.2712 - val_loss: 5.1170 - val_acc: 0.2273\n",
      "Epoch 190/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1480 - acc: 0.2683 - val_loss: 5.1142 - val_acc: 0.2979\n",
      "Epoch 191/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1482 - acc: 0.2718 - val_loss: 5.1099 - val_acc: 0.2696\n",
      "Epoch 192/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1474 - acc: 0.2710 - val_loss: 5.1112 - val_acc: 0.3337\n",
      "Epoch 193/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1482 - acc: 0.2671 - val_loss: 5.1166 - val_acc: 0.2876\n",
      "Epoch 194/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1482 - acc: 0.2699 - val_loss: 5.1173 - val_acc: 0.3473\n",
      "Epoch 195/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1467 - acc: 0.2625 - val_loss: 5.1118 - val_acc: 0.2983\n",
      "Epoch 196/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1473 - acc: 0.2632 - val_loss: 5.1165 - val_acc: 0.2465\n",
      "Epoch 197/200\n",
      "25800/25800 [==============================] - 21s 808us/step - loss: 5.1471 - acc: 0.2693 - val_loss: 5.1155 - val_acc: 0.3568\n",
      "Epoch 198/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1471 - acc: 0.2663 - val_loss: 5.1121 - val_acc: 0.2961\n",
      "Epoch 199/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1470 - acc: 0.2661 - val_loss: 5.1111 - val_acc: 0.2293\n",
      "Epoch 200/200\n",
      "25800/25800 [==============================] - 21s 807us/step - loss: 5.1464 - acc: 0.2682 - val_loss: 5.1156 - val_acc: 0.2787\n"
     ]
    }
   ],
   "source": [
    "classifier_B200 = build_model_B()\n",
    "hist_B200 = classifier_B200.fit(x_train_ori, y_train_ori, batch_size=32, nb_epoch=200, verbose=1, \n",
    "                          validation_data=(x_test_ori, y_test_ori))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_E():\n",
    "    early_stop = callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=3, verbose=1, mode=\"min\")\n",
    "    \n",
    "    filepath = \"Best-weights-my_model-{epoch:03d}-{loss:4f}-{acc:4f}.hdf5\"\n",
    "    checkpoint = callbacks.ModelCheckpoint(filepath, monitor=\"val_loss\", verbose=1, save_best_only= True, mode=\"min\")\n",
    "    \n",
    "    callbacks_list = [early_stop, checkpoint]\n",
    "    \n",
    "    #build the model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #first convolutional layer\n",
    "    model.add(Conv2D(32, (3,3), input_shape=input_shape, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    \n",
    "    #second convolutional layer\n",
    "    model.add(Conv2D(32, (3,3), activation=\"relu\", border_mode=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    \n",
    "    #third convolutional layer\n",
    "    model.add(Conv2D(64, (3,3), activation=\"relu\", border_mode = \"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "\n",
    "    #forth convolutional layer\n",
    "    model.add(Conv2D(64, (3,3), activation=\"relu\", border_mode = \"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    \n",
    "    #fifth convulutional layer\n",
    "    model.add(Conv2D(64, (3,3), activation=\"relu\", border_mode = \"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    #first FC layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "    model.add(BatchNormalization())\n",
    "        \n",
    "    #second FC layer\n",
    "    model.add(Dense(128, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    #third FC layer\n",
    "    model.add(Dense(256, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    #output layer\n",
    "    model.add(Dense(24, activation=\"sigmoid\", kernel_initializer=\"uniform\"))\n",
    "    \n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"weight-25800-CNN_B200.hdf5\"\n",
    "classifier_B200.save_weights(fname, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:28: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25800 samples, validate on 5160 samples\n",
      "Epoch 1/100\n",
      "25800/25800 [==============================] - 24s 945us/step - loss: 7.2339 - acc: 0.2927 - val_loss: 5.5839 - val_acc: 0.3688\n",
      "Epoch 2/100\n",
      "25800/25800 [==============================] - 23s 873us/step - loss: 5.8427 - acc: 0.2774 - val_loss: 5.3288 - val_acc: 0.2688\n",
      "Epoch 3/100\n",
      "25800/25800 [==============================] - 23s 873us/step - loss: 5.6262 - acc: 0.2711 - val_loss: 5.2607 - val_acc: 0.3196\n",
      "Epoch 4/100\n",
      "25800/25800 [==============================] - 23s 883us/step - loss: 5.5094 - acc: 0.2715 - val_loss: 5.2229 - val_acc: 0.2463\n",
      "Epoch 5/100\n",
      "25800/25800 [==============================] - 22s 871us/step - loss: 5.4241 - acc: 0.2664 - val_loss: 5.1873 - val_acc: 0.2008\n",
      "Epoch 6/100\n",
      "25800/25800 [==============================] - 22s 865us/step - loss: 5.3565 - acc: 0.2658 - val_loss: 5.2711 - val_acc: 0.2376\n",
      "Epoch 7/100\n",
      "25800/25800 [==============================] - 22s 864us/step - loss: 5.3178 - acc: 0.2686 - val_loss: 5.1532 - val_acc: 0.2851\n",
      "Epoch 8/100\n",
      "25800/25800 [==============================] - 22s 863us/step - loss: 5.2923 - acc: 0.2655 - val_loss: 5.1497 - val_acc: 0.2816\n",
      "Epoch 9/100\n",
      "25800/25800 [==============================] - 22s 864us/step - loss: 5.2696 - acc: 0.2688 - val_loss: 5.1535 - val_acc: 0.1676\n",
      "Epoch 10/100\n",
      "25800/25800 [==============================] - 22s 864us/step - loss: 5.2576 - acc: 0.2667 - val_loss: 5.1472 - val_acc: 0.1785\n",
      "Epoch 11/100\n",
      "25800/25800 [==============================] - 22s 864us/step - loss: 5.2541 - acc: 0.2653 - val_loss: 5.1381 - val_acc: 0.2583\n",
      "Epoch 12/100\n",
      "25800/25800 [==============================] - 22s 863us/step - loss: 5.2564 - acc: 0.2704 - val_loss: 5.1561 - val_acc: 0.3366\n",
      "Epoch 13/100\n",
      "25800/25800 [==============================] - 22s 863us/step - loss: 5.2406 - acc: 0.2683 - val_loss: 5.1374 - val_acc: 0.2638\n",
      "Epoch 14/100\n",
      "25800/25800 [==============================] - 22s 865us/step - loss: 5.2315 - acc: 0.2692 - val_loss: 5.1368 - val_acc: 0.1711\n",
      "Epoch 15/100\n",
      "25800/25800 [==============================] - 22s 864us/step - loss: 5.2357 - acc: 0.2688 - val_loss: 5.1448 - val_acc: 0.2659\n",
      "Epoch 16/100\n",
      "25800/25800 [==============================] - 22s 863us/step - loss: 5.2261 - acc: 0.2673 - val_loss: 5.1404 - val_acc: 0.2205\n",
      "Epoch 17/100\n",
      "25800/25800 [==============================] - 22s 864us/step - loss: 5.2260 - acc: 0.2685 - val_loss: 5.1324 - val_acc: 0.1860\n",
      "Epoch 18/100\n",
      "25800/25800 [==============================] - 22s 864us/step - loss: 5.2191 - acc: 0.2690 - val_loss: 5.1324 - val_acc: 0.1457\n",
      "Epoch 19/100\n",
      "25800/25800 [==============================] - 22s 864us/step - loss: 5.2158 - acc: 0.2688 - val_loss: 5.1513 - val_acc: 0.2314\n",
      "Epoch 20/100\n",
      "25800/25800 [==============================] - 22s 864us/step - loss: 5.2189 - acc: 0.2672 - val_loss: 5.1344 - val_acc: 0.3845\n",
      "Epoch 21/100\n",
      "25800/25800 [==============================] - 22s 862us/step - loss: 5.2149 - acc: 0.2681 - val_loss: 5.1313 - val_acc: 0.2585\n",
      "Epoch 22/100\n",
      "25800/25800 [==============================] - 22s 863us/step - loss: 5.2078 - acc: 0.2699 - val_loss: 5.1309 - val_acc: 0.2626\n",
      "Epoch 23/100\n",
      "25800/25800 [==============================] - 22s 862us/step - loss: 5.2198 - acc: 0.2702 - val_loss: 5.1541 - val_acc: 0.2547\n",
      "Epoch 24/100\n",
      "25800/25800 [==============================] - 22s 862us/step - loss: 5.2105 - acc: 0.2635 - val_loss: 5.1276 - val_acc: 0.1971\n",
      "Epoch 25/100\n",
      "25800/25800 [==============================] - 22s 863us/step - loss: 5.2071 - acc: 0.2640 - val_loss: 5.1295 - val_acc: 0.2911\n",
      "Epoch 26/100\n",
      "25800/25800 [==============================] - 22s 863us/step - loss: 5.2048 - acc: 0.2677 - val_loss: 5.1321 - val_acc: 0.0932\n",
      "Epoch 27/100\n",
      "25800/25800 [==============================] - 22s 862us/step - loss: 5.2108 - acc: 0.2663 - val_loss: 5.1288 - val_acc: 0.2828\n",
      "Epoch 28/100\n",
      "25800/25800 [==============================] - 22s 864us/step - loss: 5.2003 - acc: 0.2656 - val_loss: 5.1342 - val_acc: 0.1905\n",
      "Epoch 29/100\n",
      "25800/25800 [==============================] - 22s 864us/step - loss: 5.2012 - acc: 0.2629 - val_loss: 5.1302 - val_acc: 0.2244\n",
      "Epoch 30/100\n",
      "25800/25800 [==============================] - 22s 864us/step - loss: 5.1998 - acc: 0.2666 - val_loss: 5.1300 - val_acc: 0.2353\n",
      "Epoch 31/100\n",
      "25800/25800 [==============================] - 22s 864us/step - loss: 5.2008 - acc: 0.2703 - val_loss: 5.1235 - val_acc: 0.3019\n",
      "Epoch 32/100\n",
      "25800/25800 [==============================] - 22s 866us/step - loss: 5.1972 - acc: 0.2691 - val_loss: 5.1311 - val_acc: 0.2760\n",
      "Epoch 33/100\n",
      "25800/25800 [==============================] - 22s 864us/step - loss: 5.1995 - acc: 0.2659 - val_loss: 5.1267 - val_acc: 0.1359\n",
      "Epoch 34/100\n",
      "25800/25800 [==============================] - 22s 865us/step - loss: 5.2021 - acc: 0.2691 - val_loss: 5.1209 - val_acc: 0.2746\n",
      "Epoch 35/100\n",
      "25800/25800 [==============================] - 22s 865us/step - loss: 5.1906 - acc: 0.2647 - val_loss: 5.1269 - val_acc: 0.1702\n",
      "Epoch 36/100\n",
      "25800/25800 [==============================] - 22s 866us/step - loss: 5.1986 - acc: 0.2648 - val_loss: 5.1272 - val_acc: 0.3161\n",
      "Epoch 37/100\n",
      "25800/25800 [==============================] - 22s 866us/step - loss: 5.1929 - acc: 0.2631 - val_loss: 5.1212 - val_acc: 0.2047\n",
      "Epoch 38/100\n",
      "25800/25800 [==============================] - 22s 865us/step - loss: 5.1914 - acc: 0.2660 - val_loss: 5.1259 - val_acc: 0.1926\n",
      "Epoch 39/100\n",
      "25800/25800 [==============================] - 22s 866us/step - loss: 5.1941 - acc: 0.2676 - val_loss: 5.1189 - val_acc: 0.2370\n",
      "Epoch 40/100\n",
      "25800/25800 [==============================] - 22s 867us/step - loss: 5.1897 - acc: 0.2660 - val_loss: 5.1206 - val_acc: 0.1484\n",
      "Epoch 41/100\n",
      "25800/25800 [==============================] - 22s 866us/step - loss: 5.1877 - acc: 0.2633 - val_loss: 5.1219 - val_acc: 0.1857\n",
      "Epoch 42/100\n",
      "25800/25800 [==============================] - 22s 866us/step - loss: 5.1914 - acc: 0.2661 - val_loss: 5.1237 - val_acc: 0.3140\n",
      "Epoch 43/100\n",
      "25800/25800 [==============================] - 22s 866us/step - loss: 5.1910 - acc: 0.2654 - val_loss: 5.1270 - val_acc: 0.2533\n",
      "Epoch 44/100\n",
      "25800/25800 [==============================] - 22s 867us/step - loss: 5.1841 - acc: 0.2669 - val_loss: 5.1243 - val_acc: 0.1926\n",
      "Epoch 45/100\n",
      "25800/25800 [==============================] - 22s 867us/step - loss: 5.1923 - acc: 0.2677 - val_loss: 5.1241 - val_acc: 0.3595\n",
      "Epoch 46/100\n",
      "25800/25800 [==============================] - 22s 866us/step - loss: 5.1849 - acc: 0.2662 - val_loss: 5.1223 - val_acc: 0.1564\n",
      "Epoch 47/100\n",
      "25800/25800 [==============================] - 22s 870us/step - loss: 5.1834 - acc: 0.2695 - val_loss: 5.1288 - val_acc: 0.3079\n",
      "Epoch 48/100\n",
      "25800/25800 [==============================] - 22s 869us/step - loss: 5.1847 - acc: 0.2640 - val_loss: 5.1197 - val_acc: 0.1721\n",
      "Epoch 49/100\n",
      "25800/25800 [==============================] - 22s 869us/step - loss: 5.1885 - acc: 0.2658 - val_loss: 5.1306 - val_acc: 0.2426\n",
      "Epoch 50/100\n",
      "25800/25800 [==============================] - 22s 869us/step - loss: 5.1824 - acc: 0.2653 - val_loss: 5.1266 - val_acc: 0.2326\n",
      "Epoch 51/100\n",
      "25800/25800 [==============================] - 22s 869us/step - loss: 5.1851 - acc: 0.2662 - val_loss: 5.1287 - val_acc: 0.3494\n",
      "Epoch 52/100\n",
      "25800/25800 [==============================] - 22s 867us/step - loss: 5.1837 - acc: 0.2699 - val_loss: 5.1202 - val_acc: 0.1205\n",
      "Epoch 53/100\n",
      "25800/25800 [==============================] - 22s 870us/step - loss: 5.1852 - acc: 0.2661 - val_loss: 5.1268 - val_acc: 0.3839\n",
      "Epoch 54/100\n",
      "25800/25800 [==============================] - 22s 868us/step - loss: 5.1827 - acc: 0.2669 - val_loss: 5.1203 - val_acc: 0.2444\n",
      "Epoch 55/100\n",
      "25800/25800 [==============================] - 22s 871us/step - loss: 5.1817 - acc: 0.2649 - val_loss: 5.1155 - val_acc: 0.2678\n",
      "Epoch 56/100\n",
      "25800/25800 [==============================] - 22s 868us/step - loss: 5.1820 - acc: 0.2641 - val_loss: 5.1200 - val_acc: 0.2583\n",
      "Epoch 57/100\n",
      "25800/25800 [==============================] - 22s 868us/step - loss: 5.1817 - acc: 0.2645 - val_loss: 5.1164 - val_acc: 0.3329\n",
      "Epoch 58/100\n",
      "25800/25800 [==============================] - 22s 868us/step - loss: 5.1844 - acc: 0.2668 - val_loss: 5.1222 - val_acc: 0.3202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "25800/25800 [==============================] - 22s 861us/step - loss: 5.1832 - acc: 0.2687 - val_loss: 5.1205 - val_acc: 0.2791\n",
      "Epoch 60/100\n",
      "25800/25800 [==============================] - 22s 862us/step - loss: 5.1823 - acc: 0.2669 - val_loss: 5.1257 - val_acc: 0.1436\n",
      "Epoch 61/100\n",
      "25800/25800 [==============================] - 22s 864us/step - loss: 5.1774 - acc: 0.2664 - val_loss: 5.1239 - val_acc: 0.2940\n",
      "Epoch 62/100\n",
      "25800/25800 [==============================] - 22s 863us/step - loss: 5.1763 - acc: 0.2676 - val_loss: 5.1172 - val_acc: 0.3345\n",
      "Epoch 63/100\n",
      "25800/25800 [==============================] - 22s 863us/step - loss: 5.1739 - acc: 0.2690 - val_loss: 5.1180 - val_acc: 0.2512\n",
      "Epoch 64/100\n",
      "25800/25800 [==============================] - 22s 863us/step - loss: 5.1801 - acc: 0.2664 - val_loss: 5.1188 - val_acc: 0.3029\n",
      "Epoch 65/100\n",
      "25800/25800 [==============================] - 22s 863us/step - loss: 5.1799 - acc: 0.2653 - val_loss: 5.1276 - val_acc: 0.0901\n",
      "Epoch 66/100\n",
      "25800/25800 [==============================] - 22s 863us/step - loss: 5.1783 - acc: 0.2688 - val_loss: 5.1190 - val_acc: 0.1800\n",
      "Epoch 67/100\n",
      "25800/25800 [==============================] - 22s 861us/step - loss: 5.1774 - acc: 0.2639 - val_loss: 5.1225 - val_acc: 0.2481\n",
      "Epoch 68/100\n",
      "25800/25800 [==============================] - 22s 864us/step - loss: 5.1782 - acc: 0.2676 - val_loss: 5.1180 - val_acc: 0.2391\n",
      "Epoch 69/100\n",
      "25800/25800 [==============================] - 22s 863us/step - loss: 5.1748 - acc: 0.2697 - val_loss: 5.1166 - val_acc: 0.4789\n",
      "Epoch 70/100\n",
      "25800/25800 [==============================] - 22s 863us/step - loss: 5.1761 - acc: 0.2672 - val_loss: 5.1179 - val_acc: 0.3539\n",
      "Epoch 71/100\n",
      "25800/25800 [==============================] - 22s 863us/step - loss: 5.1754 - acc: 0.2687 - val_loss: 5.1276 - val_acc: 0.2126\n",
      "Epoch 72/100\n",
      "25800/25800 [==============================] - 22s 864us/step - loss: 5.1747 - acc: 0.2660 - val_loss: 5.1161 - val_acc: 0.3295\n",
      "Epoch 73/100\n",
      "25800/25800 [==============================] - 22s 864us/step - loss: 5.1728 - acc: 0.2674 - val_loss: 5.1231 - val_acc: 0.2711\n",
      "Epoch 74/100\n",
      "25800/25800 [==============================] - 22s 864us/step - loss: 5.1779 - acc: 0.2662 - val_loss: 5.1177 - val_acc: 0.2422\n",
      "Epoch 75/100\n",
      "25800/25800 [==============================] - 22s 867us/step - loss: 5.1724 - acc: 0.2649 - val_loss: 5.1168 - val_acc: 0.2027\n",
      "Epoch 76/100\n",
      "25800/25800 [==============================] - 22s 866us/step - loss: 5.1726 - acc: 0.2683 - val_loss: 5.1213 - val_acc: 0.2415\n",
      "Epoch 77/100\n",
      "25800/25800 [==============================] - 22s 866us/step - loss: 5.1718 - acc: 0.2706 - val_loss: 5.1182 - val_acc: 0.3671\n",
      "Epoch 78/100\n",
      "25800/25800 [==============================] - 22s 870us/step - loss: 5.1735 - acc: 0.2645 - val_loss: 5.1171 - val_acc: 0.1190\n",
      "Epoch 79/100\n",
      "25800/25800 [==============================] - 22s 868us/step - loss: 5.1733 - acc: 0.2632 - val_loss: 5.1184 - val_acc: 0.3074\n",
      "Epoch 80/100\n",
      "25800/25800 [==============================] - 22s 868us/step - loss: 5.1703 - acc: 0.2669 - val_loss: 5.1264 - val_acc: 0.3678\n",
      "Epoch 81/100\n",
      "25800/25800 [==============================] - 22s 869us/step - loss: 5.1745 - acc: 0.2683 - val_loss: 5.1246 - val_acc: 0.3215\n",
      "Epoch 82/100\n",
      "25800/25800 [==============================] - 22s 870us/step - loss: 5.1723 - acc: 0.2674 - val_loss: 5.1174 - val_acc: 0.1238\n",
      "Epoch 83/100\n",
      "25800/25800 [==============================] - 22s 868us/step - loss: 5.1695 - acc: 0.2682 - val_loss: 5.1167 - val_acc: 0.2779\n",
      "Epoch 84/100\n",
      "25800/25800 [==============================] - 22s 867us/step - loss: 5.1697 - acc: 0.2691 - val_loss: 5.1174 - val_acc: 0.3872\n",
      "Epoch 85/100\n",
      "25800/25800 [==============================] - 22s 868us/step - loss: 5.1729 - acc: 0.2647 - val_loss: 5.1181 - val_acc: 0.2318\n",
      "Epoch 86/100\n",
      "25800/25800 [==============================] - 22s 867us/step - loss: 5.1687 - acc: 0.2682 - val_loss: 5.1176 - val_acc: 0.1921\n",
      "Epoch 87/100\n",
      "25800/25800 [==============================] - 22s 868us/step - loss: 5.1703 - acc: 0.2634 - val_loss: 5.1203 - val_acc: 0.2849\n",
      "Epoch 88/100\n",
      "25800/25800 [==============================] - 22s 870us/step - loss: 5.1677 - acc: 0.2643 - val_loss: 5.1203 - val_acc: 0.1950\n",
      "Epoch 89/100\n",
      "25800/25800 [==============================] - 22s 868us/step - loss: 5.1684 - acc: 0.2631 - val_loss: 5.1170 - val_acc: 0.2853\n",
      "Epoch 90/100\n",
      "25800/25800 [==============================] - 22s 867us/step - loss: 5.1711 - acc: 0.2641 - val_loss: 5.1188 - val_acc: 0.2324\n",
      "Epoch 91/100\n",
      "25800/25800 [==============================] - 22s 869us/step - loss: 5.1675 - acc: 0.2651 - val_loss: 5.1184 - val_acc: 0.4289\n",
      "Epoch 92/100\n",
      "25800/25800 [==============================] - 22s 869us/step - loss: 5.1685 - acc: 0.2708 - val_loss: 5.1165 - val_acc: 0.3186\n",
      "Epoch 93/100\n",
      "25800/25800 [==============================] - 22s 871us/step - loss: 5.1680 - acc: 0.2682 - val_loss: 5.1193 - val_acc: 0.1452\n",
      "Epoch 94/100\n",
      "25800/25800 [==============================] - 22s 869us/step - loss: 5.1678 - acc: 0.2705 - val_loss: 5.1193 - val_acc: 0.2008\n",
      "Epoch 95/100\n",
      "25800/25800 [==============================] - 22s 870us/step - loss: 5.1689 - acc: 0.2685 - val_loss: 5.1139 - val_acc: 0.2426\n",
      "Epoch 96/100\n",
      "25800/25800 [==============================] - 22s 869us/step - loss: 5.1674 - acc: 0.2717 - val_loss: 5.1148 - val_acc: 0.1341\n",
      "Epoch 97/100\n",
      "25800/25800 [==============================] - 22s 868us/step - loss: 5.1680 - acc: 0.2684 - val_loss: 5.1140 - val_acc: 0.2188\n",
      "Epoch 98/100\n",
      "25800/25800 [==============================] - 22s 871us/step - loss: 5.1648 - acc: 0.2659 - val_loss: 5.1176 - val_acc: 0.2196\n",
      "Epoch 99/100\n",
      "25800/25800 [==============================] - 22s 869us/step - loss: 5.1665 - acc: 0.2649 - val_loss: 5.1175 - val_acc: 0.2442\n",
      "Epoch 100/100\n",
      "25800/25800 [==============================] - 22s 869us/step - loss: 5.1657 - acc: 0.2667 - val_loss: 5.1162 - val_acc: 0.1308\n"
     ]
    }
   ],
   "source": [
    "classifier_E = build_model_E()\n",
    "hist_E = classifier_E.fit(x_train_ori, y_train_ori, batch_size=32, nb_epoch=100, verbose=1, \n",
    "                          validation_data=(x_test_ori, y_test_ori))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"weight-25800-CNN_E.hdf5\"\n",
    "classifier_E.save_weights(fname, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_F():\n",
    "    early_stop = callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=3, verbose=1, mode=\"min\")\n",
    "    \n",
    "    filepath = \"Best-weights-my_model-{epoch:03d}-{loss:4f}-{acc:4f}.hdf5\"\n",
    "    checkpoint = callbacks.ModelCheckpoint(filepath, monitor=\"val_loss\", verbose=1, save_best_only= True, mode=\"min\")\n",
    "    \n",
    "    callbacks_list = [early_stop, checkpoint]\n",
    "    \n",
    "    #build the model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #first convolutional layer\n",
    "    model.add(Conv2D(32, (3,3), input_shape=input_shape, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    \n",
    "    #second convolutional layer\n",
    "    model.add(Conv2D(32, (3,3), activation=\"relu\", border_mode=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    \n",
    "    #third convolutional layer\n",
    "    model.add(Conv2D(32, (3,3), activation=\"relu\", border_mode = \"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "\n",
    "    #forth convolutional layer\n",
    "    model.add(Conv2D(64, (3,3), activation=\"relu\", border_mode = \"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    \n",
    "    #fifth convolutional layer\n",
    "    model.add(Conv2D(64, (3,3), activation=\"relu\", border_mode = \"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    \n",
    "    #sixth convolutional layer\n",
    "    model.add(Conv2D(64, (3,3), activation=\"relu\", border_mode = \"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    #first FC layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "    model.add(BatchNormalization())\n",
    "        \n",
    "    #second FC layer\n",
    "    model.add(Dense(128, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    #third FC layer\n",
    "    model.add(Dense(256, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    #output layer\n",
    "    model.add(Dense(24, activation=\"softmax\", kernel_initializer=\"uniform\"))\n",
    "    \n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:28: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:38: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25800 samples, validate on 5160 samples\n",
      "Epoch 1/50\n",
      "25800/25800 [==============================] - 25s 985us/step - loss: 8.7690 - acc: 0.3352 - val_loss: 6.3610 - val_acc: 0.3043\n",
      "Epoch 2/50\n",
      "25800/25800 [==============================] - 23s 887us/step - loss: 6.8167 - acc: 0.2984 - val_loss: 5.6635 - val_acc: 0.2690\n",
      "Epoch 3/50\n",
      "25800/25800 [==============================] - 23s 887us/step - loss: 6.3542 - acc: 0.2852 - val_loss: 5.4263 - val_acc: 0.2556\n",
      "Epoch 4/50\n",
      "25800/25800 [==============================] - 23s 879us/step - loss: 6.1190 - acc: 0.2832 - val_loss: 5.3327 - val_acc: 0.2215\n",
      "Epoch 5/50\n",
      "25800/25800 [==============================] - 22s 871us/step - loss: 5.9770 - acc: 0.2760 - val_loss: 5.2747 - val_acc: 0.3746\n",
      "Epoch 6/50\n",
      "25800/25800 [==============================] - 22s 868us/step - loss: 5.8715 - acc: 0.2736 - val_loss: 5.2249 - val_acc: 0.2748\n",
      "Epoch 7/50\n",
      "25800/25800 [==============================] - 22s 872us/step - loss: 5.7875 - acc: 0.2681 - val_loss: 5.2263 - val_acc: 0.3000\n",
      "Epoch 8/50\n",
      "25800/25800 [==============================] - 23s 877us/step - loss: 5.7246 - acc: 0.2719 - val_loss: 5.1884 - val_acc: 0.1901\n",
      "Epoch 9/50\n",
      "25800/25800 [==============================] - 22s 872us/step - loss: 5.6605 - acc: 0.2707 - val_loss: 5.1956 - val_acc: 0.2593\n",
      "Epoch 10/50\n",
      "25800/25800 [==============================] - 22s 866us/step - loss: 5.6338 - acc: 0.2707 - val_loss: 5.1636 - val_acc: 0.2523\n",
      "Epoch 11/50\n",
      "25800/25800 [==============================] - 22s 865us/step - loss: 5.6046 - acc: 0.2729 - val_loss: 5.1873 - val_acc: 0.2060\n",
      "Epoch 12/50\n",
      "25800/25800 [==============================] - 22s 865us/step - loss: 5.5808 - acc: 0.2718 - val_loss: 5.1654 - val_acc: 0.2432\n",
      "Epoch 13/50\n",
      "25800/25800 [==============================] - 22s 866us/step - loss: 5.5638 - acc: 0.2737 - val_loss: 5.1669 - val_acc: 0.2333\n",
      "Epoch 14/50\n",
      "25800/25800 [==============================] - 22s 865us/step - loss: 5.5492 - acc: 0.2722 - val_loss: 5.1553 - val_acc: 0.4120\n",
      "Epoch 15/50\n",
      "25800/25800 [==============================] - 22s 867us/step - loss: 5.5390 - acc: 0.2730 - val_loss: 5.1433 - val_acc: 0.2246\n",
      "Epoch 16/50\n",
      "25800/25800 [==============================] - 22s 867us/step - loss: 5.5156 - acc: 0.2725 - val_loss: 5.1626 - val_acc: 0.1645\n",
      "Epoch 17/50\n",
      "25800/25800 [==============================] - 22s 867us/step - loss: 5.5101 - acc: 0.2716 - val_loss: 5.1678 - val_acc: 0.2921\n",
      "Epoch 18/50\n",
      "25800/25800 [==============================] - 22s 867us/step - loss: 5.4976 - acc: 0.2681 - val_loss: 5.1548 - val_acc: 0.3988\n",
      "Epoch 19/50\n",
      "25800/25800 [==============================] - 22s 865us/step - loss: 5.4990 - acc: 0.2681 - val_loss: 5.1388 - val_acc: 0.3029\n",
      "Epoch 20/50\n",
      "25800/25800 [==============================] - 22s 866us/step - loss: 5.4839 - acc: 0.2722 - val_loss: 5.1460 - val_acc: 0.2496\n",
      "Epoch 21/50\n",
      "25800/25800 [==============================] - 22s 865us/step - loss: 5.4817 - acc: 0.2685 - val_loss: 5.1555 - val_acc: 0.3188\n",
      "Epoch 22/50\n",
      "25800/25800 [==============================] - 22s 866us/step - loss: 5.4687 - acc: 0.2710 - val_loss: 5.1556 - val_acc: 0.3746\n",
      "Epoch 23/50\n",
      "25800/25800 [==============================] - 22s 865us/step - loss: 5.4721 - acc: 0.2714 - val_loss: 5.1439 - val_acc: 0.2405\n",
      "Epoch 24/50\n",
      "25800/25800 [==============================] - 22s 864us/step - loss: 5.4660 - acc: 0.2676 - val_loss: 5.1530 - val_acc: 0.1816\n",
      "Epoch 25/50\n",
      "25800/25800 [==============================] - 22s 863us/step - loss: 5.4493 - acc: 0.2694 - val_loss: 5.1406 - val_acc: 0.3560\n",
      "Epoch 26/50\n",
      "25800/25800 [==============================] - 22s 864us/step - loss: 5.4525 - acc: 0.2661 - val_loss: 5.1457 - val_acc: 0.2556\n",
      "Epoch 27/50\n",
      "25800/25800 [==============================] - 22s 865us/step - loss: 5.4515 - acc: 0.2696 - val_loss: 5.1404 - val_acc: 0.2568\n",
      "Epoch 28/50\n",
      "25800/25800 [==============================] - 22s 866us/step - loss: 5.4364 - acc: 0.2693 - val_loss: 5.1462 - val_acc: 0.2145\n",
      "Epoch 29/50\n",
      "25800/25800 [==============================] - 22s 866us/step - loss: 5.4389 - acc: 0.2728 - val_loss: 5.1504 - val_acc: 0.3318\n",
      "Epoch 30/50\n",
      "25800/25800 [==============================] - 22s 866us/step - loss: 5.4377 - acc: 0.2723 - val_loss: 5.1365 - val_acc: 0.1672\n",
      "Epoch 31/50\n",
      "25800/25800 [==============================] - 22s 867us/step - loss: 5.4195 - acc: 0.2715 - val_loss: 5.1405 - val_acc: 0.2324\n",
      "Epoch 32/50\n",
      "25800/25800 [==============================] - 22s 867us/step - loss: 5.4332 - acc: 0.2677 - val_loss: 5.1390 - val_acc: 0.1789\n",
      "Epoch 33/50\n",
      "25800/25800 [==============================] - 22s 867us/step - loss: 5.4176 - acc: 0.2733 - val_loss: 5.1489 - val_acc: 0.4314\n",
      "Epoch 34/50\n",
      "25800/25800 [==============================] - 22s 869us/step - loss: 5.4206 - acc: 0.2707 - val_loss: 5.1400 - val_acc: 0.3238\n",
      "Epoch 35/50\n",
      "25800/25800 [==============================] - 22s 869us/step - loss: 5.4140 - acc: 0.2704 - val_loss: 5.1417 - val_acc: 0.3269\n",
      "Epoch 36/50\n",
      "25800/25800 [==============================] - 22s 867us/step - loss: 5.4120 - acc: 0.2697 - val_loss: 5.1382 - val_acc: 0.3963\n",
      "Epoch 37/50\n",
      "25800/25800 [==============================] - 22s 867us/step - loss: 5.4071 - acc: 0.2683 - val_loss: 5.1385 - val_acc: 0.2289\n",
      "Epoch 38/50\n",
      "25800/25800 [==============================] - 22s 867us/step - loss: 5.4015 - acc: 0.2733 - val_loss: 5.1384 - val_acc: 0.3298\n",
      "Epoch 39/50\n",
      "25800/25800 [==============================] - 22s 867us/step - loss: 5.4163 - acc: 0.2681 - val_loss: 5.1389 - val_acc: 0.2769\n",
      "Epoch 40/50\n",
      "25800/25800 [==============================] - 22s 868us/step - loss: 5.4025 - acc: 0.2683 - val_loss: 5.1422 - val_acc: 0.1847\n",
      "Epoch 41/50\n",
      "25800/25800 [==============================] - 22s 867us/step - loss: 5.3892 - acc: 0.2652 - val_loss: 5.1420 - val_acc: 0.2936\n",
      "Epoch 42/50\n",
      "25800/25800 [==============================] - 22s 868us/step - loss: 5.4084 - acc: 0.2760 - val_loss: 5.1366 - val_acc: 0.1657\n",
      "Epoch 43/50\n",
      "25800/25800 [==============================] - 22s 869us/step - loss: 5.3857 - acc: 0.2687 - val_loss: 5.1404 - val_acc: 0.0686\n",
      "Epoch 44/50\n",
      "25800/25800 [==============================] - 22s 868us/step - loss: 5.3927 - acc: 0.2691 - val_loss: 5.1410 - val_acc: 0.2384\n",
      "Epoch 45/50\n",
      "25800/25800 [==============================] - 22s 870us/step - loss: 5.3878 - acc: 0.2732 - val_loss: 5.1367 - val_acc: 0.2971\n",
      "Epoch 46/50\n",
      "25800/25800 [==============================] - 23s 873us/step - loss: 5.3836 - acc: 0.2690 - val_loss: 5.1398 - val_acc: 0.2196\n",
      "Epoch 47/50\n",
      "25800/25800 [==============================] - 23s 873us/step - loss: 5.3832 - acc: 0.2726 - val_loss: 5.1373 - val_acc: 0.3872\n",
      "Epoch 48/50\n",
      "25800/25800 [==============================] - 23s 873us/step - loss: 5.3816 - acc: 0.2698 - val_loss: 5.1352 - val_acc: 0.2988\n",
      "Epoch 49/50\n",
      "25800/25800 [==============================] - 23s 873us/step - loss: 5.3916 - acc: 0.2729 - val_loss: 5.1342 - val_acc: 0.2132\n",
      "Epoch 50/50\n",
      "25800/25800 [==============================] - 23s 882us/step - loss: 5.3857 - acc: 0.2670 - val_loss: 5.1313 - val_acc: 0.3081\n"
     ]
    }
   ],
   "source": [
    "classifier_F = build_model_F()\n",
    "hist_F = classifier_F.fit(x_train_ori, y_train_ori, batch_size=32, nb_epoch=50, verbose=1, \n",
    "                          validation_data=(x_test_ori, y_test_ori))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"weight-25800-CNN_F.hdf5\"\n",
    "classifier_F.save_weights(fname, overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
