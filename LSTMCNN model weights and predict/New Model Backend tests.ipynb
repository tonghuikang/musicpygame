{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import gc\n",
    "import keras \n",
    "from keras.layers import LSTM, Dense, TimeDistributed, Dropout, MaxPooling2D, Conv2D, Input\n",
    "from keras.layers import Flatten, BatchNormalization, Activation, Reshape, concatenate\n",
    "from keras.models import Model\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 5\n",
    "height = 108\n",
    "width = 108"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('E:/notes_database/index/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(optimizer):\n",
    "    #cnn\n",
    "    cnn_inputs = Input(shape=(height, width, 1), name='cnn_inputs')\n",
    "    layers = Conv2D(32, (3,3), padding=\"same\", activation=\"relu\")(cnn_inputs)\n",
    "    layers = BatchNormalization()(layers)\n",
    "    layers = MaxPooling2D(2,2)(layers)\n",
    "    layers = Conv2D(32, (3,3), border_mode='same', activation='relu')(layers)\n",
    "    layers = BatchNormalization()(layers)\n",
    "    layers = MaxPooling2D(2,2)(layers)\n",
    "    layers = Conv2D(64, (3,3), border_mode='same', activation='relu')(layers)\n",
    "    layers = BatchNormalization()(layers)\n",
    "    layers = MaxPooling2D(2,2)(layers)\n",
    "    layers = Conv2D(64, (3,3), border_mode='same', activation='relu')(layers)\n",
    "    layers = BatchNormalization()(layers)\n",
    "    layers = MaxPooling2D(2,2)(layers)\n",
    "    layers = Flatten()(layers)\n",
    "    layers = Dropout(0.3)(layers)\n",
    "    layers = Dense(32, activation='relu')(layers)\n",
    "    \n",
    "    #lstm\n",
    "    lstm_inputs = Input(shape=(steps, 24), name='lstm_inputs')\n",
    "    lstm_layers = LSTM(16, return_sequences=True)(lstm_inputs)\n",
    "    lstm_layers = LSTM(32)(lstm_layers)\n",
    "    lstm_layers = Dense(32, activation='relu')(lstm_layers)\n",
    "    lstm_layers = Dense(24, activation='relu')(lstm_layers)\n",
    "    \n",
    "    #main route\n",
    "    main_ = concatenate([layers, lstm_layers])\n",
    "    main_ = BatchNormalization()(main_)\n",
    "    main_ = Dense(64, activation='relu')(main_)\n",
    "    main_ = Dense(64, activation='relu')(main_)\n",
    "    output = Dense(24, activation='sigmoid')(main_)\n",
    "    \n",
    "    model = Model(inputs=[cnn_inputs, lstm_inputs], outputs=[output])\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['binary_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_order = []\n",
    "test_order = []\n",
    "files = np.array(files)\n",
    "for x,y in kf.split(files):\n",
    "    train_order.append(files[x])\n",
    "    test_order.append(files[y])\n",
    "train_order = np.array(train_order)\n",
    "test_order = np.array(test_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_score(model, cnn_test, lstm_test, y_test):\n",
    "    prediction = model.predict([cnn_test, lstm_test])\n",
    "    error = (prediction - y_test) ** 2\n",
    "    error = (error/y_test.shape[0]).sum()\n",
    "    wrong = 0\n",
    "    for i in range(prediction.shape[0]):\n",
    "        for j in range(prediction.shape[1]):\n",
    "            if abs(prediction[i][j] - y_test[i][j]) > 0.1:\n",
    "                wrong += 1\n",
    "                break\n",
    "    accuracy = 1 - wrong/(prediction.shape[0])\n",
    "    return error, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(num, name, epochs=100, batchsize=32, split=3, optimizer='adam'):\n",
    "    classifier = main(optimizer)\n",
    "    train_nums = []\n",
    "    length = train_order[num].shape[0]//split\n",
    "    for i in range(split-1):\n",
    "        train_nums.append(train_order[num][(i)*length:(i+1)*length])\n",
    "    train_nums.append(train_order[num][(split-1)*length:])\n",
    "        \n",
    "    for order in range(split):\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            #the first train_number works as the template\n",
    "            lstm_train, cnn_train, y_train = prepare_inputs(train_nums[order][0])\n",
    "            for file_name in train_nums[order][1:]:\n",
    "                lstm_input, cnn_input, y_train_input = prepare_inputs(file_name)\n",
    "                lstm_train = np.append(lstm_train, lstm_input, axis=0)\n",
    "                cnn_train = np.append(cnn_train, cnn_input, axis=0)\n",
    "                y_train = np.append(y_train, y_train_input, axis=0)\n",
    "            cnn_train = cnn_train.reshape((cnn_train.shape[0], 108, 108, 1))\n",
    "                \n",
    "            classifier.fit([cnn_train, lstm_train], y_train, epochs=1, batch_size=batchsize, verbose=1,\n",
    "                          validation_data=([cnn_test, lstm_test], y_test))\n",
    "            #CV_loss, accuracy = load_test_score(classifier, cnn_test, lstm_test, y_test)\n",
    "            #print('val_loss: ', CV_loss)\n",
    "            #print('val_accuarcy', accuracy)\n",
    "    classifier.save_weights(name, overwrite=True)\n",
    "    del classifier\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "  import sys\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138.18120694160461\n",
      "Train on 27780 samples, validate on 20929 samples\n",
      "Epoch 1/1\n",
      "27780/27780 [==============================] - 39s 1ms/step - loss: 0.0914 - acc: 0.3977 - val_loss: 0.0616 - val_acc: 0.5290\n",
      "Train on 27780 samples, validate on 20929 samples\n",
      "Epoch 1/1\n",
      "27780/27780 [==============================] - 33s 1ms/step - loss: 0.0544 - acc: 0.4889 - val_loss: 0.0466 - val_acc: 0.4891\n",
      "Train on 27780 samples, validate on 20929 samples\n",
      "Epoch 1/1\n",
      "27780/27780 [==============================] - 32s 1ms/step - loss: 0.0457 - acc: 0.4847 - val_loss: 0.0407 - val_acc: 0.4765\n",
      "Train on 27780 samples, validate on 20929 samples\n",
      "Epoch 1/1\n",
      "27780/27780 [==============================] - 32s 1ms/step - loss: 0.0419 - acc: 0.4801 - val_loss: 0.0454 - val_acc: 0.4688\n",
      "Train on 27780 samples, validate on 20929 samples\n",
      "Epoch 1/1\n",
      "27780/27780 [==============================] - 32s 1ms/step - loss: 0.0395 - acc: 0.4793 - val_loss: 0.0421 - val_acc: 0.5200\n",
      "Train on 27780 samples, validate on 20929 samples\n",
      "Epoch 1/1\n",
      "27780/27780 [==============================] - 32s 1ms/step - loss: 0.0380 - acc: 0.4822 - val_loss: 0.0604 - val_acc: 0.4764\n",
      "Train on 27780 samples, validate on 20929 samples\n",
      "Epoch 1/1\n",
      "27780/27780 [==============================] - 32s 1ms/step - loss: 0.0366 - acc: 0.4729 - val_loss: 0.0356 - val_acc: 0.4636\n",
      "Train on 27780 samples, validate on 20929 samples\n",
      "Epoch 1/1\n",
      "27780/27780 [==============================] - 32s 1ms/step - loss: 0.0356 - acc: 0.4705 - val_loss: 0.0381 - val_acc: 0.4612\n",
      "Train on 27780 samples, validate on 20929 samples\n",
      "Epoch 1/1\n",
      "27780/27780 [==============================] - 32s 1ms/step - loss: 0.0348 - acc: 0.4721 - val_loss: 0.0359 - val_acc: 0.4259\n",
      "Train on 27780 samples, validate on 20929 samples\n",
      "Epoch 1/1\n",
      "27780/27780 [==============================] - 32s 1ms/step - loss: 0.0340 - acc: 0.4695 - val_loss: 0.0474 - val_acc: 0.4969\n",
      "Train on 27780 samples, validate on 20929 samples\n",
      "Epoch 1/1\n",
      "27780/27780 [==============================] - 32s 1ms/step - loss: 0.0333 - acc: 0.4659 - val_loss: 0.0375 - val_acc: 0.4779\n",
      "Train on 27780 samples, validate on 20929 samples\n",
      "Epoch 1/1\n",
      "27780/27780 [==============================] - 32s 1ms/step - loss: 0.0328 - acc: 0.4716 - val_loss: 0.0327 - val_acc: 0.4899\n",
      "Train on 27780 samples, validate on 20929 samples\n",
      "Epoch 1/1\n",
      "27780/27780 [==============================] - 32s 1ms/step - loss: 0.0320 - acc: 0.4681 - val_loss: 0.0334 - val_acc: 0.5072\n",
      "Train on 27780 samples, validate on 20929 samples\n",
      "Epoch 1/1\n",
      "27780/27780 [==============================] - 33s 1ms/step - loss: 0.0316 - acc: 0.4663 - val_loss: 0.0481 - val_acc: 0.4992\n",
      "Train on 27780 samples, validate on 20929 samples\n",
      "Epoch 1/1\n",
      "27780/27780 [==============================] - 32s 1ms/step - loss: 0.0310 - acc: 0.4682 - val_loss: 0.0418 - val_acc: 0.4744\n",
      "Train on 27780 samples, validate on 20929 samples\n",
      "Epoch 1/1\n",
      "27780/27780 [==============================] - 34s 1ms/step - loss: 0.0305 - acc: 0.4715 - val_loss: 0.0460 - val_acc: 0.4901\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-131-659852a01d51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'weight-notes-1.hdf5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-130-615f65e9efb8>\u001b[0m in \u001b[0;36mtrain_test\u001b[1;34m(num, name, epochs, batchsize, split)\u001b[0m\n\u001b[0;32m     26\u001b[0m                 \u001b[0mlstm_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[0mlstm_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlstm_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[0mcnn_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnn_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m                 \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mcnn_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnn_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnn_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m108\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m108\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mappend\u001b[1;34m(arr, values, axis)\u001b[0m\n\u001b[0;32m   5164\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5165\u001b[0m         \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5166\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_test(0, 'weight-notes-1.hdf5')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_whole(num, name, epochs=100, batchsize=32, optimizer='adam'):\n",
    "    classifier = main(optimizer)\n",
    "        \n",
    "    #the first train_number works as the template\n",
    "    count = 1\n",
    "    ce = 0\n",
    "    lstm_train, cnn_train, y_train = prepare_inputs(train_order[num][0])\n",
    "    for file_name in train_order[num][1:]:\n",
    "        count+=1\n",
    "        if count == ce + 100:\n",
    "            print(count)\n",
    "            ce = count\n",
    "        lstm_input, cnn_input, y_train_input = prepare_inputs(file_name)\n",
    "        lstm_train = np.append(lstm_train, lstm_input, axis=0)\n",
    "        cnn_train = np.append(cnn_train, cnn_input, axis=0)\n",
    "        y_train = np.append(y_train, y_train_input, axis=0)\n",
    "    cnn_train = cnn_train.reshape((cnn_train.shape[0], 108, 108, 1))\n",
    "                \n",
    "    classifier.fit([cnn_train, lstm_train], y_train, epochs=epochs, batch_size=batchsize, verbose=1)\n",
    "    #CV_loss, accuracy = load_test_score(classifier, cnn_test, lstm_test, y_test)\n",
    "    #print('val_loss: ', CV_loss)\n",
    "    #print('val_accuarcy', accuracy)\n",
    "    classifier.save_weights(name, overwrite=True)\n",
    "    del classifier\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=3, random_state=7)\n",
    "train_order = []\n",
    "test_order = []\n",
    "files = np.array(files)[:900]\n",
    "for x,y in kf.split(files):\n",
    "    train_order.append(files[x])\n",
    "    test_order.append(files[y])\n",
    "train_order = np.array(train_order)\n",
    "test_order = np.array(test_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "  import sys\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "Epoch 1/100\n",
      "62698/62698 [==============================] - 63s 1ms/step - loss: 0.0689 - acc: 0.4481\n",
      "Epoch 2/100\n",
      "62698/62698 [==============================] - 58s 918us/step - loss: 0.0428 - acc: 0.4838TA: 0s - loss: 0.04\n",
      "Epoch 3/100\n",
      "62698/62698 [==============================] - 57s 917us/step - loss: 0.0383 - acc: 0.4805\n",
      "Epoch 4/100\n",
      "62698/62698 [==============================] - 58s 918us/step - loss: 0.0361 - acc: 0.4820\n",
      "Epoch 5/100\n",
      "62698/62698 [==============================] - 58s 917us/step - loss: 0.0347 - acc: 0.4723\n",
      "Epoch 6/100\n",
      "62698/62698 [==============================] - 58s 918us/step - loss: 0.0336 - acc: 0.4755\n",
      "Epoch 7/100\n",
      "62698/62698 [==============================] - 58s 919us/step - loss: 0.0326 - acc: 0.4757\n",
      "Epoch 8/100\n",
      "62698/62698 [==============================] - 58s 925us/step - loss: 0.0321 - acc: 0.4676\n",
      "Epoch 9/100\n",
      "62698/62698 [==============================] - 58s 925us/step - loss: 0.0315 - acc: 0.47250s - loss: 0.0315 \n",
      "Epoch 10/100\n",
      "62698/62698 [==============================] - 58s 924us/step - loss: 0.0309 - acc: 0.4709\n",
      "Epoch 11/100\n",
      "62698/62698 [==============================] - 58s 925us/step - loss: 0.0305 - acc: 0.4682\n",
      "Epoch 12/100\n",
      "62698/62698 [==============================] - 58s 929us/step - loss: 0.0300 - acc: 0.4658\n",
      "Epoch 13/100\n",
      "62698/62698 [==============================] - 58s 931us/step - loss: 0.0297 - acc: 0.46450s - loss: 0.0297 - \n",
      "Epoch 14/100\n",
      "62698/62698 [==============================] - 58s 923us/step - loss: 0.0293 - acc: 0.4683\n",
      "Epoch 15/100\n",
      "62698/62698 [==============================] - 58s 924us/step - loss: 0.0290 - acc: 0.4647\n",
      "Epoch 16/100\n",
      "62698/62698 [==============================] - 58s 925us/step - loss: 0.0286 - acc: 0.4616\n",
      "Epoch 17/100\n",
      "62698/62698 [==============================] - 58s 924us/step - loss: 0.0284 - acc: 0.4608\n",
      "Epoch 18/100\n",
      "62698/62698 [==============================] - 58s 925us/step - loss: 0.0281 - acc: 0.45841s - loss: 0.\n",
      "Epoch 19/100\n",
      "62698/62698 [==============================] - 58s 925us/step - loss: 0.0277 - acc: 0.4594\n",
      "Epoch 20/100\n",
      "62698/62698 [==============================] - 58s 923us/step - loss: 0.0275 - acc: 0.4593\n",
      "Epoch 21/100\n",
      "62698/62698 [==============================] - 58s 926us/step - loss: 0.0272 - acc: 0.4559\n",
      "Epoch 22/100\n",
      "62698/62698 [==============================] - 58s 925us/step - loss: 0.0271 - acc: 0.45211s\n",
      "Epoch 23/100\n",
      "62698/62698 [==============================] - 58s 924us/step - loss: 0.0269 - acc: 0.4554\n",
      "Epoch 24/100\n",
      "62698/62698 [==============================] - 58s 925us/step - loss: 0.0267 - acc: 0.4550\n",
      "Epoch 25/100\n",
      "62698/62698 [==============================] - 58s 926us/step - loss: 0.0266 - acc: 0.4540\n",
      "Epoch 26/100\n",
      "62698/62698 [==============================] - 58s 923us/step - loss: 0.0262 - acc: 0.4551\n",
      "Epoch 27/100\n",
      "62698/62698 [==============================] - 58s 924us/step - loss: 0.0262 - acc: 0.4559\n",
      "Epoch 28/100\n",
      "62698/62698 [==============================] - 58s 926us/step - loss: 0.0260 - acc: 0.45381s -\n",
      "Epoch 29/100\n",
      "62698/62698 [==============================] - 58s 924us/step - loss: 0.0259 - acc: 0.4489\n",
      "Epoch 30/100\n",
      "62698/62698 [==============================] - 58s 925us/step - loss: 0.0257 - acc: 0.4485\n",
      "Epoch 31/100\n",
      "62698/62698 [==============================] - 58s 924us/step - loss: 0.0255 - acc: 0.4487\n",
      "Epoch 32/100\n",
      "62698/62698 [==============================] - 58s 926us/step - loss: 0.0254 - acc: 0.4397\n",
      "Epoch 33/100\n",
      "62698/62698 [==============================] - 58s 924us/step - loss: 0.0252 - acc: 0.44130s - loss: 0.0252 - acc: 0\n",
      "Epoch 34/100\n",
      "62698/62698 [==============================] - 58s 926us/step - loss: 0.0251 - acc: 0.44405s - loss: 0.0251 - acc:  - ETA: 1s - loss: 0.\n",
      "Epoch 35/100\n",
      "62698/62698 [==============================] - 58s 927us/step - loss: 0.0250 - acc: 0.4438\n",
      "Epoch 36/100\n",
      "62698/62698 [==============================] - 58s 925us/step - loss: 0.0248 - acc: 0.4420\n",
      "Epoch 37/100\n",
      "62698/62698 [==============================] - 58s 925us/step - loss: 0.0247 - acc: 0.4426\n",
      "Epoch 38/100\n",
      "62698/62698 [==============================] - 58s 925us/step - loss: 0.0246 - acc: 0.4357\n",
      "Epoch 39/100\n",
      "62698/62698 [==============================] - 58s 926us/step - loss: 0.0245 - acc: 0.44200s - loss: 0.0245 - acc: 0.4\n",
      "Epoch 40/100\n",
      "62698/62698 [==============================] - 58s 925us/step - loss: 0.0243 - acc: 0.4382\n",
      "Epoch 41/100\n",
      "62698/62698 [==============================] - 58s 926us/step - loss: 0.0243 - acc: 0.4414\n",
      "Epoch 42/100\n",
      "62698/62698 [==============================] - 58s 930us/step - loss: 0.0243 - acc: 0.4407\n",
      "Epoch 43/100\n",
      "62698/62698 [==============================] - 58s 926us/step - loss: 0.0241 - acc: 0.4414\n",
      "Epoch 44/100\n",
      "62698/62698 [==============================] - 58s 926us/step - loss: 0.0240 - acc: 0.4416\n",
      "Epoch 45/100\n",
      "62698/62698 [==============================] - 58s 925us/step - loss: 0.0239 - acc: 0.43921s - los\n",
      "Epoch 46/100\n",
      "62698/62698 [==============================] - 58s 926us/step - loss: 0.0238 - acc: 0.4407\n",
      "Epoch 47/100\n",
      "62698/62698 [==============================] - 58s 926us/step - loss: 0.0237 - acc: 0.4447\n",
      "Epoch 48/100\n",
      "62698/62698 [==============================] - 58s 930us/step - loss: 0.0237 - acc: 0.4416\n",
      "Epoch 49/100\n",
      "62698/62698 [==============================] - 58s 926us/step - loss: 0.0236 - acc: 0.44161\n",
      "Epoch 50/100\n",
      "62698/62698 [==============================] - 58s 928us/step - loss: 0.0236 - acc: 0.4426\n",
      "Epoch 51/100\n",
      "62698/62698 [==============================] - 58s 927us/step - loss: 0.0234 - acc: 0.4394\n",
      "Epoch 52/100\n",
      "62698/62698 [==============================] - 58s 927us/step - loss: 0.0233 - acc: 0.4388\n",
      "Epoch 53/100\n",
      "62698/62698 [==============================] - 58s 928us/step - loss: 0.0233 - acc: 0.4415\n",
      "Epoch 54/100\n",
      "62698/62698 [==============================] - 58s 926us/step - loss: 0.0232 - acc: 0.4432\n",
      "Epoch 55/100\n",
      "62698/62698 [==============================] - 58s 928us/step - loss: 0.0231 - acc: 0.4375\n",
      "Epoch 56/100\n",
      "62698/62698 [==============================] - 58s 928us/step - loss: 0.0231 - acc: 0.4400\n",
      "Epoch 57/100\n",
      "62698/62698 [==============================] - 58s 927us/step - loss: 0.0230 - acc: 0.4419\n",
      "Epoch 58/100\n",
      "62698/62698 [==============================] - 58s 929us/step - loss: 0.0230 - acc: 0.4412\n",
      "Epoch 59/100\n",
      "62698/62698 [==============================] - 58s 928us/step - loss: 0.0229 - acc: 0.4404\n",
      "Epoch 60/100\n",
      "62698/62698 [==============================] - 58s 928us/step - loss: 0.0228 - acc: 0.4412\n",
      "Epoch 61/100\n",
      "62698/62698 [==============================] - 58s 929us/step - loss: 0.0228 - acc: 0.4405\n",
      "Epoch 62/100\n",
      "62698/62698 [==============================] - 58s 929us/step - loss: 0.0226 - acc: 0.4387\n",
      "Epoch 63/100\n",
      "62698/62698 [==============================] - 58s 930us/step - loss: 0.0226 - acc: 0.4388\n",
      "Epoch 64/100\n",
      "62698/62698 [==============================] - 58s 931us/step - loss: 0.0226 - acc: 0.4350\n",
      "Epoch 65/100\n",
      "62698/62698 [==============================] - 58s 929us/step - loss: 0.0225 - acc: 0.4419\n",
      "Epoch 66/100\n",
      "62698/62698 [==============================] - 58s 931us/step - loss: 0.0224 - acc: 0.4367\n",
      "Epoch 67/100\n",
      "62698/62698 [==============================] - 58s 930us/step - loss: 0.0224 - acc: 0.44342s - loss: 0.0224 - acc:  - ETA: 2s - loss: 0.0224 - - ETA\n",
      "Epoch 68/100\n",
      "62698/62698 [==============================] - 58s 931us/step - loss: 0.0223 - acc: 0.4353\n",
      "Epoch 69/100\n",
      "62698/62698 [==============================] - 59s 934us/step - loss: 0.0223 - acc: 0.4450\n",
      "Epoch 70/100\n",
      "62698/62698 [==============================] - 58s 932us/step - loss: 0.0223 - acc: 0.4394\n",
      "Epoch 71/100\n",
      "62698/62698 [==============================] - 58s 931us/step - loss: 0.0222 - acc: 0.43660s - loss: 0.0\n",
      "Epoch 72/100\n",
      "62698/62698 [==============================] - 58s 931us/step - loss: 0.0221 - acc: 0.4384\n",
      "Epoch 73/100\n",
      "62698/62698 [==============================] - 58s 932us/step - loss: 0.0221 - acc: 0.44010s - loss: 0.0221 - acc: 0\n",
      "Epoch 74/100\n",
      "62698/62698 [==============================] - 59s 933us/step - loss: 0.0219 - acc: 0.4408\n",
      "Epoch 75/100\n",
      "62698/62698 [==============================] - 58s 931us/step - loss: 0.0220 - acc: 0.4400\n",
      "Epoch 76/100\n",
      "62698/62698 [==============================] - 58s 922us/step - loss: 0.0219 - acc: 0.4418\n",
      "Epoch 77/100\n",
      "62698/62698 [==============================] - 58s 922us/step - loss: 0.0218 - acc: 0.44251s - loss: 0.0218 - ac - ETA: 0s - loss: 0.0218 \n",
      "Epoch 78/100\n",
      "62698/62698 [==============================] - 58s 922us/step - loss: 0.0218 - acc: 0.4400\n",
      "Epoch 79/100\n",
      "62698/62698 [==============================] - 58s 922us/step - loss: 0.0217 - acc: 0.4409\n",
      "Epoch 80/100\n",
      "62698/62698 [==============================] - 58s 921us/step - loss: 0.0217 - acc: 0.4428\n",
      "Epoch 81/100\n",
      "62698/62698 [==============================] - 58s 921us/step - loss: 0.0217 - acc: 0.4391\n",
      "Epoch 82/100\n",
      "62698/62698 [==============================] - 58s 921us/step - loss: 0.0216 - acc: 0.4368\n",
      "Epoch 83/100\n",
      "62698/62698 [==============================] - 58s 921us/step - loss: 0.0216 - acc: 0.4344\n",
      "Epoch 84/100\n",
      "62698/62698 [==============================] - 58s 922us/step - loss: 0.0215 - acc: 0.4392\n",
      "Epoch 85/100\n",
      "62698/62698 [==============================] - 58s 922us/step - loss: 0.0215 - acc: 0.43591s - loss - ETA: 0s - loss: 0.0215 - ac\n",
      "Epoch 86/100\n",
      "62698/62698 [==============================] - 58s 921us/step - loss: 0.0215 - acc: 0.4371\n",
      "Epoch 87/100\n",
      "62698/62698 [==============================] - 58s 922us/step - loss: 0.0214 - acc: 0.4407\n",
      "Epoch 88/100\n",
      "62698/62698 [==============================] - 58s 921us/step - loss: 0.0214 - acc: 0.44050s - loss: 0.0214 - acc: 0.4\n",
      "Epoch 89/100\n",
      "62698/62698 [==============================] - 58s 922us/step - loss: 0.0213 - acc: 0.4422\n",
      "Epoch 90/100\n",
      "62698/62698 [==============================] - 58s 923us/step - loss: 0.0214 - acc: 0.4434\n",
      "Epoch 91/100\n",
      "62698/62698 [==============================] - 58s 922us/step - loss: 0.0213 - acc: 0.4410\n",
      "Epoch 92/100\n",
      "62698/62698 [==============================] - 58s 922us/step - loss: 0.0212 - acc: 0.4387\n",
      "Epoch 93/100\n",
      "62698/62698 [==============================] - 58s 922us/step - loss: 0.0212 - acc: 0.4406\n",
      "Epoch 94/100\n",
      "62698/62698 [==============================] - 58s 922us/step - loss: 0.0212 - acc: 0.4400\n",
      "Epoch 95/100\n",
      "62698/62698 [==============================] - 58s 922us/step - loss: 0.0211 - acc: 0.4366\n",
      "Epoch 96/100\n",
      "62698/62698 [==============================] - 58s 922us/step - loss: 0.0211 - acc: 0.4429\n",
      "Epoch 97/100\n",
      "62698/62698 [==============================] - 58s 921us/step - loss: 0.0210 - acc: 0.4458\n",
      "Epoch 98/100\n",
      "62698/62698 [==============================] - 58s 921us/step - loss: 0.0210 - acc: 0.4434\n",
      "Epoch 99/100\n",
      "62698/62698 [==============================] - 58s 921us/step - loss: 0.0210 - acc: 0.4420\n",
      "Epoch 100/100\n",
      "62698/62698 [==============================] - 58s 923us/step - loss: 0.0210 - acc: 0.44440s - loss: 0.0210 - acc:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "56497"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_whole(0, 'weight-notes-1.hdf5')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "  import sys\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "Epoch 1/100\n",
      "62698/62698 [==============================] - 59s 945us/step - loss: 0.0754 - acc: 0.4544\n",
      "Epoch 2/100\n",
      "62698/62698 [==============================] - 57s 915us/step - loss: 0.0514 - acc: 0.5000\n",
      "Epoch 3/100\n",
      "62698/62698 [==============================] - 57s 914us/step - loss: 0.0449 - acc: 0.4936\n",
      "Epoch 4/100\n",
      "62698/62698 [==============================] - 57s 914us/step - loss: 0.0425 - acc: 0.4878\n",
      "Epoch 5/100\n",
      "62698/62698 [==============================] - 57s 915us/step - loss: 0.0408 - acc: 0.4914\n",
      "Epoch 6/100\n",
      "62698/62698 [==============================] - 57s 916us/step - loss: 0.0398 - acc: 0.4952\n",
      "Epoch 7/100\n",
      "62698/62698 [==============================] - 57s 914us/step - loss: 0.0388 - acc: 0.5016\n",
      "Epoch 8/100\n",
      "62698/62698 [==============================] - 57s 915us/step - loss: 0.0383 - acc: 0.50081s - loss: \n",
      "Epoch 9/100\n",
      "62698/62698 [==============================] - 57s 915us/step - loss: 0.0379 - acc: 0.5013\n",
      "Epoch 10/100\n",
      "62698/62698 [==============================] - 57s 915us/step - loss: 0.0372 - acc: 0.5022\n",
      "Epoch 11/100\n",
      "62698/62698 [==============================] - 57s 915us/step - loss: 0.0368 - acc: 0.50340s - loss: 0.0368 - ac\n",
      "Epoch 12/100\n",
      "62698/62698 [==============================] - 57s 915us/step - loss: 0.0364 - acc: 0.5112\n",
      "Epoch 13/100\n",
      "62698/62698 [==============================] - 57s 915us/step - loss: 0.0361 - acc: 0.5061\n",
      "Epoch 14/100\n",
      "62698/62698 [==============================] - 57s 916us/step - loss: 0.0356 - acc: 0.5022\n",
      "Epoch 15/100\n",
      "62698/62698 [==============================] - 57s 916us/step - loss: 0.0354 - acc: 0.5144\n",
      "Epoch 16/100\n",
      "62698/62698 [==============================] - 57s 916us/step - loss: 0.0352 - acc: 0.5066\n",
      "Epoch 17/100\n",
      "62698/62698 [==============================] - 57s 916us/step - loss: 0.0350 - acc: 0.5082\n",
      "Epoch 18/100\n",
      "62698/62698 [==============================] - 57s 915us/step - loss: 0.0347 - acc: 0.51151s\n",
      "Epoch 19/100\n",
      "62698/62698 [==============================] - 57s 916us/step - loss: 0.0344 - acc: 0.5214\n",
      "Epoch 20/100\n",
      "62698/62698 [==============================] - 57s 915us/step - loss: 0.0342 - acc: 0.5177\n",
      "Epoch 21/100\n",
      "62698/62698 [==============================] - 57s 915us/step - loss: 0.0338 - acc: 0.5275\n",
      "Epoch 22/100\n",
      "62698/62698 [==============================] - 57s 916us/step - loss: 0.0339 - acc: 0.5251\n",
      "Epoch 23/100\n",
      "62698/62698 [==============================] - 57s 915us/step - loss: 0.0336 - acc: 0.5255\n",
      "Epoch 24/100\n",
      "62698/62698 [==============================] - 57s 916us/step - loss: 0.0335 - acc: 0.5343\n",
      "Epoch 25/100\n",
      "62698/62698 [==============================] - 57s 914us/step - loss: 0.0334 - acc: 0.5348\n",
      "Epoch 26/100\n",
      "62698/62698 [==============================] - 57s 915us/step - loss: 0.0335 - acc: 0.5399\n",
      "Epoch 27/100\n",
      "62698/62698 [==============================] - 57s 914us/step - loss: 0.0330 - acc: 0.5420\n",
      "Epoch 28/100\n",
      "62698/62698 [==============================] - 57s 915us/step - loss: 0.0329 - acc: 0.5431\n",
      "Epoch 29/100\n",
      "62698/62698 [==============================] - 57s 915us/step - loss: 0.0329 - acc: 0.5540\n",
      "Epoch 30/100\n",
      "62698/62698 [==============================] - 57s 914us/step - loss: 0.0328 - acc: 0.5519\n",
      "Epoch 31/100\n",
      "62698/62698 [==============================] - 57s 914us/step - loss: 0.0326 - acc: 0.5448\n",
      "Epoch 32/100\n",
      "62698/62698 [==============================] - 57s 914us/step - loss: 0.0325 - acc: 0.5393\n",
      "Epoch 33/100\n",
      "62698/62698 [==============================] - 57s 915us/step - loss: 0.0324 - acc: 0.5461\n",
      "Epoch 34/100\n",
      "62698/62698 [==============================] - 57s 914us/step - loss: 0.0324 - acc: 0.5571\n",
      "Epoch 35/100\n",
      "62698/62698 [==============================] - 57s 914us/step - loss: 0.0320 - acc: 0.5643\n",
      "Epoch 36/100\n",
      "62698/62698 [==============================] - 57s 913us/step - loss: 0.0323 - acc: 0.5550\n",
      "Epoch 37/100\n",
      "62698/62698 [==============================] - 57s 914us/step - loss: 0.0319 - acc: 0.5577\n",
      "Epoch 38/100\n",
      "62698/62698 [==============================] - 57s 914us/step - loss: 0.0321 - acc: 0.5669\n",
      "Epoch 39/100\n",
      "62698/62698 [==============================] - 57s 914us/step - loss: 0.0318 - acc: 0.5659\n",
      "Epoch 40/100\n",
      "62698/62698 [==============================] - 57s 907us/step - loss: 0.0319 - acc: 0.5663\n",
      "Epoch 41/100\n",
      "62698/62698 [==============================] - 56s 894us/step - loss: 0.0316 - acc: 0.5634\n",
      "Epoch 42/100\n",
      "62698/62698 [==============================] - 56s 894us/step - loss: 0.0316 - acc: 0.5641\n",
      "Epoch 43/100\n",
      "62698/62698 [==============================] - 56s 893us/step - loss: 0.0315 - acc: 0.5592\n",
      "Epoch 44/100\n",
      "62698/62698 [==============================] - 56s 894us/step - loss: 0.0314 - acc: 0.5673\n",
      "Epoch 45/100\n",
      "62698/62698 [==============================] - 56s 894us/step - loss: 0.0316 - acc: 0.5704\n",
      "Epoch 46/100\n",
      "62698/62698 [==============================] - 56s 894us/step - loss: 0.0313 - acc: 0.5678\n",
      "Epoch 47/100\n",
      "62698/62698 [==============================] - 56s 894us/step - loss: 0.0314 - acc: 0.5742\n",
      "Epoch 48/100\n",
      "62698/62698 [==============================] - 57s 914us/step - loss: 0.0311 - acc: 0.5762\n",
      "Epoch 49/100\n",
      "62698/62698 [==============================] - 57s 911us/step - loss: 0.0315 - acc: 0.5918\n",
      "Epoch 50/100\n",
      "62698/62698 [==============================] - 57s 913us/step - loss: 0.0310 - acc: 0.5924\n",
      "Epoch 51/100\n",
      "62698/62698 [==============================] - 57s 913us/step - loss: 0.0313 - acc: 0.5988\n",
      "Epoch 52/100\n",
      "62698/62698 [==============================] - 57s 913us/step - loss: 0.0310 - acc: 0.5920\n",
      "Epoch 53/100\n",
      "62698/62698 [==============================] - 57s 913us/step - loss: 0.0311 - acc: 0.5965\n",
      "Epoch 54/100\n",
      "62698/62698 [==============================] - 57s 914us/step - loss: 0.0311 - acc: 0.5986\n",
      "Epoch 55/100\n",
      "62698/62698 [==============================] - 57s 914us/step - loss: 0.0309 - acc: 0.6009\n",
      "Epoch 56/100\n",
      "62698/62698 [==============================] - 58s 921us/step - loss: 0.0310 - acc: 0.60871s - loss: 0 - ETA: 0s - loss: 0.0310 -\n",
      "Epoch 57/100\n",
      "62698/62698 [==============================] - 58s 921us/step - loss: 0.0309 - acc: 0.6047\n",
      "Epoch 58/100\n",
      "62698/62698 [==============================] - 57s 913us/step - loss: 0.0310 - acc: 0.6040\n",
      "Epoch 59/100\n",
      "62698/62698 [==============================] - 57s 913us/step - loss: 0.0308 - acc: 0.61391s - loss: 0\n",
      "Epoch 60/100\n",
      "62698/62698 [==============================] - 57s 913us/step - loss: 0.0309 - acc: 0.6203\n",
      "Epoch 61/100\n",
      "62698/62698 [==============================] - 57s 914us/step - loss: 0.0309 - acc: 0.6221\n",
      "Epoch 62/100\n",
      "62698/62698 [==============================] - 57s 914us/step - loss: 0.0311 - acc: 0.6294\n",
      "Epoch 63/100\n",
      "62698/62698 [==============================] - 57s 914us/step - loss: 0.0307 - acc: 0.6242\n",
      "Epoch 64/100\n",
      "62698/62698 [==============================] - 57s 913us/step - loss: 0.0308 - acc: 0.6263\n",
      "Epoch 65/100\n",
      "62698/62698 [==============================] - 57s 914us/step - loss: 0.0306 - acc: 0.6237\n",
      "Epoch 66/100\n",
      "62698/62698 [==============================] - 57s 914us/step - loss: 0.0305 - acc: 0.6295\n",
      "Epoch 67/100\n",
      "62698/62698 [==============================] - 57s 915us/step - loss: 0.0305 - acc: 0.63321s - loss\n",
      "Epoch 68/100\n",
      "62698/62698 [==============================] - 57s 914us/step - loss: 0.0306 - acc: 0.6320\n",
      "Epoch 69/100\n",
      "62698/62698 [==============================] - 57s 917us/step - loss: 0.0306 - acc: 0.6377\n",
      "Epoch 70/100\n",
      "62698/62698 [==============================] - 57s 914us/step - loss: 0.0306 - acc: 0.6411\n",
      "Epoch 71/100\n",
      "62698/62698 [==============================] - 57s 915us/step - loss: 0.0306 - acc: 0.6483\n",
      "Epoch 72/100\n",
      "62698/62698 [==============================] - 57s 914us/step - loss: 0.0306 - acc: 0.6504\n",
      "Epoch 73/100\n",
      "62698/62698 [==============================] - 57s 914us/step - loss: 0.0308 - acc: 0.6512\n",
      "Epoch 74/100\n",
      "62698/62698 [==============================] - 57s 914us/step - loss: 0.0308 - acc: 0.6576\n",
      "Epoch 75/100\n",
      "62698/62698 [==============================] - 57s 915us/step - loss: 0.0307 - acc: 0.6568\n",
      "Epoch 76/100\n",
      "62698/62698 [==============================] - 57s 914us/step - loss: 0.0308 - acc: 0.6581\n",
      "Epoch 77/100\n",
      "62698/62698 [==============================] - 57s 915us/step - loss: 0.0308 - acc: 0.6638\n",
      "Epoch 78/100\n",
      "62698/62698 [==============================] - 57s 912us/step - loss: 0.0309 - acc: 0.6714\n",
      "Epoch 79/100\n",
      "62698/62698 [==============================] - 57s 908us/step - loss: 0.0310 - acc: 0.6706\n",
      "Epoch 80/100\n",
      "62698/62698 [==============================] - 56s 894us/step - loss: 0.0310 - acc: 0.6750\n",
      "Epoch 81/100\n",
      "62698/62698 [==============================] - 56s 893us/step - loss: 0.0312 - acc: 0.6723\n",
      "Epoch 82/100\n",
      "62698/62698 [==============================] - 56s 894us/step - loss: 0.0312 - acc: 0.6827\n",
      "Epoch 83/100\n",
      "62698/62698 [==============================] - 56s 893us/step - loss: 0.0311 - acc: 0.6867\n",
      "Epoch 84/100\n",
      "62698/62698 [==============================] - 56s 893us/step - loss: 0.0310 - acc: 0.6854\n",
      "Epoch 85/100\n",
      "62698/62698 [==============================] - 56s 893us/step - loss: 0.0313 - acc: 0.6787\n",
      "Epoch 86/100\n",
      "62698/62698 [==============================] - 56s 894us/step - loss: 0.0315 - acc: 0.6930\n",
      "Epoch 87/100\n",
      "62698/62698 [==============================] - 56s 893us/step - loss: 0.0316 - acc: 0.6933\n",
      "Epoch 88/100\n",
      "62698/62698 [==============================] - 56s 894us/step - loss: 0.0318 - acc: 0.6972\n",
      "Epoch 89/100\n",
      "62698/62698 [==============================] - 56s 893us/step - loss: 0.0317 - acc: 0.7072\n",
      "Epoch 90/100\n",
      "62698/62698 [==============================] - 56s 895us/step - loss: 0.0318 - acc: 0.7151\n",
      "Epoch 91/100\n",
      "62698/62698 [==============================] - 56s 893us/step - loss: 0.0319 - acc: 0.7141\n",
      "Epoch 92/100\n",
      "62698/62698 [==============================] - 56s 893us/step - loss: 0.0321 - acc: 0.7194\n",
      "Epoch 93/100\n",
      "62698/62698 [==============================] - 56s 893us/step - loss: 0.0325 - acc: 0.7167\n",
      "Epoch 94/100\n",
      "62698/62698 [==============================] - 56s 893us/step - loss: 0.0327 - acc: 0.7213\n",
      "Epoch 95/100\n",
      "62698/62698 [==============================] - 56s 893us/step - loss: 0.0326 - acc: 0.7267\n",
      "Epoch 96/100\n",
      "62698/62698 [==============================] - 61s 968us/step - loss: 0.0330 - acc: 0.7331\n",
      "Epoch 97/100\n",
      "62698/62698 [==============================] - 56s 893us/step - loss: 0.0336 - acc: 0.7454\n",
      "Epoch 98/100\n",
      "62698/62698 [==============================] - 56s 894us/step - loss: 0.0344 - acc: 0.7506\n",
      "Epoch 99/100\n",
      "62698/62698 [==============================] - 56s 893us/step - loss: 0.0345 - acc: 0.7613\n",
      "Epoch 100/100\n",
      "62698/62698 [==============================] - 56s 893us/step - loss: 0.0358 - acc: 0.7691\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam = keras.optimizers.Adam(lr=0.01)\n",
    "train_test_whole(0, 'weight-notes-2.hdf5', optimizer=adam)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = keras.optimizers.Adam(lr=0.01)\n",
    "train_test_whole(0, 'weight-notes-200.hdf5', optimizer=adam)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "  import sys\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135.32289028167725\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-137-91a3197bf912>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_test_whole\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'weight-notes-1.hdf5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-136-abb7c1be6bff>\u001b[0m in \u001b[0;36mtrain_test_whole\u001b[1;34m(num, name, epochs, batchsize)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mlstm_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mlstm_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlstm_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mcnn_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnn_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mcnn_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnn_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnn_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m108\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m108\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mappend\u001b[1;34m(arr, values, axis)\u001b[0m\n\u001b[0;32m   5164\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5165\u001b[0m         \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5166\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_test_whole(0, 'weight-notes-1.hdf5')\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
