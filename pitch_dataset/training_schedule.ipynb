{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hui-Kang Tong admin, [2 Apr 2018 at 19:31:29]:\n",
    "ok we can now automatically generate midi files, convert midi files to wave files (with hkâ€™s mac)\n",
    "\n",
    "However to create training data, we need many data (audio samples) with the same target/label (note). So these are a few ways to vary it:\n",
    "- note velocity\n",
    "- note volume\n",
    "- noise\n",
    "- onset timing (one slightly later than the other)\n",
    "- soundfont (which mean instrument)\n",
    "\n",
    "So I will write a notebook which\n",
    "- leave a space for the function to create .mid file with note\n",
    "- use fluidsynth to convert this .mid file .wav\n",
    "- call this .mid file, identify onset with our onset function, and splice 4096 entries from it\n",
    "- leave a space for the training function\n",
    "- delete the .mid file and .wav file\n",
    "- repeat\n",
    "\n",
    "What needs to be considered/written\n",
    "what kind of model are we using, how do we train such a model, what sort of instructions does the .mid files have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from midiutil.MidiFile import MIDIFile\n",
    "from itertools import combinations\n",
    "import os\n",
    "import subprocess\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "\n",
    "scales = range(3,4) #C1 to C4\n",
    "no_of_notes = range(2, 5) #2 notes to 4 notes\n",
    "\n",
    "channel = 0\n",
    "time = 0.5\n",
    "duration = 2\n",
    "tempo = 100\n",
    "volume = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_onset(signal, chunksize=2048, tempo_res=32, plotting=False):\n",
    "    '''\n",
    "    # this is to be called at the end of every chunk (2048 entries), starting from 4th chunk\n",
    "    # the input to this function is four chunks\n",
    "    These are never likely to be changed, maybe? : chunksize=2048, tempo_res=32\n",
    "\n",
    "    TODO: make become on the fly\n",
    "    '''\n",
    "    # default value of onset, overwritten if onset is detected\n",
    "    onset = -1\n",
    "    assert len(signal) == 4*chunksize\n",
    "\n",
    "    # to detect difference\n",
    "    difference = np.cumsum(np.add(np.absolute(signal[chunksize:-chunksize]), -np.absolute(signal[:-2 * chunksize])))\n",
    "\n",
    "    # white noise is added the the difference\n",
    "    # this is to desensitise detection of low amplitdue onset sounds\n",
    "    noise = 10 * np.array(np.random.randn(len(difference)))\n",
    "    difference = np.add(difference, noise)\n",
    "\n",
    "    # calculation of r-coefficient\n",
    "    # -1 is negatively correlated\n",
    "    # +1 is positively correlated\n",
    "    # onset is when r-coefficient cuts above 0.8\n",
    "    roceff = np.full(tempo_res, 0.)\n",
    "    tempo_num = int(chunksize / tempo_res)\n",
    "    for i in range(tempo_res):\n",
    "        roceff[i] = np.corrcoef(difference[i * tempo_num:(i * tempo_num + chunksize)],\n",
    "                                np.arange(chunksize))[0, 1]\n",
    "        # special case when i = 0, because we do not have the previous value\n",
    "        #         if i == 0 and roceff[0] > 0.8:\n",
    "        #             onset = i\n",
    "#         print(roceff[i])\n",
    "        if roceff[0] < 0.7 and roceff[i] > 0.7 and np.max(roceff[:i]) < 0.7:\n",
    "            onset = i\n",
    "\n",
    "    #     if onset != -1:\n",
    "    #         # clear output from jupyter\n",
    "    #         clear_output(wait=True)\n",
    "    #         plt.figure(figsize=(16,2))\n",
    "    #         plt.plot(np.arange(2048*1,2048*3),np.array(difference)/np.max(difference))\n",
    "    #         plt.plot(np.arange(2048*2,2048*3,64),roceff)\n",
    "    #         plt.plot(signal)\n",
    "    #         plt.axvline(x=2048*1+64*onset, color=\"r\")\n",
    "    #         plt.axvline(x=2048*3+64*onset, color=\"r\")\n",
    "    #         plt.show()\n",
    "    return onset  # none, or a value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cqt_function(signal_to_ayse, plotting=False):\n",
    "    '''\n",
    "    Compute the cqt response over a set of notes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    signal_to_ayse : input array, real\n",
    "                     4096 entries long\n",
    "\n",
    "    Todo\n",
    "    ----\n",
    "    We should not be generating the kernels every time.\n",
    "    Actually we could simply multiply with the FFT of the signal.\n",
    "    So:\n",
    "    Make it customisable to a random starting note.\n",
    "    Outsource the precomputation to a function instead - do something like load kernels.\n",
    "    Do both of these at the same time.\n",
    "    (but anyway we just want an accurate demo, computation time doesn't seem to be a problem here)\n",
    "    '''\n",
    "\n",
    "    length = len(signal_to_ayse)\n",
    "    # print(length)\n",
    "\n",
    "    # fast fourier transform\n",
    "    freq_domain = np.fft.fft(signal_to_ayse)\n",
    "    plt.plot(np.absolute(freq_domain[:400]))\n",
    "    plt.show()\n",
    "\n",
    "    # defining the 36 notes bins\n",
    "    bins = 40\n",
    "    freq_ref_notes = [261.625565 * (2. ** (n / 36. - 5. / 72.)) for n in range(bins)]\n",
    "\n",
    "    # defining the time kernel\n",
    "    # different note have different \"kernels\"\n",
    "    bell_curves = []\n",
    "    for note in range(len(freq_ref_notes)):\n",
    "        # I don't think this is defined properly?\n",
    "        bell_curve = np.exp(-((np.arange(-1., 1., 2. / length)) * (2. ** (note / 36.))) ** 2.)\n",
    "        bell_curves.append(bell_curve)\n",
    "\n",
    "    # multiplying the waveform with the window\n",
    "    kernels = []\n",
    "    for note in range(bins):\n",
    "        # generate cosine/sine wave\n",
    "        wave = np.exp((np.arange(length) - length / 2.) * -1.j * 2. * np.pi * freq_ref_notes[note] / 44100.)\n",
    "        # multiplying a window over it\n",
    "        kernels.append(np.multiply(wave, bell_curves[note]))\n",
    "\n",
    "    # taking fourier transform for the frequency kernel\n",
    "    fft_kernels = []\n",
    "    for note in range(bins):\n",
    "        fft_kernels.append(np.fft.fft(kernels[note]))\n",
    "    # all of the above should be run only once\n",
    "    # but the maximum complexity is only the FFT, it is still not a problem\n",
    "    # the focus is an MVP anyway\n",
    "    # we prioritse accuracy here though\n",
    "    # creating arrays to populate for each one-third-semitone\n",
    "    cqt_resp_specs = []\n",
    "    cqt_resp = []\n",
    "\n",
    "    # populating the arrays\n",
    "    for note in range(bins):\n",
    "        cqt_resp_spec = []\n",
    "        for entry in range(length):\n",
    "            cqt_resp_spec.append(fft_kernels[note][entry] * freq_domain[entry])\n",
    "        cqt_resp_specs.append(cqt_resp_spec)\n",
    "        cqt_resp.append(sum([abs(x) for x in cqt_resp_spec]))\n",
    "\n",
    "    # finding peaks in the cqt response\n",
    "    notesrum = cqt_resp  # remove redundant variable please\n",
    "\n",
    "    #     plt.figure(figsize=(16,2))\n",
    "    #     plt.plot(np.linspace(0, 70*44100/4096, num=70, endpoint=False),\n",
    "    #              np.absolute(freq_domain[:70])/np.max(np.absolute(freq_domain[:70])))\n",
    "    #     plt.plot(np.geomspace(261.625565*2**(-3/36), 261.625565*2**(37/36), num=40, endpoint=False),\n",
    "    #              np.absolute(cqt_resp)/np.max(np.absolute(cqt_resp)))\n",
    "    #     plt.show()\n",
    "\n",
    "    notesrum_peak_only = [0.0] * len(notesrum)\n",
    "    notesrum_sum = sum(notesrum)\n",
    "\n",
    "    for index in range(bins - 1)[1:]:\n",
    "        if notesrum[index - 1] < notesrum[index] and notesrum[index + 1] < notesrum[index]:\n",
    "            notesrum_peak_only[index] = notesrum[index]\n",
    "\n",
    "        # known_octave = notesrum_peak_only[12:12+36] # don't know what is this for\n",
    "    known_octave = notesrum_peak_only[2:-2]\n",
    "    # print(np.round(known_octave,5)/notesrum_sum)\n",
    "\n",
    "    notesrum_peak_only_sum = sum(notesrum_peak_only)\n",
    "\n",
    "    for x in range(36):\n",
    "        # if known_octave[x]/notesrum_sum < 0.1:\n",
    "        if known_octave[x] / notesrum_peak_only_sum < 0.2:\n",
    "            known_octave[x] = 0\n",
    "\n",
    "    known_octave_notes = []\n",
    "    for notes in range(36 // 3):\n",
    "        known_octave_notes.append(known_octave[3 * notes]\n",
    "                                  + known_octave[3 * notes + 1]\n",
    "                                  + known_octave[3 * notes + 2])\n",
    "\n",
    "    # notestrum_sum = sum(notesrum)  # alternate demoninator to calc threshold\n",
    "    # print(np.round(known_octave_notes,5)/notesrum_peak_only_sum)\n",
    "    # print(\"check\")\n",
    "\n",
    "    notesrum_peak_only_sum = sum(notesrum_peak_only)\n",
    "\n",
    "    output = []\n",
    "\n",
    "    for x in range(12):\n",
    "        if known_octave_notes[x] / notesrum_peak_only_sum > 0.1:\n",
    "            output.append(x + 1)\n",
    "\n",
    "    #     plt.figure(figsize=(16,2))\n",
    "    #     plt.plot(notesrum_peak_only)\n",
    "    #     plt.plot(notesrum)\n",
    "    #     plt.axhline(y=0.1*notesrum_peak_only_sum, color='r', linestyle='-')\n",
    "    #     plt.show()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from midiutil.MidiFile import MIDIFile\n",
    "from itertools import combinations\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "\n",
    "\n",
    "# Yield successive n-sized\n",
    "# chunks from l.\n",
    "def divide_chunks(l, n):\n",
    "     \n",
    "    # looping till length l\n",
    "    for i in range(0, len(l), n): \n",
    "        yield l[i:i + n]\n",
    " \n",
    "# How many elements each\n",
    "# list should have\n",
    "n = 5\n",
    " \n",
    "\n",
    "\n",
    "# def chunks(chunkable, n):\n",
    "#     \"\"\" Yield successive n-sized chunks from l.\"\"\"\n",
    "#     for i in xrange(0, len(chunkable), n):\n",
    "#         yield chunkable[i:i+n]\n",
    "\n",
    "def generate_and_train(no_,track,com,channel=0):\n",
    "# for each cycle\n",
    "\n",
    "    # generate and save midifile\n",
    "    mf = MIDIFile(no_)\n",
    "\n",
    "    for num in range(no_):\n",
    "        mf.addNote(track[num], channel, com[num], time, duration, volume)\n",
    "\n",
    "    code = []\n",
    "    for num in com:\n",
    "        code.append(num)\n",
    "    while len(code) < 4:\n",
    "        code.append(\"x\")\n",
    "\n",
    "    filename = \"C{}_{}_{}_{}_{}\".format(scale, code[0], code[1], code[2], code[3])\n",
    "    with open(\"midifile_sch/{}.mid\".format(filename), \"wb\") as outf:\n",
    "        mf.writeFile(outf)\n",
    "\n",
    "    # generate sound\n",
    "    subprocess.run(\"fluidsynth -F soundfile_sch/{}.wav ~/mgen/Sonatina_Symphonic_Orchestra.sf2 midifile_sch/{}.mid\".format(filename,filename),shell=True)\n",
    "    sleep(1)\n",
    "    subprocess.run(\"rm midifile_sch/{}.mid\".format(filename),shell=True) # remove midifile\n",
    "    \n",
    "    audio, sr = sf.read(\"soundfile_sch/{}.wav\".format(filename))\n",
    "    \n",
    "#     print(len(audio[0]))  # length of audio file\n",
    "    chunk_array = list(divide_chunks(audio[:,0], 2048))\n",
    "#     print(len(audio[:,0]))\n",
    "#     print(len(chunk_array))\n",
    "#     print(audio[:,0])\n",
    "    plt.plot(audio[:,0])\n",
    "    plt.show()\n",
    "    \n",
    "    sd.play(audio[:,0], 44100)\n",
    "    detected = -1\n",
    "    position = 0\n",
    "    for i in range(len(chunk_array)-5):\n",
    "        detected = detect_onset(np.concatenate(chunk_array[i:i+4]))\n",
    "        print(detected)\n",
    "        if detected != -1:\n",
    "            position = i\n",
    "            break\n",
    "            \n",
    "    signal_to_ayse = audio[i+2048*position:,0]\n",
    "    result = cqt_function(signal_to_ayse)\n",
    "    print(result)\n",
    "\n",
    "    # CALL SOME TRAINING FUNCTION HERE, UPDATE THE MODEL\n",
    "    sleep(1)\n",
    "    subprocess.run(\"detect_onsetrm soundfile_sch/{}.wav\".format(filename),shell=True) # remove soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# can loop in other ways or random\n",
    "for scale in scales:\n",
    "    for no_ in no_of_notes:\n",
    "        \n",
    "        pitch = range(12*(scale+1), 12*(scale+2))\n",
    "        comb = [x for x in combinations(pitch, no_)]\n",
    "        track = range(no_)\n",
    "        \n",
    "        count = 0    \n",
    "        for com in comb:\n",
    "            generate_and_train(no_,track,com,channel=channel)\n",
    "            \n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mir]",
   "language": "python",
   "name": "conda-env-mir-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
