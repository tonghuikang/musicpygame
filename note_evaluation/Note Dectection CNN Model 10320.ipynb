{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras import callbacks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor, log10\n",
    "def dig(number, dig=2):\n",
    "    return round(number, dig - floor(log10(abs(number))) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_row = 108\n",
    "img_col = 108\n",
    "path = \"E:/database108/\"\n",
    "batch_size = 64\n",
    "output_classes = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "x_train_ori = []\n",
    "y_train_ori = []\n",
    "x_test_ori = []\n",
    "y_test_ori = []\n",
    "for i in range(516):\n",
    "    print(i)\n",
    "    file = pd.read_csv(path + \"database_{}.csv\".format(i), dtype=str)\n",
    "    array = file.values\n",
    "    array = list(array)\n",
    "    train_test = random.sample(range(100), 25)\n",
    "    \n",
    "    for j in train_test[0:20]:\n",
    "        sample = list(array[j])\n",
    "        lst = []\n",
    "        for x in sample[1:11665]:\n",
    "            if type(x) == str:\n",
    "                x = \"\".join(x.split())\n",
    "                lst.append(float(x))\n",
    "            elif type(x) == float:\n",
    "                lst.append(x)\n",
    "            else:\n",
    "                print(type(x))\n",
    "        x_train_ori.append(lst)\n",
    "        y_train_ori.append(sample[0])\n",
    "        \n",
    "    for l in train_test[20:]:\n",
    "        sample = list(array[l])\n",
    "        lst = []\n",
    "        for x in sample[1:11665]:\n",
    "            if type(x) == str:\n",
    "                x = \"\".join(x.split())\n",
    "                lst.append(float(x))\n",
    "            elif type(x) == float:\n",
    "                lst.append(x)\n",
    "            else:\n",
    "                print(type(x))\n",
    "        x_test_ori.append(lst)\n",
    "        y_test_ori.append(sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10320, 11664)\n",
      "(2580, 11664)\n",
      "(10320,)\n",
      "(2580,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(x_train_ori))\n",
    "print(np.shape(x_test_ori))\n",
    "print(np.shape(y_train_ori))\n",
    "print(np.shape(y_test_ori))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_convert(array, flag=0):\n",
    "    y = []\n",
    "    for i in array: # i is the name of each sample:\n",
    "        name = i.split(\"_\")\n",
    "        name = name[1:5]\n",
    "        if flag == 0:\n",
    "            # category of appearance of a note\n",
    "            category = np.zeros(24, dtype=int)\n",
    "            for j in name:\n",
    "                if j != \"x\":\n",
    "                    num = int(j)\n",
    "                    category[num-60] = 1\n",
    "            y.append(list(category))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "test = [\"C_60_61_65_x\", \"C_67_76_78_79\"]\n",
    "print(y_convert(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_1 = []\n",
    "lst_2 = []\n",
    "\n",
    "for i in y_train_ori:\n",
    "    label = \"\".join(i.split())\n",
    "    lst_1.append(label)\n",
    "for j in y_test_ori:\n",
    "    label = \"\".join(j.split())\n",
    "    lst_2.append(label)\n",
    "    \n",
    "y_train_ori = y_convert(lst_1)\n",
    "y_test_ori = y_convert(lst_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10320, 24)\n",
      "(2580, 24)\n"
     ]
    }
   ],
   "source": [
    "y_train_ori = np.array(y_train_ori)\n",
    "y_test_ori = np.array(y_test_ori)\n",
    "x_train_ori = np.array(x_train_ori)\n",
    "x_test_ori = np.array(x_test_ori)\n",
    "\n",
    "print(y_train_ori.shape)\n",
    "print(y_test_ori.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_training = 10320\n",
    "no_test = 2580\n",
    "x_train_ori = np.array(x_train_ori).flatten()\n",
    "x_train_ori = x_train_ori.reshape(no_training, img_row, img_col, 1)\n",
    "x_test_ori = np.array(x_test_ori).flatten()\n",
    "x_test_ori = x_test_ori.reshape(no_test, img_row, img_col, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10320, 108, 108, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_ori.shape)\n",
    "input_shape = (108, 108, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_A ():    \n",
    "    early_stop = callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=3, verbose=1, mode=\"min\")\n",
    "    \n",
    "    filepath = \"Best-weights-my_model-{epoch:03d}-{loss:4f}-{acc:4f}.hdf5\"\n",
    "    checkpoint = callbacks.ModelCheckpoint(filepath, monitor=\"val_loss\", verbose=1, save_best_only= True, mode=\"min\")\n",
    "    \n",
    "    callbacks_list = [early_stop, checkpoint]\n",
    "    \n",
    "    #build the model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #first convolutional layer\n",
    "    model.add(Conv2D(32, (3,3), input_shape=input_shape, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    \n",
    "    #second convolutional layer\n",
    "    model.add(Conv2D(32, (3,3), activation=\"relu\", border_mode=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    \n",
    "    #third convolutional layer\n",
    "    model.add(Conv2D(64, (3,3), activation=\"relu\", border_mode = \"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    #first FC layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    #output layer\n",
    "    model.add(Dense(24, activation=\"softmax\", kernel_initializer=\"uniform\"))\n",
    "    \n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"SGD\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10320 samples, validate on 2580 samples\n",
      "Epoch 1/50\n",
      "10320/10320 [==============================] - 7s 675us/step - loss: 9.4651 - acc: 0.3154 - val_loss: 8.2796 - val_acc: 0.4023\n",
      "Epoch 2/50\n",
      "10320/10320 [==============================] - 6s 604us/step - loss: 7.1168 - acc: 0.3332 - val_loss: 6.7068 - val_acc: 0.2857\n",
      "Epoch 3/50\n",
      "10320/10320 [==============================] - 6s 602us/step - loss: 6.5351 - acc: 0.2939 - val_loss: 6.3834 - val_acc: 0.1795\n",
      "Epoch 4/50\n",
      "10320/10320 [==============================] - 6s 605us/step - loss: 6.3285 - acc: 0.2922 - val_loss: 6.0262 - val_acc: 0.2694\n",
      "Epoch 5/50\n",
      "10320/10320 [==============================] - 6s 606us/step - loss: 6.2187 - acc: 0.2841 - val_loss: 5.7460 - val_acc: 0.2322\n",
      "Epoch 6/50\n",
      "10320/10320 [==============================] - 6s 601us/step - loss: 6.1511 - acc: 0.2830 - val_loss: 5.6148 - val_acc: 0.2353\n",
      "Epoch 7/50\n",
      "10320/10320 [==============================] - 6s 607us/step - loss: 6.0970 - acc: 0.2807 - val_loss: 5.6133 - val_acc: 0.2853\n",
      "Epoch 8/50\n",
      "10320/10320 [==============================] - 6s 612us/step - loss: 6.0688 - acc: 0.2797 - val_loss: 5.4976 - val_acc: 0.2640\n",
      "Epoch 9/50\n",
      "10320/10320 [==============================] - 6s 608us/step - loss: 6.0236 - acc: 0.2763 - val_loss: 5.5633 - val_acc: 0.2822\n",
      "Epoch 10/50\n",
      "10320/10320 [==============================] - 6s 600us/step - loss: 5.9969 - acc: 0.2734 - val_loss: 5.6141 - val_acc: 0.2531\n",
      "Epoch 11/50\n",
      "10320/10320 [==============================] - 6s 598us/step - loss: 5.9772 - acc: 0.2834 - val_loss: 5.4514 - val_acc: 0.2329\n",
      "Epoch 12/50\n",
      "10320/10320 [==============================] - 6s 603us/step - loss: 5.9531 - acc: 0.2734 - val_loss: 5.4842 - val_acc: 0.2070\n",
      "Epoch 13/50\n",
      "10320/10320 [==============================] - 6s 606us/step - loss: 5.9425 - acc: 0.2740 - val_loss: 5.4710 - val_acc: 0.3275\n",
      "Epoch 14/50\n",
      "10320/10320 [==============================] - 6s 600us/step - loss: 5.9225 - acc: 0.2732 - val_loss: 5.5053 - val_acc: 0.2535\n",
      "Epoch 15/50\n",
      "10320/10320 [==============================] - 6s 599us/step - loss: 5.9042 - acc: 0.2778 - val_loss: 5.4668 - val_acc: 0.2961\n",
      "Epoch 16/50\n",
      "10320/10320 [==============================] - 6s 601us/step - loss: 5.8976 - acc: 0.2766 - val_loss: 5.5499 - val_acc: 0.2415\n",
      "Epoch 17/50\n",
      "10320/10320 [==============================] - 6s 600us/step - loss: 5.8862 - acc: 0.2685 - val_loss: 5.5312 - val_acc: 0.2469\n",
      "Epoch 18/50\n",
      "10320/10320 [==============================] - 6s 604us/step - loss: 5.8617 - acc: 0.2761 - val_loss: 5.4271 - val_acc: 0.2822\n",
      "Epoch 19/50\n",
      "10320/10320 [==============================] - 6s 600us/step - loss: 5.8541 - acc: 0.2705 - val_loss: 5.4759 - val_acc: 0.2457\n",
      "Epoch 20/50\n",
      "10320/10320 [==============================] - 6s 600us/step - loss: 5.8442 - acc: 0.2745 - val_loss: 5.5036 - val_acc: 0.2740\n",
      "Epoch 21/50\n",
      "10320/10320 [==============================] - 6s 600us/step - loss: 5.8334 - acc: 0.2719 - val_loss: 5.4795 - val_acc: 0.2256\n",
      "Epoch 22/50\n",
      "10320/10320 [==============================] - 6s 602us/step - loss: 5.8241 - acc: 0.2704 - val_loss: 5.4813 - val_acc: 0.2837\n",
      "Epoch 23/50\n",
      "10320/10320 [==============================] - 6s 600us/step - loss: 5.8115 - acc: 0.2670 - val_loss: 5.5017 - val_acc: 0.2628\n",
      "Epoch 24/50\n",
      "10320/10320 [==============================] - 6s 600us/step - loss: 5.8083 - acc: 0.2664 - val_loss: 5.4125 - val_acc: 0.1957\n",
      "Epoch 25/50\n",
      "10320/10320 [==============================] - 6s 600us/step - loss: 5.7949 - acc: 0.2744 - val_loss: 5.3966 - val_acc: 0.2345\n",
      "Epoch 26/50\n",
      "10320/10320 [==============================] - 6s 600us/step - loss: 5.7938 - acc: 0.2701 - val_loss: 5.4119 - val_acc: 0.2795\n",
      "Epoch 27/50\n",
      "10320/10320 [==============================] - 6s 600us/step - loss: 5.7834 - acc: 0.2637 - val_loss: 5.3798 - val_acc: 0.2271\n",
      "Epoch 28/50\n",
      "10320/10320 [==============================] - 6s 601us/step - loss: 5.7742 - acc: 0.2672 - val_loss: 5.4438 - val_acc: 0.2174\n",
      "Epoch 29/50\n",
      "10320/10320 [==============================] - 6s 601us/step - loss: 5.7643 - acc: 0.2658 - val_loss: 5.4216 - val_acc: 0.2609\n",
      "Epoch 30/50\n",
      "10320/10320 [==============================] - 6s 606us/step - loss: 5.7619 - acc: 0.2792 - val_loss: 5.3816 - val_acc: 0.1915\n",
      "Epoch 31/50\n",
      "10320/10320 [==============================] - 6s 608us/step - loss: 5.7525 - acc: 0.2713 - val_loss: 5.4466 - val_acc: 0.2911\n",
      "Epoch 32/50\n",
      "10320/10320 [==============================] - 6s 606us/step - loss: 5.7573 - acc: 0.2647 - val_loss: 5.4019 - val_acc: 0.2934\n",
      "Epoch 33/50\n",
      "10320/10320 [==============================] - 6s 615us/step - loss: 5.7472 - acc: 0.2708 - val_loss: 5.4253 - val_acc: 0.3143\n",
      "Epoch 34/50\n",
      "10320/10320 [==============================] - 6s 601us/step - loss: 5.7445 - acc: 0.2690 - val_loss: 5.3692 - val_acc: 0.2097\n",
      "Epoch 35/50\n",
      "10320/10320 [==============================] - 6s 600us/step - loss: 5.7354 - acc: 0.2695 - val_loss: 5.4189 - val_acc: 0.2318\n",
      "Epoch 36/50\n",
      "10320/10320 [==============================] - 6s 599us/step - loss: 5.7297 - acc: 0.2668 - val_loss: 5.3707 - val_acc: 0.2535\n",
      "Epoch 37/50\n",
      "10320/10320 [==============================] - 6s 600us/step - loss: 5.7308 - acc: 0.2687 - val_loss: 5.3612 - val_acc: 0.2019\n",
      "Epoch 38/50\n",
      "10320/10320 [==============================] - 6s 600us/step - loss: 5.7217 - acc: 0.2695 - val_loss: 5.3645 - val_acc: 0.3360\n",
      "Epoch 39/50\n",
      "10320/10320 [==============================] - 6s 600us/step - loss: 5.7128 - acc: 0.2711 - val_loss: 5.4685 - val_acc: 0.2337\n",
      "Epoch 40/50\n",
      "10320/10320 [==============================] - 6s 600us/step - loss: 5.7094 - acc: 0.2740 - val_loss: 5.3228 - val_acc: 0.2911\n",
      "Epoch 41/50\n",
      "10320/10320 [==============================] - 6s 600us/step - loss: 5.7117 - acc: 0.2746 - val_loss: 5.4480 - val_acc: 0.2453\n",
      "Epoch 42/50\n",
      "10320/10320 [==============================] - 6s 601us/step - loss: 5.7020 - acc: 0.2701 - val_loss: 5.3426 - val_acc: 0.2283\n",
      "Epoch 43/50\n",
      "10320/10320 [==============================] - 6s 600us/step - loss: 5.7026 - acc: 0.2666 - val_loss: 5.3850 - val_acc: 0.2473\n",
      "Epoch 44/50\n",
      "10320/10320 [==============================] - 6s 605us/step - loss: 5.6941 - acc: 0.2729 - val_loss: 5.4224 - val_acc: 0.2120\n",
      "Epoch 45/50\n",
      "10320/10320 [==============================] - 6s 607us/step - loss: 5.6884 - acc: 0.2720 - val_loss: 5.3578 - val_acc: 0.1791\n",
      "Epoch 46/50\n",
      "10320/10320 [==============================] - 6s 605us/step - loss: 5.6876 - acc: 0.2683 - val_loss: 5.3290 - val_acc: 0.2736\n",
      "Epoch 47/50\n",
      "10320/10320 [==============================] - 6s 608us/step - loss: 5.6826 - acc: 0.2715 - val_loss: 5.3931 - val_acc: 0.2740\n",
      "Epoch 48/50\n",
      "10320/10320 [==============================] - 6s 608us/step - loss: 5.6772 - acc: 0.2678 - val_loss: 5.3659 - val_acc: 0.2922\n",
      "Epoch 49/50\n",
      "10320/10320 [==============================] - 6s 603us/step - loss: 5.6815 - acc: 0.2709 - val_loss: 5.3994 - val_acc: 0.3186\n",
      "Epoch 50/50\n",
      "10320/10320 [==============================] - 6s 600us/step - loss: 5.6700 - acc: 0.2697 - val_loss: 5.3411 - val_acc: 0.2581\n"
     ]
    }
   ],
   "source": [
    "# randomly taking 10320 data samples\n",
    "classifier_A = build_model_A()\n",
    "hist_A = classifier_A.fit(x_train_ori, y_train_ori, batch_size=128, nb_epoch=50, verbose=1, \n",
    "                        validation_data=(x_test_ori, y_test_ori))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"weight-10320-CNN_A.hdf5\"\n",
    "classifier_A.save_weights(fname, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_B():\n",
    "    early_stop = callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=3, verbose=1, mode=\"min\")\n",
    "    \n",
    "    filepath = \"Best-weights-my_model-{epoch:03d}-{loss:4f}-{acc:4f}.hdf5\"\n",
    "    checkpoint = callbacks.ModelCheckpoint(filepath, monitor=\"val_loss\", verbose=1, save_best_only= True, mode=\"min\")\n",
    "    \n",
    "    callbacks_list = [early_stop, checkpoint]\n",
    "    \n",
    "    #build the model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #first convolutional layer\n",
    "    model.add(Conv2D(32, (3,3), input_shape=input_shape, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    \n",
    "    #second convolutional layer\n",
    "    model.add(Conv2D(32, (3,3), activation=\"relu\", border_mode=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    \n",
    "    #third convolutional layer\n",
    "    model.add(Conv2D(64, (3,3), activation=\"relu\", border_mode = \"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    \n",
    "    #forth convolutional layer\n",
    "    model.add(Conv2D(64, (3,3), activation=\"relu\", border_mode = \"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    #first FC layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "    model.add(BatchNormalization())\n",
    "        \n",
    "    #second FC layer\n",
    "    model.add(Dense(256, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    #output layer\n",
    "    model.add(Dense(24, activation=\"softmax\", kernel_initializer=\"uniform\"))\n",
    "    \n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:28: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10320 samples, validate on 2580 samples\n",
      "Epoch 1/50\n",
      "10320/10320 [==============================] - 9s 843us/step - loss: 7.6112 - acc: 0.2736 - val_loss: 5.9686 - val_acc: 0.1477\n",
      "Epoch 2/50\n",
      "10320/10320 [==============================] - 7s 698us/step - loss: 6.1199 - acc: 0.2801 - val_loss: 5.6523 - val_acc: 0.1969\n",
      "Epoch 3/50\n",
      "10320/10320 [==============================] - 7s 692us/step - loss: 5.8392 - acc: 0.2703 - val_loss: 5.4194 - val_acc: 0.3198\n",
      "Epoch 4/50\n",
      "10320/10320 [==============================] - 7s 693us/step - loss: 5.6918 - acc: 0.2750 - val_loss: 5.3505 - val_acc: 0.2399\n",
      "Epoch 5/50\n",
      "10320/10320 [==============================] - 7s 694us/step - loss: 5.6087 - acc: 0.2697 - val_loss: 5.3014 - val_acc: 0.3360\n",
      "Epoch 6/50\n",
      "10320/10320 [==============================] - 7s 694us/step - loss: 5.5397 - acc: 0.2706 - val_loss: 5.2748 - val_acc: 0.3163\n",
      "Epoch 7/50\n",
      "10320/10320 [==============================] - 7s 693us/step - loss: 5.4848 - acc: 0.2698 - val_loss: 5.2431 - val_acc: 0.2713\n",
      "Epoch 8/50\n",
      "10320/10320 [==============================] - 7s 692us/step - loss: 5.4474 - acc: 0.2664 - val_loss: 5.1988 - val_acc: 0.3101\n",
      "Epoch 9/50\n",
      "10320/10320 [==============================] - 7s 703us/step - loss: 5.4145 - acc: 0.2722 - val_loss: 5.2108 - val_acc: 0.3054\n",
      "Epoch 10/50\n",
      "10320/10320 [==============================] - 7s 700us/step - loss: 5.3850 - acc: 0.2682 - val_loss: 5.1686 - val_acc: 0.2376\n",
      "Epoch 11/50\n",
      "10320/10320 [==============================] - 7s 698us/step - loss: 5.3638 - acc: 0.2702 - val_loss: 5.1740 - val_acc: 0.2039\n",
      "Epoch 12/50\n",
      "10320/10320 [==============================] - 7s 699us/step - loss: 5.3424 - acc: 0.2674 - val_loss: 5.1636 - val_acc: 0.2562\n",
      "Epoch 13/50\n",
      "10320/10320 [==============================] - 7s 694us/step - loss: 5.3144 - acc: 0.2699 - val_loss: 5.1757 - val_acc: 0.2326\n",
      "Epoch 14/50\n",
      "10320/10320 [==============================] - 7s 690us/step - loss: 5.3001 - acc: 0.2636 - val_loss: 5.1554 - val_acc: 0.2802\n",
      "Epoch 15/50\n",
      "10320/10320 [==============================] - 7s 688us/step - loss: 5.2840 - acc: 0.2690 - val_loss: 5.1550 - val_acc: 0.3194\n",
      "Epoch 16/50\n",
      "10320/10320 [==============================] - 7s 689us/step - loss: 5.2672 - acc: 0.2641 - val_loss: 5.1568 - val_acc: 0.3674\n",
      "Epoch 17/50\n",
      "10320/10320 [==============================] - 7s 688us/step - loss: 5.2573 - acc: 0.2764 - val_loss: 5.1465 - val_acc: 0.1950\n",
      "Epoch 18/50\n",
      "10320/10320 [==============================] - 7s 689us/step - loss: 5.2458 - acc: 0.2644 - val_loss: 5.1321 - val_acc: 0.2965\n",
      "Epoch 19/50\n",
      "10320/10320 [==============================] - 7s 688us/step - loss: 5.2281 - acc: 0.2668 - val_loss: 5.1274 - val_acc: 0.3198\n",
      "Epoch 20/50\n",
      "10320/10320 [==============================] - 7s 689us/step - loss: 5.2185 - acc: 0.2661 - val_loss: 5.2376 - val_acc: 0.2298\n",
      "Epoch 21/50\n",
      "10320/10320 [==============================] - 7s 689us/step - loss: 5.2083 - acc: 0.2698 - val_loss: 5.1291 - val_acc: 0.3717\n",
      "Epoch 22/50\n",
      "10320/10320 [==============================] - 7s 689us/step - loss: 5.2005 - acc: 0.2676 - val_loss: 5.1268 - val_acc: 0.2519\n",
      "Epoch 23/50\n",
      "10320/10320 [==============================] - 7s 689us/step - loss: 5.1935 - acc: 0.2658 - val_loss: 5.1278 - val_acc: 0.2221\n",
      "Epoch 24/50\n",
      "10320/10320 [==============================] - 7s 690us/step - loss: 5.1897 - acc: 0.2703 - val_loss: 5.1283 - val_acc: 0.3535\n",
      "Epoch 25/50\n",
      "10320/10320 [==============================] - 7s 689us/step - loss: 5.1846 - acc: 0.2638 - val_loss: 5.1233 - val_acc: 0.1849\n",
      "Epoch 26/50\n",
      "10320/10320 [==============================] - 7s 689us/step - loss: 5.1808 - acc: 0.2587 - val_loss: 5.1231 - val_acc: 0.2527\n",
      "Epoch 27/50\n",
      "10320/10320 [==============================] - 7s 688us/step - loss: 5.1746 - acc: 0.2697 - val_loss: 5.1224 - val_acc: 0.2182\n",
      "Epoch 28/50\n",
      "10320/10320 [==============================] - 7s 688us/step - loss: 5.1761 - acc: 0.2629 - val_loss: 5.1253 - val_acc: 0.1752\n",
      "Epoch 29/50\n",
      "10320/10320 [==============================] - 7s 689us/step - loss: 5.1738 - acc: 0.2581 - val_loss: 5.1224 - val_acc: 0.3112\n",
      "Epoch 30/50\n",
      "10320/10320 [==============================] - 7s 688us/step - loss: 5.1702 - acc: 0.2686 - val_loss: 5.1330 - val_acc: 0.2721\n",
      "Epoch 31/50\n",
      "10320/10320 [==============================] - 7s 689us/step - loss: 5.1709 - acc: 0.2655 - val_loss: 5.1283 - val_acc: 0.2473\n",
      "Epoch 32/50\n",
      "10320/10320 [==============================] - 7s 688us/step - loss: 5.1739 - acc: 0.2666 - val_loss: 5.1231 - val_acc: 0.3202\n",
      "Epoch 33/50\n",
      "10320/10320 [==============================] - 7s 689us/step - loss: 5.1700 - acc: 0.2567 - val_loss: 5.1238 - val_acc: 0.1663\n",
      "Epoch 34/50\n",
      "10320/10320 [==============================] - 7s 689us/step - loss: 5.1671 - acc: 0.2652 - val_loss: 5.1203 - val_acc: 0.1484\n",
      "Epoch 35/50\n",
      "10320/10320 [==============================] - 7s 689us/step - loss: 5.1671 - acc: 0.2577 - val_loss: 5.1212 - val_acc: 0.1357\n",
      "Epoch 36/50\n",
      "10320/10320 [==============================] - 7s 689us/step - loss: 5.1678 - acc: 0.2671 - val_loss: 5.1174 - val_acc: 0.2298\n",
      "Epoch 37/50\n",
      "10320/10320 [==============================] - 7s 688us/step - loss: 5.1678 - acc: 0.2608 - val_loss: 5.1212 - val_acc: 0.2194\n",
      "Epoch 38/50\n",
      "10320/10320 [==============================] - 7s 689us/step - loss: 5.1671 - acc: 0.2641 - val_loss: 5.1206 - val_acc: 0.2236\n",
      "Epoch 39/50\n",
      "10320/10320 [==============================] - 7s 688us/step - loss: 5.1643 - acc: 0.2699 - val_loss: 5.1249 - val_acc: 0.3368\n",
      "Epoch 40/50\n",
      "10320/10320 [==============================] - 7s 689us/step - loss: 5.1627 - acc: 0.2656 - val_loss: 5.1213 - val_acc: 0.2647\n",
      "Epoch 41/50\n",
      "10320/10320 [==============================] - 7s 690us/step - loss: 5.1597 - acc: 0.2604 - val_loss: 5.1325 - val_acc: 0.2376\n",
      "Epoch 42/50\n",
      "10320/10320 [==============================] - 7s 689us/step - loss: 5.1676 - acc: 0.2721 - val_loss: 5.1431 - val_acc: 0.4531\n",
      "Epoch 43/50\n",
      "10320/10320 [==============================] - 7s 688us/step - loss: 5.1621 - acc: 0.2611 - val_loss: 5.1173 - val_acc: 0.1764\n",
      "Epoch 44/50\n",
      "10320/10320 [==============================] - 7s 689us/step - loss: 5.1636 - acc: 0.2665 - val_loss: 5.1252 - val_acc: 0.2911\n",
      "Epoch 45/50\n",
      "10320/10320 [==============================] - 7s 689us/step - loss: 5.1702 - acc: 0.2630 - val_loss: 5.1173 - val_acc: 0.2143\n",
      "Epoch 46/50\n",
      "10320/10320 [==============================] - 7s 689us/step - loss: 5.1630 - acc: 0.2703 - val_loss: 5.1149 - val_acc: 0.2085\n",
      "Epoch 47/50\n",
      "10320/10320 [==============================] - 7s 689us/step - loss: 5.1610 - acc: 0.2662 - val_loss: 5.1150 - val_acc: 0.2190\n",
      "Epoch 48/50\n",
      "10320/10320 [==============================] - 7s 689us/step - loss: 5.1561 - acc: 0.2644 - val_loss: 5.1244 - val_acc: 0.2093\n",
      "Epoch 49/50\n",
      "10320/10320 [==============================] - 7s 690us/step - loss: 5.1595 - acc: 0.2650 - val_loss: 5.1226 - val_acc: 0.2357\n",
      "Epoch 50/50\n",
      "10320/10320 [==============================] - 7s 690us/step - loss: 5.1591 - acc: 0.2704 - val_loss: 5.1366 - val_acc: 0.1360\n"
     ]
    }
   ],
   "source": [
    "# randomly taking 10320 data samples\n",
    "classifier_B = build_model_B()\n",
    "hist_B = classifier_B.fit(x_train_ori, y_train_ori, batch_size=64, nb_epoch=50, verbose=1, \n",
    "                          validation_data=(x_test_ori, y_test_ori))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"weight-10320-CNN_B.hdf5\"\n",
    "classifier_B.save_weights(fname, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_C():\n",
    "    early_stop = callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=3, verbose=1, mode=\"min\")\n",
    "    \n",
    "    filepath = \"Best-weights-my_model-{epoch:03d}-{loss:4f}-{acc:4f}.hdf5\"\n",
    "    checkpoint = callbacks.ModelCheckpoint(filepath, monitor=\"val_loss\", verbose=1, save_best_only= True, mode=\"min\")\n",
    "    \n",
    "    callbacks_list = [early_stop, checkpoint]\n",
    "    \n",
    "    #build the model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #first convolutional layer\n",
    "    model.add(Conv2D(32, (3,3), input_shape=input_shape, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    \n",
    "    #second convolutional layer\n",
    "    model.add(Conv2D(32, (3,3), activation=\"relu\", border_mode=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    #first FC layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    #output layer\n",
    "    model.add(Dense(24, activation=\"softmax\", kernel_initializer=\"uniform\"))\n",
    "    \n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"SGD\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")`\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10320 samples, validate on 2580 samples\n",
      "Epoch 1/50\n",
      "10320/10320 [==============================] - 7s 695us/step - loss: 7.5317 - acc: 0.3270 - val_loss: 6.6098 - val_acc: 0.1229\n",
      "Epoch 2/50\n",
      "10320/10320 [==============================] - 6s 623us/step - loss: 5.9865 - acc: 0.2791 - val_loss: 6.0474 - val_acc: 0.1023\n",
      "Epoch 3/50\n",
      "10320/10320 [==============================] - 6s 623us/step - loss: 5.8112 - acc: 0.2726 - val_loss: 5.6609 - val_acc: 0.2047\n",
      "Epoch 4/50\n",
      "10320/10320 [==============================] - 6s 623us/step - loss: 5.7467 - acc: 0.2747 - val_loss: 5.4659 - val_acc: 0.2950\n",
      "Epoch 5/50\n",
      "10320/10320 [==============================] - 6s 623us/step - loss: 5.7031 - acc: 0.2665 - val_loss: 5.7294 - val_acc: 0.1845\n",
      "Epoch 6/50\n",
      "10320/10320 [==============================] - 6s 623us/step - loss: 5.6828 - acc: 0.2770 - val_loss: 5.5264 - val_acc: 0.3174\n",
      "Epoch 7/50\n",
      "10320/10320 [==============================] - 6s 623us/step - loss: 5.6523 - acc: 0.2676 - val_loss: 5.4765 - val_acc: 0.1035\n",
      "Epoch 8/50\n",
      "10320/10320 [==============================] - 6s 624us/step - loss: 5.6298 - acc: 0.2657 - val_loss: 5.4573 - val_acc: 0.1895\n",
      "Epoch 9/50\n",
      "10320/10320 [==============================] - 6s 624us/step - loss: 5.6066 - acc: 0.2707 - val_loss: 5.4346 - val_acc: 0.4357\n",
      "Epoch 10/50\n",
      "10320/10320 [==============================] - 6s 623us/step - loss: 5.5938 - acc: 0.2723 - val_loss: 5.4007 - val_acc: 0.0795\n",
      "Epoch 11/50\n",
      "10320/10320 [==============================] - 6s 624us/step - loss: 5.5850 - acc: 0.2651 - val_loss: 5.5001 - val_acc: 0.2729\n",
      "Epoch 12/50\n",
      "10320/10320 [==============================] - 6s 623us/step - loss: 5.5666 - acc: 0.2633 - val_loss: 5.3972 - val_acc: 0.2767\n",
      "Epoch 13/50\n",
      "10320/10320 [==============================] - 6s 623us/step - loss: 5.5674 - acc: 0.2731 - val_loss: 5.3951 - val_acc: 0.1469\n",
      "Epoch 14/50\n",
      "10320/10320 [==============================] - 6s 624us/step - loss: 5.5558 - acc: 0.2708 - val_loss: 5.3775 - val_acc: 0.1787\n",
      "Epoch 15/50\n",
      "10320/10320 [==============================] - 6s 624us/step - loss: 5.5487 - acc: 0.2712 - val_loss: 5.3322 - val_acc: 0.1640\n",
      "Epoch 16/50\n",
      "10320/10320 [==============================] - 6s 624us/step - loss: 5.5315 - acc: 0.2649 - val_loss: 5.3541 - val_acc: 0.2535\n",
      "Epoch 17/50\n",
      "10320/10320 [==============================] - 6s 623us/step - loss: 5.5313 - acc: 0.2691 - val_loss: 5.3747 - val_acc: 0.2085\n",
      "Epoch 18/50\n",
      "10320/10320 [==============================] - 6s 624us/step - loss: 5.5240 - acc: 0.2713 - val_loss: 5.3167 - val_acc: 0.1860\n",
      "Epoch 19/50\n",
      "10320/10320 [==============================] - 6s 623us/step - loss: 5.5172 - acc: 0.2648 - val_loss: 5.3282 - val_acc: 0.3864\n",
      "Epoch 20/50\n",
      "10320/10320 [==============================] - 6s 623us/step - loss: 5.5045 - acc: 0.2686 - val_loss: 5.3112 - val_acc: 0.2826\n",
      "Epoch 21/50\n",
      "10320/10320 [==============================] - 6s 623us/step - loss: 5.4967 - acc: 0.2697 - val_loss: 5.3671 - val_acc: 0.1473\n",
      "Epoch 22/50\n",
      "10320/10320 [==============================] - 6s 624us/step - loss: 5.4988 - acc: 0.2634 - val_loss: 5.2933 - val_acc: 0.2705\n",
      "Epoch 23/50\n",
      "10320/10320 [==============================] - 6s 625us/step - loss: 5.4933 - acc: 0.2641 - val_loss: 5.3094 - val_acc: 0.2585\n",
      "Epoch 24/50\n",
      "10320/10320 [==============================] - 6s 624us/step - loss: 5.4816 - acc: 0.2632 - val_loss: 5.3318 - val_acc: 0.3244\n",
      "Epoch 25/50\n",
      "10320/10320 [==============================] - 6s 623us/step - loss: 5.4826 - acc: 0.2696 - val_loss: 5.2873 - val_acc: 0.3783\n",
      "Epoch 26/50\n",
      "10320/10320 [==============================] - 6s 624us/step - loss: 5.4820 - acc: 0.2703 - val_loss: 5.3598 - val_acc: 0.1787\n",
      "Epoch 27/50\n",
      "10320/10320 [==============================] - 6s 624us/step - loss: 5.4741 - acc: 0.2627 - val_loss: 5.2643 - val_acc: 0.2422\n",
      "Epoch 28/50\n",
      "10320/10320 [==============================] - 6s 624us/step - loss: 5.4694 - acc: 0.2617 - val_loss: 5.2812 - val_acc: 0.1977\n",
      "Epoch 29/50\n",
      "10320/10320 [==============================] - 6s 624us/step - loss: 5.4600 - acc: 0.2615 - val_loss: 5.2799 - val_acc: 0.2310\n",
      "Epoch 30/50\n",
      "10320/10320 [==============================] - 6s 624us/step - loss: 5.4563 - acc: 0.2596 - val_loss: 5.2842 - val_acc: 0.2977\n",
      "Epoch 31/50\n",
      "10320/10320 [==============================] - 6s 624us/step - loss: 5.4598 - acc: 0.2666 - val_loss: 5.3491 - val_acc: 0.2984\n",
      "Epoch 32/50\n",
      "10320/10320 [==============================] - 6s 624us/step - loss: 5.4463 - acc: 0.2622 - val_loss: 5.2803 - val_acc: 0.2663\n",
      "Epoch 33/50\n",
      "10320/10320 [==============================] - 6s 624us/step - loss: 5.4566 - acc: 0.2684 - val_loss: 5.2828 - val_acc: 0.3829\n",
      "Epoch 34/50\n",
      "10320/10320 [==============================] - 6s 624us/step - loss: 5.4447 - acc: 0.2716 - val_loss: 5.2598 - val_acc: 0.2430\n",
      "Epoch 35/50\n",
      "10320/10320 [==============================] - 6s 624us/step - loss: 5.4441 - acc: 0.2656 - val_loss: 5.3021 - val_acc: 0.4562\n",
      "Epoch 36/50\n",
      "10320/10320 [==============================] - 6s 624us/step - loss: 5.4430 - acc: 0.2644 - val_loss: 5.2538 - val_acc: 0.2225\n",
      "Epoch 37/50\n",
      "10320/10320 [==============================] - 6s 624us/step - loss: 5.4331 - acc: 0.2685 - val_loss: 5.2651 - val_acc: 0.1527\n",
      "Epoch 38/50\n",
      "10320/10320 [==============================] - 6s 623us/step - loss: 5.4293 - acc: 0.2691 - val_loss: 5.2597 - val_acc: 0.2357\n",
      "Epoch 39/50\n",
      "10320/10320 [==============================] - 6s 623us/step - loss: 5.4265 - acc: 0.2610 - val_loss: 5.3093 - val_acc: 0.2353\n",
      "Epoch 40/50\n",
      "10320/10320 [==============================] - 6s 624us/step - loss: 5.4266 - acc: 0.2687 - val_loss: 5.2714 - val_acc: 0.1349\n",
      "Epoch 41/50\n",
      "10320/10320 [==============================] - 6s 624us/step - loss: 5.4196 - acc: 0.2591 - val_loss: 5.2408 - val_acc: 0.1984\n",
      "Epoch 42/50\n",
      "10320/10320 [==============================] - 6s 624us/step - loss: 5.4250 - acc: 0.2650 - val_loss: 5.2549 - val_acc: 0.3368\n",
      "Epoch 43/50\n",
      "10320/10320 [==============================] - 6s 624us/step - loss: 5.4145 - acc: 0.2693 - val_loss: 5.2414 - val_acc: 0.3016\n",
      "Epoch 44/50\n",
      "10320/10320 [==============================] - 7s 630us/step - loss: 5.4187 - acc: 0.2674 - val_loss: 5.2433 - val_acc: 0.1891\n",
      "Epoch 45/50\n",
      "10320/10320 [==============================] - 7s 640us/step - loss: 5.4106 - acc: 0.2712 - val_loss: 5.2255 - val_acc: 0.1942\n",
      "Epoch 46/50\n",
      "10320/10320 [==============================] - 7s 636us/step - loss: 5.3989 - acc: 0.2625 - val_loss: 5.2260 - val_acc: 0.2581\n",
      "Epoch 47/50\n",
      "10320/10320 [==============================] - 7s 636us/step - loss: 5.4051 - acc: 0.2681 - val_loss: 5.2259 - val_acc: 0.1911\n",
      "Epoch 48/50\n",
      "10320/10320 [==============================] - 7s 641us/step - loss: 5.4025 - acc: 0.2635 - val_loss: 5.2864 - val_acc: 0.1632\n",
      "Epoch 49/50\n",
      "10320/10320 [==============================] - 7s 636us/step - loss: 5.4023 - acc: 0.2668 - val_loss: 5.3014 - val_acc: 0.2260\n",
      "Epoch 50/50\n",
      "10320/10320 [==============================] - 7s 633us/step - loss: 5.3940 - acc: 0.2643 - val_loss: 5.2669 - val_acc: 0.1965\n"
     ]
    }
   ],
   "source": [
    "# randomly taking 10320 data samples\n",
    "classifier_C = build_model_C()\n",
    "hist_C = classifier_C.fit(x_train_ori, y_train_ori, batch_size=64, nb_epoch=50, verbose=1,\n",
    "                          validation_data=(x_test_ori, y_test_ori))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"weight-10320-CNN_C.hdf5\"\n",
    "classifier_C.save_weights(fname, overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
