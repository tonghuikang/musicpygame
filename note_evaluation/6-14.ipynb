{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras import callbacks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_row = 108\n",
    "img_col = 108\n",
    "path = \"E:/database108/\"\n",
    "batch_size = 64\n",
    "output_classes = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "x_train_ori = []\n",
    "y_train_ori = []\n",
    "x_test_ori = []\n",
    "y_test_ori = []\n",
    "for i in range(516):\n",
    "    print(i)\n",
    "    file = pd.read_csv(path + \"database_{}.csv\".format(i), dtype=str)\n",
    "    array = file.values\n",
    "    array = list(array)\n",
    "    train_test = random.sample(range(100), 60)\n",
    "    \n",
    "    for j in train_test[:50]:\n",
    "        sample = list(array[j])\n",
    "        lst = []\n",
    "        for x in sample[1:11665]:\n",
    "            if type(x) == str:\n",
    "                x = \"\".join(x.split())\n",
    "                lst.append(float(x))\n",
    "            elif type(x) == float:\n",
    "                lst.append(x)\n",
    "            else:\n",
    "                print(type(x))\n",
    "        x_train_ori.append(lst)\n",
    "        y_train_ori.append(sample[0])\n",
    "        \n",
    "    for l in train_test[50:]:\n",
    "        sample = list(array[l])\n",
    "        lst = []\n",
    "        for x in sample[1:11665]:\n",
    "            if type(x) == str:\n",
    "                x = \"\".join(x.split())\n",
    "                lst.append(float(x))\n",
    "            elif type(x) == float:\n",
    "                lst.append(x)\n",
    "            else:\n",
    "                print(type(x))\n",
    "        x_test_ori.append(lst)\n",
    "        y_test_ori.append(sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25800, 11664)\n",
      "(5160, 11664)\n",
      "(25800,)\n",
      "(5160,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(x_train_ori))\n",
    "print(np.shape(x_test_ori))\n",
    "print(np.shape(y_train_ori))\n",
    "print(np.shape(y_test_ori))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_convert(array, flag=0):\n",
    "    y = []\n",
    "    for i in array: # i is the name of each sample:\n",
    "        name = i.split(\"_\")\n",
    "        name = name[1:5]\n",
    "        if flag == 0:\n",
    "            # category of appearance of a note\n",
    "            category = np.zeros(24, dtype=int)\n",
    "            for j in name:\n",
    "                if j != \"x\":\n",
    "                    num = int(j)\n",
    "                    category[num-60] = 1\n",
    "            y.append(list(category))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_1 = []\n",
    "lst_2 = []\n",
    "\n",
    "for i in y_train_ori:\n",
    "    label = \"\".join(i.split())\n",
    "    lst_1.append(label)\n",
    "for j in y_test_ori:\n",
    "    label = \"\".join(j.split())\n",
    "    lst_2.append(label)\n",
    "    \n",
    "y_train_ori = y_convert(lst_1)\n",
    "y_test_ori = y_convert(lst_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_ori = np.array(y_train_ori)\n",
    "y_test_ori = np.array(y_test_ori)\n",
    "x_train_ori = np.array(x_train_ori)\n",
    "x_test_ori = np.array(x_test_ori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_ori = x_train_ori.flatten()\n",
    "x_train_ori = x_train_ori.reshape(25800, img_row, img_col, 1)\n",
    "x_test_ori = x_test_ori.flatten()\n",
    "x_test_ori = x_test_ori.reshape(5160, img_row, img_col, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25800, 108, 108, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_ori.shape)\n",
    "input_shape = (108, 108, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_E():\n",
    "    early_stop = callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=3, verbose=1, mode=\"min\")\n",
    "    \n",
    "    filepath = \"Best-weights-my_model-{epoch:03d}-{loss:4f}-{acc:4f}.hdf5\"\n",
    "    checkpoint = callbacks.ModelCheckpoint(filepath, monitor=\"val_loss\", verbose=1, save_best_only= True, mode=\"min\")\n",
    "    \n",
    "    callbacks_list = [early_stop, checkpoint]\n",
    "    \n",
    "    #build the model\n",
    "    model = Sequential()\n",
    "    \n",
    "    #first convolutional layer\n",
    "    model.add(Conv2D(32, (3,3), input_shape=input_shape, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    \n",
    "    #second convolutional layer\n",
    "    model.add(Conv2D(32, (3,3), activation=\"relu\", border_mode=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    \n",
    "    #third convolutional layer\n",
    "    model.add(Conv2D(64, (3,3), activation=\"relu\", border_mode = \"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "\n",
    "    #forth convolutional layer\n",
    "    model.add(Conv2D(64, (3,3), activation=\"relu\", border_mode = \"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    \n",
    "    #fifth convulutional layer\n",
    "    model.add(Conv2D(64, (3,3), activation=\"relu\", border_mode = \"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(2,2))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    #first FC layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "    model.add(BatchNormalization())\n",
    "        \n",
    "    #second FC layer\n",
    "    model.add(Dense(128, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    #third FC layer\n",
    "    model.add(Dense(256, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    #output layer\n",
    "    model.add(Dense(24, activation=\"sigmoid\", kernel_initializer=\"uniform\"))\n",
    "    \n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:28: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25800 samples, validate on 5160 samples\n",
      "Epoch 1/100\n",
      "25800/25800 [==============================] - 22s 871us/step - loss: 0.0641 - acc: 0.3021 - val_loss: 0.0017 - val_acc: 0.2936\n",
      "Epoch 2/100\n",
      "25800/25800 [==============================] - 18s 689us/step - loss: 0.0046 - acc: 0.2790 - val_loss: 8.6305e-04 - val_acc: 0.2329\n",
      "Epoch 3/100\n",
      "25800/25800 [==============================] - 18s 690us/step - loss: 0.0026 - acc: 0.2753 - val_loss: 4.0472e-04 - val_acc: 0.4172\n",
      "Epoch 4/100\n",
      "25800/25800 [==============================] - 18s 691us/step - loss: 0.0018 - acc: 0.2717 - val_loss: 2.0647e-04 - val_acc: 0.1516\n",
      "Epoch 5/100\n",
      "25800/25800 [==============================] - 18s 692us/step - loss: 0.0016 - acc: 0.2740 - val_loss: 5.3611e-05 - val_acc: 0.2921\n",
      "Epoch 6/100\n",
      "25800/25800 [==============================] - 18s 691us/step - loss: 0.0016 - acc: 0.2770 - val_loss: 8.9945e-05 - val_acc: 0.2198\n",
      "Epoch 7/100\n",
      "25800/25800 [==============================] - 18s 691us/step - loss: 0.0012 - acc: 0.2690 - val_loss: 4.6743e-05 - val_acc: 0.2907\n",
      "Epoch 8/100\n",
      "25800/25800 [==============================] - 18s 692us/step - loss: 0.0012 - acc: 0.2850 - val_loss: 1.0718e-04 - val_acc: 0.3184\n",
      "Epoch 9/100\n",
      "25800/25800 [==============================] - 18s 692us/step - loss: 8.8844e-04 - acc: 0.2833 - val_loss: 1.4666e-05 - val_acc: 0.2626\n",
      "Epoch 10/100\n",
      "25800/25800 [==============================] - 18s 692us/step - loss: 0.0012 - acc: 0.2836 - val_loss: 4.3292e-05 - val_acc: 0.3531\n",
      "Epoch 11/100\n",
      "25800/25800 [==============================] - 18s 692us/step - loss: 0.0011 - acc: 0.2889 - val_loss: 0.2691 - val_acc: 0.0595\n",
      "Epoch 12/100\n",
      "25800/25800 [==============================] - 18s 692us/step - loss: 6.1919e-04 - acc: 0.2857 - val_loss: 2.8844e-06 - val_acc: 0.2498\n",
      "Epoch 13/100\n",
      "25800/25800 [==============================] - 18s 695us/step - loss: 6.6234e-04 - acc: 0.3141 - val_loss: 0.0443 - val_acc: 0.3936\n",
      "Epoch 14/100\n",
      "25800/25800 [==============================] - 18s 692us/step - loss: 7.5301e-04 - acc: 0.3115 - val_loss: 1.2363e-05 - val_acc: 0.3957\n",
      "Epoch 15/100\n",
      "25800/25800 [==============================] - 18s 693us/step - loss: 6.4353e-04 - acc: 0.3196 - val_loss: 8.6555e-05 - val_acc: 0.2283\n",
      "Epoch 16/100\n",
      "25800/25800 [==============================] - 18s 693us/step - loss: 6.5312e-04 - acc: 0.3350 - val_loss: 0.0086 - val_acc: 0.2758\n",
      "Epoch 17/100\n",
      "25800/25800 [==============================] - 18s 691us/step - loss: 6.9587e-04 - acc: 0.3290 - val_loss: 0.2383 - val_acc: 0.0568\n",
      "Epoch 18/100\n",
      "25800/25800 [==============================] - 18s 691us/step - loss: 6.1572e-04 - acc: 0.3499 - val_loss: 6.7488e-05 - val_acc: 0.3853\n",
      "Epoch 19/100\n",
      "25800/25800 [==============================] - 18s 692us/step - loss: 7.3265e-04 - acc: 0.3609 - val_loss: 1.2097e-06 - val_acc: 0.3820\n",
      "Epoch 20/100\n",
      "25800/25800 [==============================] - 18s 693us/step - loss: 5.2690e-04 - acc: 0.3750 - val_loss: 1.2815e-05 - val_acc: 0.4413\n",
      "Epoch 21/100\n",
      "25800/25800 [==============================] - 18s 691us/step - loss: 5.1068e-04 - acc: 0.3894 - val_loss: 2.0202e-04 - val_acc: 0.3312\n",
      "Epoch 22/100\n",
      "25800/25800 [==============================] - 18s 692us/step - loss: 6.2414e-04 - acc: 0.4205 - val_loss: 4.0155e-06 - val_acc: 0.5122\n",
      "Epoch 23/100\n",
      "25800/25800 [==============================] - 18s 692us/step - loss: 4.6025e-04 - acc: 0.4076 - val_loss: 1.2399e-04 - val_acc: 0.5428\n",
      "Epoch 24/100\n",
      "25800/25800 [==============================] - 18s 691us/step - loss: 4.6398e-04 - acc: 0.4188 - val_loss: 0.2902 - val_acc: 0.0388\n",
      "Epoch 25/100\n",
      "25800/25800 [==============================] - 18s 692us/step - loss: 4.8556e-04 - acc: 0.4364 - val_loss: 0.0030 - val_acc: 0.4473\n",
      "Epoch 26/100\n",
      "25800/25800 [==============================] - 18s 692us/step - loss: 4.1681e-04 - acc: 0.4699 - val_loss: 1.7545e-04 - val_acc: 0.2651\n",
      "Epoch 27/100\n",
      "25800/25800 [==============================] - 18s 693us/step - loss: 4.4808e-04 - acc: 0.4521 - val_loss: 0.2389 - val_acc: 0.0422\n",
      "Epoch 28/100\n",
      "25800/25800 [==============================] - 18s 691us/step - loss: 5.4314e-04 - acc: 0.4833 - val_loss: 6.9445e-06 - val_acc: 0.4492\n",
      "Epoch 29/100\n",
      "25800/25800 [==============================] - 18s 693us/step - loss: 7.3333e-04 - acc: 0.4901 - val_loss: 7.2257e-06 - val_acc: 0.4248\n",
      "Epoch 30/100\n",
      "25800/25800 [==============================] - 18s 692us/step - loss: 5.8334e-04 - acc: 0.4776 - val_loss: 1.2048e-05 - val_acc: 0.6965\n",
      "Epoch 31/100\n",
      "25800/25800 [==============================] - 18s 692us/step - loss: 3.6876e-04 - acc: 0.5129 - val_loss: 0.0022 - val_acc: 0.2246\n",
      "Epoch 32/100\n",
      "25800/25800 [==============================] - 18s 695us/step - loss: 6.3092e-04 - acc: 0.5330 - val_loss: 3.4298e-07 - val_acc: 0.5672\n",
      "Epoch 33/100\n",
      "25800/25800 [==============================] - 18s 703us/step - loss: 7.0648e-04 - acc: 0.5359 - val_loss: 3.2940e-06 - val_acc: 0.4913\n",
      "Epoch 34/100\n",
      "25800/25800 [==============================] - 18s 697us/step - loss: 2.9057e-04 - acc: 0.5471 - val_loss: 3.3450e-06 - val_acc: 0.4552\n",
      "Epoch 35/100\n",
      "25800/25800 [==============================] - 18s 701us/step - loss: 3.4021e-04 - acc: 0.5594 - val_loss: 5.7174e-06 - val_acc: 0.5264\n",
      "Epoch 36/100\n",
      "25800/25800 [==============================] - 18s 696us/step - loss: 4.3667e-04 - acc: 0.5696 - val_loss: 0.0343 - val_acc: 0.3240\n",
      "Epoch 37/100\n",
      "25800/25800 [==============================] - 18s 702us/step - loss: 4.0520e-04 - acc: 0.5911 - val_loss: 0.0486 - val_acc: 0.2820\n",
      "Epoch 38/100\n",
      "25800/25800 [==============================] - 18s 712us/step - loss: 3.3061e-04 - acc: 0.6181 - val_loss: 1.2635e-05 - val_acc: 0.5725\n",
      "Epoch 39/100\n",
      "25800/25800 [==============================] - 18s 699us/step - loss: 4.5407e-04 - acc: 0.5987 - val_loss: 5.7738e-07 - val_acc: 0.7421\n",
      "Epoch 40/100\n",
      "25800/25800 [==============================] - 18s 696us/step - loss: 2.0742e-04 - acc: 0.6198 - val_loss: 0.0024 - val_acc: 0.4184\n",
      "Epoch 41/100\n",
      "25800/25800 [==============================] - 18s 688us/step - loss: 4.6014e-04 - acc: 0.6087 - val_loss: 0.0110 - val_acc: 0.3306\n",
      "Epoch 42/100\n",
      "25800/25800 [==============================] - 18s 706us/step - loss: 2.8808e-04 - acc: 0.6216 - val_loss: 1.4844e-05 - val_acc: 0.7525\n",
      "Epoch 43/100\n",
      "25800/25800 [==============================] - 18s 692us/step - loss: 4.8138e-04 - acc: 0.6092 - val_loss: 0.1180 - val_acc: 0.2603\n",
      "Epoch 44/100\n",
      "25800/25800 [==============================] - 18s 693us/step - loss: 6.3130e-04 - acc: 0.6055 - val_loss: 4.3353e-06 - val_acc: 0.7184\n",
      "Epoch 45/100\n",
      "25800/25800 [==============================] - 18s 699us/step - loss: 3.3723e-04 - acc: 0.6507 - val_loss: 5.0716e-06 - val_acc: 0.6593\n",
      "Epoch 46/100\n",
      "25800/25800 [==============================] - 18s 694us/step - loss: 3.1888e-04 - acc: 0.6483 - val_loss: 2.9364e-08 - val_acc: 0.7692\n",
      "Epoch 47/100\n",
      "25800/25800 [==============================] - 18s 710us/step - loss: 3.6741e-04 - acc: 0.6690 - val_loss: 5.4041e-07 - val_acc: 0.6078\n",
      "Epoch 48/100\n",
      "25800/25800 [==============================] - 18s 687us/step - loss: 2.5164e-04 - acc: 0.6862 - val_loss: 1.7171e-07 - val_acc: 0.7238\n",
      "Epoch 49/100\n",
      "25800/25800 [==============================] - 18s 691us/step - loss: 4.7096e-04 - acc: 0.6841 - val_loss: 1.4677e-07 - val_acc: 0.8560\n",
      "Epoch 50/100\n",
      "25800/25800 [==============================] - 18s 700us/step - loss: 3.6010e-04 - acc: 0.6779 - val_loss: 9.1560e-04 - val_acc: 0.4481\n",
      "Epoch 51/100\n",
      "25800/25800 [==============================] - 18s 695us/step - loss: 3.3441e-04 - acc: 0.7045 - val_loss: 1.9900e-04 - val_acc: 0.5444\n",
      "Epoch 52/100\n",
      "25800/25800 [==============================] - 18s 706us/step - loss: 6.8114e-04 - acc: 0.6793 - val_loss: 6.1168e-07 - val_acc: 0.8372\n",
      "Epoch 53/100\n",
      "25800/25800 [==============================] - 18s 686us/step - loss: 2.2456e-04 - acc: 0.7062 - val_loss: 3.7727e-06 - val_acc: 0.8893\n",
      "Epoch 54/100\n",
      "25800/25800 [==============================] - 18s 690us/step - loss: 4.3214e-04 - acc: 0.6561 - val_loss: 6.7597e-04 - val_acc: 0.3826\n",
      "Epoch 55/100\n",
      "25800/25800 [==============================] - 18s 689us/step - loss: 2.7811e-04 - acc: 0.7118 - val_loss: 0.2328 - val_acc: 0.0643\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25800/25800 [==============================] - 18s 714us/step - loss: 3.2281e-04 - acc: 0.7151 - val_loss: 3.4271e-08 - val_acc: 0.8438\n",
      "Epoch 57/100\n",
      "25800/25800 [==============================] - 18s 710us/step - loss: 2.0581e-04 - acc: 0.7233 - val_loss: 5.3592e-08 - val_acc: 0.9178\n",
      "Epoch 58/100\n",
      "25800/25800 [==============================] - 18s 692us/step - loss: 2.5602e-04 - acc: 0.7378 - val_loss: 7.4405e-05 - val_acc: 0.6797\n",
      "Epoch 59/100\n",
      "25800/25800 [==============================] - 18s 692us/step - loss: 4.0609e-04 - acc: 0.7276 - val_loss: 5.4883e-08 - val_acc: 0.8337\n",
      "Epoch 60/100\n",
      "25800/25800 [==============================] - 18s 694us/step - loss: 2.8725e-04 - acc: 0.7338 - val_loss: 0.2850 - val_acc: 0.0537\n",
      "Epoch 61/100\n",
      "25800/25800 [==============================] - 18s 694us/step - loss: 6.6636e-04 - acc: 0.7414 - val_loss: 8.4885e-07 - val_acc: 0.7713\n",
      "Epoch 62/100\n",
      "25800/25800 [==============================] - 18s 696us/step - loss: 3.0758e-04 - acc: 0.7503 - val_loss: 0.2750 - val_acc: 0.1081\n",
      "Epoch 63/100\n",
      "25800/25800 [==============================] - 19s 717us/step - loss: 2.1655e-04 - acc: 0.7500 - val_loss: 6.8375e-08 - val_acc: 0.8657\n",
      "Epoch 64/100\n",
      "25800/25800 [==============================] - 18s 714us/step - loss: 2.5509e-04 - acc: 0.7568 - val_loss: 0.0028 - val_acc: 0.5167\n",
      "Epoch 65/100\n",
      "25800/25800 [==============================] - 18s 706us/step - loss: 2.2936e-04 - acc: 0.7571 - val_loss: 1.1222e-05 - val_acc: 0.8640\n",
      "Epoch 66/100\n",
      "25800/25800 [==============================] - 18s 705us/step - loss: 2.7780e-04 - acc: 0.7503 - val_loss: 0.2891 - val_acc: 0.1122\n",
      "Epoch 67/100\n",
      "25800/25800 [==============================] - 18s 701us/step - loss: 4.2710e-04 - acc: 0.7603 - val_loss: 1.7040e-06 - val_acc: 0.7097\n",
      "Epoch 68/100\n",
      "25800/25800 [==============================] - 18s 703us/step - loss: 3.3443e-04 - acc: 0.7585 - val_loss: 1.3024e-08 - val_acc: 0.7413\n",
      "Epoch 69/100\n",
      "25800/25800 [==============================] - 18s 694us/step - loss: 2.6262e-04 - acc: 0.7698 - val_loss: 3.3903e-06 - val_acc: 0.7250\n",
      "Epoch 70/100\n",
      "25800/25800 [==============================] - 18s 702us/step - loss: 2.7949e-04 - acc: 0.7560 - val_loss: 5.3184e-09 - val_acc: 0.8847\n",
      "Epoch 71/100\n",
      "25800/25800 [==============================] - 18s 702us/step - loss: 2.2205e-04 - acc: 0.7828 - val_loss: 9.9252e-07 - val_acc: 0.8579\n",
      "Epoch 72/100\n",
      "25800/25800 [==============================] - 18s 706us/step - loss: 3.8671e-04 - acc: 0.7597 - val_loss: 1.9427e-04 - val_acc: 0.7678\n",
      "Epoch 73/100\n",
      "25800/25800 [==============================] - 18s 699us/step - loss: 4.8692e-04 - acc: 0.7709 - val_loss: 1.3611e-06 - val_acc: 0.9589\n",
      "Epoch 74/100\n",
      "25800/25800 [==============================] - 18s 696us/step - loss: 2.4371e-04 - acc: 0.7741 - val_loss: 0.0061 - val_acc: 0.4477\n",
      "Epoch 75/100\n",
      "25800/25800 [==============================] - 18s 698us/step - loss: 3.3596e-04 - acc: 0.7731 - val_loss: 2.0338e-09 - val_acc: 0.8736\n",
      "Epoch 76/100\n",
      "25800/25800 [==============================] - 18s 690us/step - loss: 1.7632e-04 - acc: 0.7916 - val_loss: 0.0934 - val_acc: 0.2492\n",
      "Epoch 77/100\n",
      "25800/25800 [==============================] - 18s 698us/step - loss: 5.5627e-04 - acc: 0.7848 - val_loss: 3.4029e-08 - val_acc: 0.9275\n",
      "Epoch 78/100\n",
      "25800/25800 [==============================] - 18s 690us/step - loss: 2.4807e-04 - acc: 0.7866 - val_loss: 2.2373e-10 - val_acc: 0.9194\n",
      "Epoch 79/100\n",
      "25800/25800 [==============================] - 18s 686us/step - loss: 2.6432e-04 - acc: 0.7907 - val_loss: 2.7120e-10 - val_acc: 0.9516\n",
      "Epoch 80/100\n",
      "25800/25800 [==============================] - 18s 686us/step - loss: 2.2529e-04 - acc: 0.7922 - val_loss: 7.9322e-08 - val_acc: 0.9273\n",
      "Epoch 81/100\n",
      "25800/25800 [==============================] - 18s 686us/step - loss: 5.3139e-04 - acc: 0.7845 - val_loss: 1.5225e-08 - val_acc: 0.8585\n",
      "Epoch 82/100\n",
      "25800/25800 [==============================] - 18s 686us/step - loss: 2.5693e-04 - acc: 0.7917 - val_loss: 6.0898e-07 - val_acc: 0.9345\n",
      "Epoch 83/100\n",
      "25800/25800 [==============================] - 18s 696us/step - loss: 2.1717e-04 - acc: 0.8037 - val_loss: 3.3965e-08 - val_acc: 0.8764\n",
      "Epoch 84/100\n",
      "25800/25800 [==============================] - 18s 713us/step - loss: 1.8371e-04 - acc: 0.8078 - val_loss: 6.6360e-10 - val_acc: 0.9853\n",
      "Epoch 85/100\n",
      "25800/25800 [==============================] - 18s 698us/step - loss: 3.6498e-04 - acc: 0.8063 - val_loss: 1.5398e-06 - val_acc: 0.9424\n",
      "Epoch 86/100\n",
      "25800/25800 [==============================] - 18s 701us/step - loss: 2.4150e-04 - acc: 0.8173 - val_loss: 1.2401e-09 - val_acc: 0.9254\n",
      "Epoch 87/100\n",
      "25800/25800 [==============================] - 18s 698us/step - loss: 2.2041e-04 - acc: 0.8103 - val_loss: 1.6559e-08 - val_acc: 0.9612\n",
      "Epoch 88/100\n",
      "25800/25800 [==============================] - 18s 709us/step - loss: 1.8529e-04 - acc: 0.8307 - val_loss: 5.7851e-09 - val_acc: 0.9645\n",
      "Epoch 89/100\n",
      "25800/25800 [==============================] - 18s 698us/step - loss: 2.2009e-04 - acc: 0.8367 - val_loss: 1.1526e-09 - val_acc: 0.9893\n",
      "Epoch 90/100\n",
      "25800/25800 [==============================] - 18s 689us/step - loss: 2.7337e-04 - acc: 0.8172 - val_loss: 1.2249e-08 - val_acc: 0.9568\n",
      "Epoch 91/100\n",
      "25800/25800 [==============================] - 18s 685us/step - loss: 2.3070e-04 - acc: 0.8278 - val_loss: 2.9704e-08 - val_acc: 0.7961\n",
      "Epoch 92/100\n",
      "25800/25800 [==============================] - 18s 697us/step - loss: 2.0763e-04 - acc: 0.8208 - val_loss: 1.5753e-08 - val_acc: 0.9426\n",
      "Epoch 93/100\n",
      "25800/25800 [==============================] - 18s 693us/step - loss: 1.7826e-04 - acc: 0.8498 - val_loss: 2.3511e-09 - val_acc: 0.9349\n",
      "Epoch 94/100\n",
      "25800/25800 [==============================] - 18s 693us/step - loss: 2.8115e-04 - acc: 0.8297 - val_loss: 1.3981e-06 - val_acc: 0.9453\n",
      "Epoch 95/100\n",
      "25800/25800 [==============================] - 18s 694us/step - loss: 2.7216e-04 - acc: 0.8539 - val_loss: 2.0905e-06 - val_acc: 0.9802\n",
      "Epoch 96/100\n",
      "25800/25800 [==============================] - 18s 695us/step - loss: 4.4169e-04 - acc: 0.8462 - val_loss: 4.4855e-09 - val_acc: 0.9767\n",
      "Epoch 97/100\n",
      "25800/25800 [==============================] - 18s 695us/step - loss: 2.3024e-04 - acc: 0.8530 - val_loss: 3.3713e-07 - val_acc: 0.9886\n",
      "Epoch 98/100\n",
      "25800/25800 [==============================] - 18s 688us/step - loss: 2.6180e-04 - acc: 0.8467 - val_loss: 8.9185e-09 - val_acc: 0.9618\n",
      "Epoch 99/100\n",
      "25800/25800 [==============================] - 18s 687us/step - loss: 1.6793e-04 - acc: 0.8517 - val_loss: 1.9719e-09 - val_acc: 0.9953\n",
      "Epoch 100/100\n",
      "25800/25800 [==============================] - 18s 687us/step - loss: 2.9639e-04 - acc: 0.8406 - val_loss: 6.1313e-09 - val_acc: 0.9804\n"
     ]
    }
   ],
   "source": [
    "classifier_E = build_model_E()\n",
    "hist_E = classifier_E.fit(x_train_ori, y_train_ori, batch_size=64, nb_epoch=100, verbose=1,\n",
    "                          validation_data=(x_test_ori, y_test_ori))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"weight-25800-CNN_E_good.hdf5\"\n",
    "classifier_E.save_weights(fname, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:28: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")`\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\keras\\models.py:942: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25800 samples, validate on 5160 samples\n",
      "Epoch 1/200\n",
      "25800/25800 [==============================] - 20s 775us/step - loss: 0.0636 - acc: 0.2650 - val_loss: 0.0042 - val_acc: 0.1512\n",
      "Epoch 2/200\n",
      "25800/25800 [==============================] - 18s 688us/step - loss: 0.0045 - acc: 0.2765 - val_loss: 0.0020 - val_acc: 0.2777\n",
      "Epoch 3/200\n",
      "25800/25800 [==============================] - 18s 688us/step - loss: 0.0027 - acc: 0.2568 - val_loss: 4.5560e-04 - val_acc: 0.4023\n",
      "Epoch 4/200\n",
      "25800/25800 [==============================] - 18s 695us/step - loss: 0.0018 - acc: 0.2676 - val_loss: 0.0076 - val_acc: 0.3952\n",
      "Epoch 5/200\n",
      "25800/25800 [==============================] - 18s 693us/step - loss: 0.0015 - acc: 0.2613 - val_loss: 6.9962e-04 - val_acc: 0.2533\n",
      "Epoch 6/200\n",
      "25800/25800 [==============================] - 18s 703us/step - loss: 0.0015 - acc: 0.2622 - val_loss: 0.0170 - val_acc: 0.3587\n",
      "Epoch 7/200\n",
      "25800/25800 [==============================] - 18s 687us/step - loss: 0.0020 - acc: 0.2692 - val_loss: 0.0433 - val_acc: 0.3971\n",
      "Epoch 8/200\n",
      "25800/25800 [==============================] - 18s 694us/step - loss: 0.0011 - acc: 0.2763 - val_loss: 0.2348 - val_acc: 0.0663\n",
      "Epoch 9/200\n",
      "25800/25800 [==============================] - 18s 700us/step - loss: 0.0017 - acc: 0.2688 - val_loss: 6.8080e-05 - val_acc: 0.2866\n",
      "Epoch 10/200\n",
      "25800/25800 [==============================] - 18s 694us/step - loss: 0.0012 - acc: 0.2757 - val_loss: 3.2671e-04 - val_acc: 0.1880\n",
      "Epoch 11/200\n",
      "25800/25800 [==============================] - 18s 710us/step - loss: 9.2718e-04 - acc: 0.2794 - val_loss: 2.7319e-05 - val_acc: 0.2640\n",
      "Epoch 12/200\n",
      "25800/25800 [==============================] - 18s 715us/step - loss: 6.1272e-04 - acc: 0.2826 - val_loss: 0.0461 - val_acc: 0.3219\n",
      "Epoch 13/200\n",
      "25800/25800 [==============================] - 18s 708us/step - loss: 5.7857e-04 - acc: 0.2810 - val_loss: 0.1612 - val_acc: 0.1120\n",
      "Epoch 14/200\n",
      "25800/25800 [==============================] - 18s 715us/step - loss: 8.7673e-04 - acc: 0.2968 - val_loss: 3.9486e-05 - val_acc: 0.2382\n",
      "Epoch 15/200\n",
      "25800/25800 [==============================] - 18s 701us/step - loss: 6.2593e-04 - acc: 0.3209 - val_loss: 0.0812 - val_acc: 0.2492\n",
      "Epoch 16/200\n",
      "25800/25800 [==============================] - 18s 693us/step - loss: 0.0010 - acc: 0.3312 - val_loss: 3.4634e-05 - val_acc: 0.3110\n",
      "Epoch 17/200\n",
      "25800/25800 [==============================] - 18s 692us/step - loss: 5.5774e-04 - acc: 0.3296 - val_loss: 6.5132e-05 - val_acc: 0.2426\n",
      "Epoch 18/200\n",
      "25800/25800 [==============================] - 18s 692us/step - loss: 6.4344e-04 - acc: 0.3397 - val_loss: 0.0011 - val_acc: 0.4452\n",
      "Epoch 19/200\n",
      "25800/25800 [==============================] - 18s 692us/step - loss: 7.6424e-04 - acc: 0.3453 - val_loss: 9.1588e-05 - val_acc: 0.4019\n",
      "Epoch 20/200\n",
      "25800/25800 [==============================] - 18s 692us/step - loss: 7.9229e-04 - acc: 0.3741 - val_loss: 2.1787e-05 - val_acc: 0.4262\n",
      "Epoch 21/200\n",
      "25800/25800 [==============================] - 18s 691us/step - loss: 5.5265e-04 - acc: 0.3526 - val_loss: 2.1683e-05 - val_acc: 0.3764\n",
      "Epoch 22/200\n",
      "25800/25800 [==============================] - 18s 691us/step - loss: 5.3044e-04 - acc: 0.3720 - val_loss: 2.4797e-06 - val_acc: 0.3297\n",
      "Epoch 23/200\n",
      "25800/25800 [==============================] - 18s 692us/step - loss: 5.2661e-04 - acc: 0.4070 - val_loss: 1.0598e-04 - val_acc: 0.3703\n",
      "Epoch 24/200\n",
      "25800/25800 [==============================] - 18s 690us/step - loss: 4.0116e-04 - acc: 0.4096 - val_loss: 1.3308e-05 - val_acc: 0.4554\n",
      "Epoch 25/200\n",
      "25800/25800 [==============================] - 18s 691us/step - loss: 4.8769e-04 - acc: 0.4153 - val_loss: 6.5348e-05 - val_acc: 0.5099\n",
      "Epoch 26/200\n",
      "25800/25800 [==============================] - 18s 690us/step - loss: 4.0813e-04 - acc: 0.4320 - val_loss: 3.6161e-04 - val_acc: 0.4029\n",
      "Epoch 27/200\n",
      "25800/25800 [==============================] - 18s 691us/step - loss: 5.6701e-04 - acc: 0.4440 - val_loss: 0.0066 - val_acc: 0.3752\n",
      "Epoch 28/200\n",
      "25800/25800 [==============================] - 18s 691us/step - loss: 0.0011 - acc: 0.4417 - val_loss: 3.1510e-06 - val_acc: 0.4851\n",
      "Epoch 29/200\n",
      "25800/25800 [==============================] - 18s 692us/step - loss: 5.5960e-04 - acc: 0.4525 - val_loss: 2.1988e-05 - val_acc: 0.2791\n",
      "Epoch 30/200\n",
      "25800/25800 [==============================] - 18s 691us/step - loss: 6.2820e-04 - acc: 0.4721 - val_loss: 2.1783e-06 - val_acc: 0.4975\n",
      "Epoch 31/200\n",
      "25800/25800 [==============================] - 18s 691us/step - loss: 3.9764e-04 - acc: 0.4937 - val_loss: 6.5979e-07 - val_acc: 0.5725\n",
      "Epoch 32/200\n",
      "25800/25800 [==============================] - 18s 692us/step - loss: 3.7052e-04 - acc: 0.5147 - val_loss: 7.8739e-05 - val_acc: 0.4682\n",
      "Epoch 33/200\n",
      "25800/25800 [==============================] - 18s 691us/step - loss: 2.5545e-04 - acc: 0.5367 - val_loss: 4.5462e-04 - val_acc: 0.3506\n",
      "Epoch 34/200\n",
      "25800/25800 [==============================] - 18s 692us/step - loss: 3.5202e-04 - acc: 0.5453 - val_loss: 8.3571e-06 - val_acc: 0.4905\n",
      "Epoch 35/200\n",
      "25800/25800 [==============================] - 18s 691us/step - loss: 5.4817e-04 - acc: 0.5133 - val_loss: 1.1330e-05 - val_acc: 0.3878\n",
      "Epoch 36/200\n",
      "25800/25800 [==============================] - 18s 693us/step - loss: 4.1843e-04 - acc: 0.5407 - val_loss: 6.1712e-05 - val_acc: 0.4283\n",
      "Epoch 37/200\n",
      "25800/25800 [==============================] - 18s 691us/step - loss: 7.1489e-04 - acc: 0.5469 - val_loss: 2.6994e-05 - val_acc: 0.6359\n",
      "Epoch 38/200\n",
      "25800/25800 [==============================] - 18s 691us/step - loss: 4.1952e-04 - acc: 0.5789 - val_loss: 7.3054e-06 - val_acc: 0.6440\n",
      "Epoch 39/200\n",
      "25800/25800 [==============================] - 18s 692us/step - loss: 4.1848e-04 - acc: 0.5974 - val_loss: 1.9638e-06 - val_acc: 0.7056\n",
      "Epoch 40/200\n",
      "25800/25800 [==============================] - 18s 691us/step - loss: 3.1651e-04 - acc: 0.5903 - val_loss: 5.0679e-06 - val_acc: 0.6171\n",
      "Epoch 41/200\n",
      "25800/25800 [==============================] - 18s 692us/step - loss: 3.8709e-04 - acc: 0.5648 - val_loss: 0.1070 - val_acc: 0.2514\n",
      "Epoch 42/200\n",
      "25800/25800 [==============================] - 18s 692us/step - loss: 5.7109e-04 - acc: 0.5798 - val_loss: 6.3698e-06 - val_acc: 0.6426\n",
      "Epoch 43/200\n",
      "25800/25800 [==============================] - 18s 717us/step - loss: 4.0350e-04 - acc: 0.5794 - val_loss: 0.0012 - val_acc: 0.4285\n",
      "Epoch 44/200\n",
      "25800/25800 [==============================] - 19s 723us/step - loss: 2.3488e-04 - acc: 0.6025 - val_loss: 2.1298e-07 - val_acc: 0.6864\n",
      "Epoch 45/200\n",
      "25800/25800 [==============================] - 18s 706us/step - loss: 2.7381e-04 - acc: 0.6147 - val_loss: 0.0040 - val_acc: 0.5519\n",
      "Epoch 46/200\n",
      "25800/25800 [==============================] - 19s 728us/step - loss: 4.4860e-04 - acc: 0.6254 - val_loss: 9.3681e-06 - val_acc: 0.7116\n",
      "Epoch 47/200\n",
      "25800/25800 [==============================] - 18s 709us/step - loss: 2.2998e-04 - acc: 0.6348 - val_loss: 1.2024e-05 - val_acc: 0.7610\n",
      "Epoch 48/200\n",
      "25800/25800 [==============================] - 18s 699us/step - loss: 2.9022e-04 - acc: 0.6747 - val_loss: 0.0016 - val_acc: 0.5779\n",
      "Epoch 49/200\n",
      "25800/25800 [==============================] - 18s 715us/step - loss: 4.0864e-04 - acc: 0.6721 - val_loss: 3.9779e-07 - val_acc: 0.8037\n",
      "Epoch 50/200\n",
      "25800/25800 [==============================] - 19s 749us/step - loss: 3.0938e-04 - acc: 0.6652 - val_loss: 7.0419e-07 - val_acc: 0.7163\n",
      "Epoch 51/200\n",
      "25800/25800 [==============================] - 19s 754us/step - loss: 2.5198e-04 - acc: 0.6533 - val_loss: 1.6302e-08 - val_acc: 0.6645\n",
      "Epoch 52/200\n",
      "25800/25800 [==============================] - 20s 756us/step - loss: 3.2514e-04 - acc: 0.6643 - val_loss: 2.9480e-07 - val_acc: 0.7663\n",
      "Epoch 53/200\n",
      "25800/25800 [==============================] - 18s 715us/step - loss: 3.7469e-04 - acc: 0.6606 - val_loss: 0.0026 - val_acc: 0.5862\n",
      "Epoch 54/200\n",
      "25800/25800 [==============================] - 19s 737us/step - loss: 3.9757e-04 - acc: 0.6662 - val_loss: 0.1084 - val_acc: 0.3128\n",
      "Epoch 55/200\n",
      "25800/25800 [==============================] - 19s 751us/step - loss: 3.8468e-04 - acc: 0.6788 - val_loss: 7.4560e-08 - val_acc: 0.7219\n",
      "Epoch 56/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25800/25800 [==============================] - 19s 742us/step - loss: 3.4611e-04 - acc: 0.6762 - val_loss: 7.3281e-06 - val_acc: 0.7990\n",
      "Epoch 57/200\n",
      "25800/25800 [==============================] - 19s 746us/step - loss: 4.5596e-04 - acc: 0.6823 - val_loss: 2.7369e-06 - val_acc: 0.8847\n",
      "Epoch 58/200\n",
      "25800/25800 [==============================] - 19s 746us/step - loss: 1.9902e-04 - acc: 0.7125 - val_loss: 3.1915e-06 - val_acc: 0.8421\n",
      "Epoch 59/200\n",
      "25800/25800 [==============================] - 19s 735us/step - loss: 2.5181e-04 - acc: 0.6979 - val_loss: 7.6767e-07 - val_acc: 0.8021\n",
      "Epoch 60/200\n",
      "25800/25800 [==============================] - 19s 726us/step - loss: 2.7619e-04 - acc: 0.6906 - val_loss: 8.1812e-08 - val_acc: 0.7744\n",
      "Epoch 61/200\n",
      "25800/25800 [==============================] - 19s 724us/step - loss: 4.3120e-04 - acc: 0.7219 - val_loss: 4.5584e-04 - val_acc: 0.5450\n",
      "Epoch 62/200\n",
      "25800/25800 [==============================] - 19s 727us/step - loss: 1.8008e-04 - acc: 0.7076 - val_loss: 0.2587 - val_acc: 0.0359\n",
      "Epoch 63/200\n",
      "25800/25800 [==============================] - 19s 726us/step - loss: 2.2839e-04 - acc: 0.7375 - val_loss: 1.6276e-05 - val_acc: 0.7434\n",
      "Epoch 64/200\n",
      "25800/25800 [==============================] - 19s 720us/step - loss: 5.2722e-04 - acc: 0.7225 - val_loss: 3.5137e-07 - val_acc: 0.8279\n",
      "Epoch 65/200\n",
      "25800/25800 [==============================] - 19s 719us/step - loss: 3.7981e-04 - acc: 0.7368 - val_loss: 1.1571e-06 - val_acc: 0.7324\n",
      "Epoch 66/200\n",
      "25800/25800 [==============================] - 19s 738us/step - loss: 2.1590e-04 - acc: 0.7519 - val_loss: 4.8061e-04 - val_acc: 0.7219\n",
      "Epoch 67/200\n",
      "25800/25800 [==============================] - 19s 728us/step - loss: 4.3003e-04 - acc: 0.7354 - val_loss: 5.4234e-06 - val_acc: 0.8616\n",
      "Epoch 68/200\n",
      "25800/25800 [==============================] - 19s 729us/step - loss: 3.7592e-04 - acc: 0.7393 - val_loss: 5.4302e-05 - val_acc: 0.6390\n",
      "Epoch 69/200\n",
      "25800/25800 [==============================] - 19s 725us/step - loss: 2.0564e-04 - acc: 0.7502 - val_loss: 2.6900e-05 - val_acc: 0.6855\n",
      "Epoch 70/200\n",
      "25800/25800 [==============================] - 19s 718us/step - loss: 2.8983e-04 - acc: 0.7429 - val_loss: 1.4235e-08 - val_acc: 0.9091\n",
      "Epoch 71/200\n",
      "25800/25800 [==============================] - 18s 710us/step - loss: 1.8137e-04 - acc: 0.7526 - val_loss: 2.4740e-09 - val_acc: 0.8463\n",
      "Epoch 72/200\n",
      "25800/25800 [==============================] - 19s 725us/step - loss: 2.0852e-04 - acc: 0.7580 - val_loss: 5.7437e-07 - val_acc: 0.9391\n",
      "Epoch 73/200\n",
      "25800/25800 [==============================] - 19s 733us/step - loss: 5.7403e-04 - acc: 0.7513 - val_loss: 6.6458e-06 - val_acc: 0.8566\n",
      "Epoch 74/200\n",
      "25800/25800 [==============================] - 19s 737us/step - loss: 2.4261e-04 - acc: 0.7595 - val_loss: 3.1844e-06 - val_acc: 0.9279\n",
      "Epoch 75/200\n",
      "25800/25800 [==============================] - 19s 728us/step - loss: 2.1700e-04 - acc: 0.7937 - val_loss: 6.8958e-09 - val_acc: 0.9357\n",
      "Epoch 76/200\n",
      "25800/25800 [==============================] - 19s 728us/step - loss: 3.1690e-04 - acc: 0.7905 - val_loss: 4.6648e-06 - val_acc: 0.8818\n",
      "Epoch 77/200\n",
      "25800/25800 [==============================] - 19s 740us/step - loss: 3.5753e-04 - acc: 0.7608 - val_loss: 1.6309e-08 - val_acc: 0.9128\n",
      "Epoch 78/200\n",
      "25800/25800 [==============================] - 19s 735us/step - loss: 4.8642e-04 - acc: 0.7704 - val_loss: 2.0141e-06 - val_acc: 0.9143\n",
      "Epoch 79/200\n",
      "25800/25800 [==============================] - 19s 741us/step - loss: 2.9410e-04 - acc: 0.7891 - val_loss: 1.2805e-10 - val_acc: 0.9246\n",
      "Epoch 80/200\n",
      "25800/25800 [==============================] - 19s 737us/step - loss: 2.4077e-04 - acc: 0.7695 - val_loss: 1.3673e-09 - val_acc: 0.9308\n",
      "Epoch 81/200\n",
      "25800/25800 [==============================] - 19s 721us/step - loss: 1.9898e-04 - acc: 0.7996 - val_loss: 4.4583e-09 - val_acc: 0.9636\n",
      "Epoch 82/200\n",
      "25800/25800 [==============================] - 19s 736us/step - loss: 3.4452e-04 - acc: 0.8033 - val_loss: 6.8725e-06 - val_acc: 0.9562\n",
      "Epoch 83/200\n",
      "25800/25800 [==============================] - 19s 742us/step - loss: 2.8111e-04 - acc: 0.7959 - val_loss: 1.4786e-05 - val_acc: 0.8829\n",
      "Epoch 84/200\n",
      "25800/25800 [==============================] - 19s 743us/step - loss: 2.0006e-04 - acc: 0.7971 - val_loss: 1.0630e-05 - val_acc: 0.8855\n",
      "Epoch 85/200\n",
      "25800/25800 [==============================] - 19s 743us/step - loss: 2.4344e-04 - acc: 0.7879 - val_loss: 7.6761e-08 - val_acc: 0.9000\n",
      "Epoch 86/200\n",
      "25800/25800 [==============================] - 19s 744us/step - loss: 1.8005e-04 - acc: 0.7825 - val_loss: 1.3093e-08 - val_acc: 0.9421\n",
      "Epoch 87/200\n",
      "25800/25800 [==============================] - 19s 742us/step - loss: 1.8903e-04 - acc: 0.8038 - val_loss: 9.4303e-09 - val_acc: 0.9583\n",
      "Epoch 88/200\n",
      "25800/25800 [==============================] - 19s 738us/step - loss: 2.1051e-04 - acc: 0.8050 - val_loss: 6.9804e-06 - val_acc: 0.9174\n",
      "Epoch 89/200\n",
      "25800/25800 [==============================] - 19s 739us/step - loss: 1.6829e-04 - acc: 0.7980 - val_loss: 4.0406e-10 - val_acc: 0.9432\n",
      "Epoch 90/200\n",
      "25800/25800 [==============================] - 19s 739us/step - loss: 1.8009e-04 - acc: 0.8007 - val_loss: 3.1819e-06 - val_acc: 0.8748\n",
      "Epoch 91/200\n",
      "25800/25800 [==============================] - 19s 732us/step - loss: 1.6077e-04 - acc: 0.8064 - val_loss: 1.4458e-05 - val_acc: 0.9641\n",
      "Epoch 92/200\n",
      "25800/25800 [==============================] - 19s 740us/step - loss: 2.5648e-04 - acc: 0.8150 - val_loss: 2.0467e-05 - val_acc: 0.8074\n",
      "Epoch 93/200\n",
      "25800/25800 [==============================] - 19s 738us/step - loss: 4.2204e-04 - acc: 0.8096 - val_loss: 1.6351e-08 - val_acc: 0.8690\n",
      "Epoch 94/200\n",
      "25800/25800 [==============================] - 19s 741us/step - loss: 2.4270e-04 - acc: 0.8312 - val_loss: 9.6167e-11 - val_acc: 0.9783\n",
      "Epoch 95/200\n",
      "25800/25800 [==============================] - 19s 739us/step - loss: 2.7383e-04 - acc: 0.8252 - val_loss: 0.0045 - val_acc: 0.5647\n",
      "Epoch 96/200\n",
      "25800/25800 [==============================] - 19s 736us/step - loss: 3.5327e-04 - acc: 0.8283 - val_loss: 8.7380e-06 - val_acc: 0.9601\n",
      "Epoch 97/200\n",
      "25800/25800 [==============================] - 19s 741us/step - loss: 1.9635e-04 - acc: 0.8433 - val_loss: 5.5409e-08 - val_acc: 0.9409\n",
      "Epoch 98/200\n",
      "25800/25800 [==============================] - 19s 739us/step - loss: 2.2989e-04 - acc: 0.8222 - val_loss: 3.9714e-05 - val_acc: 0.9434\n",
      "Epoch 99/200\n",
      "25800/25800 [==============================] - 19s 741us/step - loss: 1.8415e-04 - acc: 0.8395 - val_loss: 9.7195e-06 - val_acc: 0.8905\n",
      "Epoch 100/200\n",
      "25800/25800 [==============================] - 19s 745us/step - loss: 4.4234e-04 - acc: 0.8011 - val_loss: 4.8106e-09 - val_acc: 0.9797\n",
      "Epoch 101/200\n",
      "25800/25800 [==============================] - 19s 743us/step - loss: 1.7041e-04 - acc: 0.8096 - val_loss: 1.4276e-08 - val_acc: 0.8984\n",
      "Epoch 102/200\n",
      "25800/25800 [==============================] - 19s 740us/step - loss: 3.5001e-04 - acc: 0.8099 - val_loss: 4.7587e-10 - val_acc: 0.9711\n",
      "Epoch 103/200\n",
      "25800/25800 [==============================] - 19s 734us/step - loss: 2.0439e-04 - acc: 0.8505 - val_loss: 2.2076e-05 - val_acc: 0.9806\n",
      "Epoch 104/200\n",
      "25800/25800 [==============================] - 19s 742us/step - loss: 5.2190e-04 - acc: 0.8266 - val_loss: 5.0093e-08 - val_acc: 0.9630\n",
      "Epoch 105/200\n",
      "25800/25800 [==============================] - 19s 746us/step - loss: 2.8135e-04 - acc: 0.8411 - val_loss: 1.6703e-10 - val_acc: 0.9893\n",
      "Epoch 106/200\n",
      "25800/25800 [==============================] - 19s 746us/step - loss: 1.9185e-04 - acc: 0.8496 - val_loss: 8.1445e-09 - val_acc: 0.9837\n",
      "Epoch 107/200\n",
      "25800/25800 [==============================] - 19s 743us/step - loss: 2.4490e-04 - acc: 0.8358 - val_loss: 1.0297e-06 - val_acc: 0.9227\n",
      "Epoch 108/200\n",
      "25800/25800 [==============================] - 19s 745us/step - loss: 2.1374e-04 - acc: 0.8503 - val_loss: 4.1652e-11 - val_acc: 0.9636\n",
      "Epoch 109/200\n",
      "25800/25800 [==============================] - 19s 745us/step - loss: 2.5304e-04 - acc: 0.8369 - val_loss: 8.5815e-09 - val_acc: 0.9859\n",
      "Epoch 110/200\n",
      "25800/25800 [==============================] - 19s 742us/step - loss: 1.7671e-04 - acc: 0.8516 - val_loss: 2.5943e-09 - val_acc: 0.9872\n",
      "Epoch 111/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25800/25800 [==============================] - 19s 729us/step - loss: 2.3672e-04 - acc: 0.8451 - val_loss: 5.8989e-08 - val_acc: 0.9641\n",
      "Epoch 112/200\n",
      "25800/25800 [==============================] - 19s 727us/step - loss: 1.9967e-04 - acc: 0.8641 - val_loss: 3.1889e-11 - val_acc: 0.9744\n",
      "Epoch 113/200\n",
      "25800/25800 [==============================] - 19s 745us/step - loss: 2.5136e-04 - acc: 0.8650 - val_loss: 7.3232e-09 - val_acc: 0.9845\n",
      "Epoch 114/200\n",
      "25800/25800 [==============================] - 19s 744us/step - loss: 2.4620e-04 - acc: 0.8476 - val_loss: 3.9304e-07 - val_acc: 0.8686\n",
      "Epoch 115/200\n",
      "25800/25800 [==============================] - 19s 743us/step - loss: 2.6180e-04 - acc: 0.8391 - val_loss: 2.0453e-08 - val_acc: 0.9738\n",
      "Epoch 116/200\n",
      "25800/25800 [==============================] - 19s 742us/step - loss: 3.2666e-04 - acc: 0.8556 - val_loss: 8.1108e-07 - val_acc: 0.9928\n",
      "Epoch 117/200\n",
      "25800/25800 [==============================] - 19s 746us/step - loss: 1.7281e-04 - acc: 0.8757 - val_loss: 7.3416e-11 - val_acc: 0.9868\n",
      "Epoch 118/200\n",
      "25800/25800 [==============================] - 19s 743us/step - loss: 2.4074e-04 - acc: 0.8588 - val_loss: 3.5139e-07 - val_acc: 0.9585\n",
      "Epoch 119/200\n",
      "25800/25800 [==============================] - 19s 745us/step - loss: 1.1966e-04 - acc: 0.8647 - val_loss: 2.2033e-08 - val_acc: 0.9583\n",
      "Epoch 120/200\n",
      "25800/25800 [==============================] - 19s 745us/step - loss: 2.2440e-04 - acc: 0.8597 - val_loss: 0.0047 - val_acc: 0.6448\n",
      "Epoch 121/200\n",
      "25800/25800 [==============================] - 19s 745us/step - loss: 1.8153e-04 - acc: 0.8674 - val_loss: 4.7597e-06 - val_acc: 0.9422\n",
      "Epoch 122/200\n",
      "25800/25800 [==============================] - 19s 742us/step - loss: 1.3564e-04 - acc: 0.8709 - val_loss: 2.5953e-06 - val_acc: 0.9773\n",
      "Epoch 123/200\n",
      "25800/25800 [==============================] - 19s 741us/step - loss: 2.6105e-04 - acc: 0.8509 - val_loss: 4.5698e-09 - val_acc: 0.9913\n",
      "Epoch 124/200\n",
      "25800/25800 [==============================] - 19s 743us/step - loss: 1.5690e-04 - acc: 0.8719 - val_loss: 6.1420e-07 - val_acc: 0.9810\n",
      "Epoch 125/200\n",
      "25800/25800 [==============================] - 19s 745us/step - loss: 2.9886e-04 - acc: 0.8727 - val_loss: 1.2755e-05 - val_acc: 0.9483\n",
      "Epoch 126/200\n",
      "25800/25800 [==============================] - 19s 740us/step - loss: 3.2450e-04 - acc: 0.8729 - val_loss: 1.1556e-10 - val_acc: 0.9878\n",
      "Epoch 127/200\n",
      "25800/25800 [==============================] - 19s 741us/step - loss: 2.1145e-04 - acc: 0.8677 - val_loss: 1.0115e-10 - val_acc: 0.9833\n",
      "Epoch 128/200\n",
      "25800/25800 [==============================] - 19s 740us/step - loss: 1.8464e-04 - acc: 0.8820 - val_loss: 7.5997e-06 - val_acc: 0.9698\n",
      "Epoch 129/200\n",
      "25800/25800 [==============================] - 19s 741us/step - loss: 1.3388e-04 - acc: 0.8883 - val_loss: 1.1538e-11 - val_acc: 0.9826\n",
      "Epoch 130/200\n",
      "25800/25800 [==============================] - 19s 740us/step - loss: 1.2457e-04 - acc: 0.8860 - val_loss: 3.1591e-08 - val_acc: 0.9895\n",
      "Epoch 131/200\n",
      "25800/25800 [==============================] - 19s 730us/step - loss: 1.4434e-04 - acc: 0.8870 - val_loss: 3.0713e-09 - val_acc: 0.9630\n",
      "Epoch 132/200\n",
      "25800/25800 [==============================] - 19s 733us/step - loss: 4.0554e-04 - acc: 0.8536 - val_loss: 4.5545e-06 - val_acc: 0.9672\n",
      "Epoch 133/200\n",
      "25800/25800 [==============================] - 19s 741us/step - loss: 1.3780e-04 - acc: 0.8714 - val_loss: 2.5948e-06 - val_acc: 0.9676\n",
      "Epoch 134/200\n",
      "25800/25800 [==============================] - 19s 740us/step - loss: 1.8487e-04 - acc: 0.8757 - val_loss: 4.0043e-11 - val_acc: 0.9901\n",
      "Epoch 135/200\n",
      "25800/25800 [==============================] - 19s 744us/step - loss: 1.5593e-04 - acc: 0.8856 - val_loss: 1.3344e-11 - val_acc: 0.9816\n",
      "Epoch 136/200\n",
      "25800/25800 [==============================] - 19s 742us/step - loss: 1.5297e-04 - acc: 0.8847 - val_loss: 2.1231e-09 - val_acc: 0.9953\n",
      "Epoch 137/200\n",
      "25800/25800 [==============================] - 19s 741us/step - loss: 1.3433e-04 - acc: 0.8916 - val_loss: 3.2036e-08 - val_acc: 0.9535\n",
      "Epoch 138/200\n",
      "25800/25800 [==============================] - 19s 743us/step - loss: 1.1531e-04 - acc: 0.8857 - val_loss: 4.0041e-08 - val_acc: 0.9672\n",
      "Epoch 139/200\n",
      "25800/25800 [==============================] - 19s 742us/step - loss: 1.9214e-04 - acc: 0.8923 - val_loss: 5.8308e-10 - val_acc: 0.9767\n",
      "Epoch 140/200\n",
      "25800/25800 [==============================] - 19s 738us/step - loss: 3.1452e-04 - acc: 0.8887 - val_loss: 1.5855e-11 - val_acc: 0.9833\n",
      "Epoch 141/200\n",
      "25800/25800 [==============================] - 19s 743us/step - loss: 2.7388e-04 - acc: 0.8855 - val_loss: 3.8245e-10 - val_acc: 0.9926\n",
      "Epoch 142/200\n",
      "25800/25800 [==============================] - 19s 741us/step - loss: 1.4396e-04 - acc: 0.9003 - val_loss: 1.0731e-07 - val_acc: 0.9723\n",
      "Epoch 143/200\n",
      "25800/25800 [==============================] - 19s 740us/step - loss: 1.6622e-04 - acc: 0.8959 - val_loss: 2.9302e-11 - val_acc: 0.9876\n",
      "Epoch 144/200\n",
      "25800/25800 [==============================] - 19s 744us/step - loss: 1.6478e-04 - acc: 0.8947 - val_loss: 2.9645e-11 - val_acc: 0.9901\n",
      "Epoch 145/200\n",
      "25800/25800 [==============================] - 19s 740us/step - loss: 1.7177e-04 - acc: 0.9057 - val_loss: 8.6057e-09 - val_acc: 0.9826\n",
      "Epoch 146/200\n",
      "25800/25800 [==============================] - 19s 738us/step - loss: 1.4162e-04 - acc: 0.9078 - val_loss: 1.6160e-11 - val_acc: 0.9833\n",
      "Epoch 147/200\n",
      "25800/25800 [==============================] - 19s 743us/step - loss: 3.3898e-04 - acc: 0.8982 - val_loss: 6.3682e-12 - val_acc: 0.9957\n",
      "Epoch 148/200\n",
      "25800/25800 [==============================] - 19s 741us/step - loss: 1.2463e-04 - acc: 0.9040 - val_loss: 3.0400e-11 - val_acc: 0.9891\n",
      "Epoch 149/200\n",
      "25800/25800 [==============================] - 19s 743us/step - loss: 1.3698e-04 - acc: 0.8994 - val_loss: 2.4675e-09 - val_acc: 0.9936\n",
      "Epoch 150/200\n",
      "25800/25800 [==============================] - 19s 744us/step - loss: 1.8380e-04 - acc: 0.9024 - val_loss: 3.8064e-08 - val_acc: 0.9752\n",
      "Epoch 151/200\n",
      "25800/25800 [==============================] - 19s 742us/step - loss: 2.3139e-04 - acc: 0.9101 - val_loss: 6.2560e-06 - val_acc: 0.9990\n",
      "Epoch 152/200\n",
      "25800/25800 [==============================] - 19s 744us/step - loss: 1.5448e-04 - acc: 0.9227 - val_loss: 1.9968e-09 - val_acc: 0.9979\n",
      "Epoch 153/200\n",
      "25800/25800 [==============================] - 19s 735us/step - loss: 1.1317e-04 - acc: 0.9076 - val_loss: 4.3725e-13 - val_acc: 0.9971\n",
      "Epoch 154/200\n",
      "25800/25800 [==============================] - 20s 757us/step - loss: 2.0814e-04 - acc: 0.9102 - val_loss: 3.1599e-08 - val_acc: 0.9831\n",
      "Epoch 155/200\n",
      "25800/25800 [==============================] - 20s 759us/step - loss: 1.4003e-04 - acc: 0.8972 - val_loss: 6.1383e-07 - val_acc: 0.9603\n",
      "Epoch 156/200\n",
      "25800/25800 [==============================] - 20s 761us/step - loss: 2.1178e-04 - acc: 0.9095 - val_loss: 7.6634e-09 - val_acc: 0.9981\n",
      "Epoch 157/200\n",
      "25800/25800 [==============================] - 20s 760us/step - loss: 1.5092e-04 - acc: 0.9192 - val_loss: 1.4504e-10 - val_acc: 0.9897\n",
      "Epoch 158/200\n",
      "25800/25800 [==============================] - 20s 760us/step - loss: 1.9404e-04 - acc: 0.9069 - val_loss: 8.1361e-07 - val_acc: 0.9977\n",
      "Epoch 159/200\n",
      "25800/25800 [==============================] - 19s 752us/step - loss: 1.1438e-04 - acc: 0.9152 - val_loss: 9.8190e-06 - val_acc: 0.9657\n",
      "Epoch 160/200\n",
      "25800/25800 [==============================] - 20s 756us/step - loss: 2.5033e-04 - acc: 0.9032 - val_loss: 7.6913e-12 - val_acc: 0.9955\n",
      "Epoch 161/200\n",
      "25800/25800 [==============================] - 19s 755us/step - loss: 1.8495e-04 - acc: 0.9048 - val_loss: 3.3146e-07 - val_acc: 0.9969\n",
      "Epoch 162/200\n",
      "25800/25800 [==============================] - 19s 753us/step - loss: 2.0018e-04 - acc: 0.9168 - val_loss: 8.6678e-13 - val_acc: 0.9913\n",
      "Epoch 163/200\n",
      "25800/25800 [==============================] - 19s 745us/step - loss: 1.7206e-04 - acc: 0.9171 - val_loss: 4.4872e-11 - val_acc: 0.9899\n",
      "Epoch 164/200\n",
      "25800/25800 [==============================] - 19s 750us/step - loss: 2.0813e-04 - acc: 0.9174 - val_loss: 3.9451e-11 - val_acc: 0.9986\n",
      "Epoch 165/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25800/25800 [==============================] - 19s 749us/step - loss: 3.1632e-04 - acc: 0.9241 - val_loss: 3.3034e-06 - val_acc: 0.9938\n",
      "Epoch 166/200\n",
      "25800/25800 [==============================] - 19s 750us/step - loss: 1.9668e-04 - acc: 0.9217 - val_loss: 1.4140e-13 - val_acc: 0.9996\n",
      "Epoch 167/200\n",
      "25800/25800 [==============================] - 19s 750us/step - loss: 1.1533e-04 - acc: 0.9233 - val_loss: 2.5621e-12 - val_acc: 0.9948\n",
      "Epoch 168/200\n",
      "25800/25800 [==============================] - 19s 748us/step - loss: 2.9674e-04 - acc: 0.9102 - val_loss: 8.9739e-11 - val_acc: 0.9913\n",
      "Epoch 169/200\n",
      "25800/25800 [==============================] - 19s 751us/step - loss: 1.8968e-04 - acc: 0.9109 - val_loss: 1.1495e-10 - val_acc: 0.9975\n",
      "Epoch 170/200\n",
      "25800/25800 [==============================] - 19s 745us/step - loss: 1.4965e-04 - acc: 0.9219 - val_loss: 1.8195e-12 - val_acc: 0.9901\n",
      "Epoch 171/200\n",
      "25800/25800 [==============================] - 19s 752us/step - loss: 1.1572e-04 - acc: 0.9333 - val_loss: 5.5972e-12 - val_acc: 0.9907\n",
      "Epoch 172/200\n",
      "25800/25800 [==============================] - 19s 732us/step - loss: 2.4291e-04 - acc: 0.9086 - val_loss: 2.4661e-12 - val_acc: 0.9950\n",
      "Epoch 173/200\n",
      "25800/25800 [==============================] - 19s 740us/step - loss: 1.6220e-04 - acc: 0.9239 - val_loss: 1.2518e-11 - val_acc: 0.9977\n",
      "Epoch 174/200\n",
      "25800/25800 [==============================] - 19s 751us/step - loss: 1.7421e-04 - acc: 0.9288 - val_loss: 6.3510e-09 - val_acc: 0.9965\n",
      "Epoch 175/200\n",
      "25800/25800 [==============================] - 19s 751us/step - loss: 2.4340e-04 - acc: 0.9215 - val_loss: 1.6692e-11 - val_acc: 0.9953\n",
      "Epoch 176/200\n",
      "25800/25800 [==============================] - 19s 751us/step - loss: 1.6645e-04 - acc: 0.9196 - val_loss: 2.1543e-12 - val_acc: 0.9874\n",
      "Epoch 177/200\n",
      "25800/25800 [==============================] - 19s 755us/step - loss: 1.5344e-04 - acc: 0.9285 - val_loss: 5.0070e-08 - val_acc: 0.9969\n",
      "Epoch 178/200\n",
      "25800/25800 [==============================] - 19s 753us/step - loss: 2.1185e-04 - acc: 0.9195 - val_loss: 3.6837e-11 - val_acc: 0.9948\n",
      "Epoch 179/200\n",
      "25800/25800 [==============================] - 19s 754us/step - loss: 1.8872e-04 - acc: 0.9213 - val_loss: 1.1492e-12 - val_acc: 0.9921\n",
      "Epoch 180/200\n",
      "25800/25800 [==============================] - 19s 754us/step - loss: 1.2292e-04 - acc: 0.9212 - val_loss: 2.3338e-11 - val_acc: 0.9986\n",
      "Epoch 181/200\n",
      "25800/25800 [==============================] - 19s 753us/step - loss: 2.0740e-04 - acc: 0.9228 - val_loss: 8.3540e-07 - val_acc: 0.9963\n",
      "Epoch 182/200\n",
      "25800/25800 [==============================] - 19s 751us/step - loss: 2.1084e-04 - acc: 0.9233 - val_loss: 4.5625e-12 - val_acc: 0.9948\n",
      "Epoch 183/200\n",
      "25800/25800 [==============================] - 20s 756us/step - loss: 1.3141e-04 - acc: 0.9308 - val_loss: 6.2650e-10 - val_acc: 0.9915\n",
      "Epoch 184/200\n",
      "25800/25800 [==============================] - 19s 721us/step - loss: 1.9220e-04 - acc: 0.9300 - val_loss: 9.7572e-13 - val_acc: 0.9971\n",
      "Epoch 185/200\n",
      "25800/25800 [==============================] - 19s 745us/step - loss: 1.4669e-04 - acc: 0.9217 - val_loss: 9.6015e-13 - val_acc: 0.9994\n",
      "Epoch 186/200\n",
      "25800/25800 [==============================] - 19s 752us/step - loss: 1.9912e-04 - acc: 0.9267 - val_loss: 2.3142e-06 - val_acc: 0.9967\n",
      "Epoch 187/200\n",
      "25800/25800 [==============================] - 19s 747us/step - loss: 1.7866e-04 - acc: 0.9216 - val_loss: 3.8386e-06 - val_acc: 0.9543\n",
      "Epoch 188/200\n",
      "25800/25800 [==============================] - 19s 750us/step - loss: 1.5960e-04 - acc: 0.9197 - val_loss: 1.9169e-09 - val_acc: 0.9983\n",
      "Epoch 189/200\n",
      "25800/25800 [==============================] - 19s 754us/step - loss: 1.1432e-04 - acc: 0.9281 - val_loss: 4.8498e-09 - val_acc: 0.9996\n",
      "Epoch 190/200\n",
      "25800/25800 [==============================] - 19s 752us/step - loss: 1.3289e-04 - acc: 0.9303 - val_loss: 6.1913e-13 - val_acc: 0.9981\n",
      "Epoch 191/200\n",
      "25800/25800 [==============================] - 19s 754us/step - loss: 1.8383e-04 - acc: 0.9293 - val_loss: 5.1438e-07 - val_acc: 0.9901\n",
      "Epoch 192/200\n",
      "25800/25800 [==============================] - 19s 754us/step - loss: 2.4304e-04 - acc: 0.9290 - val_loss: 2.2149e-06 - val_acc: 0.9983\n",
      "Epoch 193/200\n",
      "25800/25800 [==============================] - 19s 752us/step - loss: 2.6838e-04 - acc: 0.9315 - val_loss: 5.0714e-13 - val_acc: 0.9983\n",
      "Epoch 194/200\n",
      "25800/25800 [==============================] - 19s 751us/step - loss: 1.4130e-04 - acc: 0.9297 - val_loss: 6.2510e-11 - val_acc: 0.9952\n",
      "Epoch 195/200\n",
      "25800/25800 [==============================] - 19s 753us/step - loss: 1.5985e-04 - acc: 0.9374 - val_loss: 1.4120e-10 - val_acc: 0.9992\n",
      "Epoch 196/200\n",
      "25800/25800 [==============================] - 19s 752us/step - loss: 8.9817e-05 - acc: 0.9374 - val_loss: 8.0055e-06 - val_acc: 0.9994\n",
      "Epoch 197/200\n",
      "25800/25800 [==============================] - 19s 746us/step - loss: 1.3252e-04 - acc: 0.9355 - val_loss: 1.7100e-13 - val_acc: 0.9963\n",
      "Epoch 198/200\n",
      "25800/25800 [==============================] - 19s 755us/step - loss: 2.5549e-04 - acc: 0.9285 - val_loss: 2.3993e-09 - val_acc: 0.9983\n",
      "Epoch 199/200\n",
      "25800/25800 [==============================] - 19s 753us/step - loss: 2.6969e-04 - acc: 0.9327 - val_loss: 7.3511e-10 - val_acc: 0.9994\n",
      "Epoch 200/200\n",
      "25800/25800 [==============================] - 19s 751us/step - loss: 1.6798e-04 - acc: 0.9475 - val_loss: 9.3589e-13 - val_acc: 0.9994\n"
     ]
    }
   ],
   "source": [
    "classifier_E = build_model_E()\n",
    "hist_E = classifier_E.fit(x_train_ori, y_train_ori, batch_size=64, nb_epoch=200, verbose=1,\n",
    "                          validation_data=(x_test_ori, y_test_ori))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_yaml = classifier_E.to_yaml()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.yaml', 'w') as yaml_file:\n",
    "    yaml_file.write(model_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, model_path, weight_path):\n",
    "    yaml_file = open(model_path, 'r')\n",
    "    loaded = yaml_file.read()\n",
    "    yaml_file.close()\n",
    "    classifier = keras.models.model_from_yaml(loaded)\n",
    "    classifier.load_weights(weight_path)\n",
    "    result = classifier.predict(data)[0].tolist()\n",
    "    output = 'C'\n",
    "    possible = 'possible notes: '\n",
    "    for i in range(len(result)):\n",
    "        if result[i] < 0.01:\n",
    "            result[i] = 0\n",
    "        elif result[i] > 0.99:\n",
    "            result[i] = 1\n",
    "            output += '_{}'.format(i+60)\n",
    "        else:\n",
    "            possible += '_{}__{}'.format(i+60, result[i])\n",
    "    print(output)\n",
    "    print(possible)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(path + 'database_{}.csv'.format(432), dtype=str)\n",
    "array = data.values\n",
    "array = list(array)\n",
    "sample = array[34][1:11665].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for x in sample:\n",
    "    if type(x) == str:\n",
    "        x = \"\".join(x.split())\n",
    "        data.append(float(x))\n",
    "    elif type(x) == float:\n",
    "        data.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)\n",
    "data = data.reshape(1, 108, 108, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_67_70_78_82\n",
      "possible notes: \n"
     ]
    }
   ],
   "source": [
    "result = predict(data, 'model.yaml', \"weight-25800-CNN_E_good.hdf5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
