{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import gc\n",
    "import keras \n",
    "from keras.layers import LSTM, Dense, TimeDistributed, Dropout, MaxPooling2D, Conv2D, Input\n",
    "from keras.layers import Flatten, BatchNormalization, Activation, Reshape, concatenate\n",
    "from keras.models import Model\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "import keras.backend as K\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 20\n",
    "height = 108\n",
    "width = 108"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the path to where you save the input numpy arrays\n",
    "path = 'E:/notes_database/75_overlap/'\n",
    "lstm = np.array(glob.glob(path+'lstm_inputs/*'))\n",
    "cnn = np.array(glob.glob(path+'cnn_inputs/*'))\n",
    "y_train = np.array(glob.glob(path+'y_train/*'))\n",
    "c_lstm = np.array(glob.glob(path+'counter_lstm_inputs/*'))\n",
    "c_cnn = np.array(glob.glob(path+'counter_cnn_inputs/*'))\n",
    "c_y_train = np.array(glob.glob(path+'counter_y_train/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The function analyzes the actual accuracy\n",
    "def load_test_score(model, cnn_test, lstm_test, y_test):\n",
    "    prediction = model.predict([cnn_test, lstm_test])\n",
    "    error = (prediction - y_test) ** 2\n",
    "    error = (error/y_test.shape[0]).sum()\n",
    "    wrong = 0\n",
    "    for x in range(prediction.shape[0]):\n",
    "        for y in range(prediction.shape[1]):\n",
    "            if abs(prediction[x][y] - y_test[x][y]) > 0.5:\n",
    "                wrong += 1\n",
    "                break\n",
    "    accuracy = 1 - wrong/(prediction.shape[0])\n",
    "    return error, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inputs_to_model(order, num, start, ending):\n",
    "    cnn_inputs = np.load(cnn[order[num][0]])\n",
    "    labels_inputs = np.load(y_train[order[num][0]])\n",
    "    lstm_inputs = np.load(lstm[order[num][0]])\n",
    "    \n",
    "    lstm_inputs = np.append(lstm_inputs, np.load(c_lstm[order[num][0]]), axis=0)\n",
    "    cnn_inputs = np.append(cnn_inputs, np.load(c_cnn[order[num][0]]), axis=0)\n",
    "    labels_inputs = np.append(labels_inputs, np.load(c_y_train[order[num][0]]), axis=0)\n",
    "    \n",
    "    for file in order[num][start+1:ending]:\n",
    "        \n",
    "        lstm_inputs = np.append(lstm_inputs, np.load(lstm[file]), axis=0)\n",
    "        cnn_inputs = np.append(cnn_inputs, np.load(cnn[file]), axis=0)\n",
    "        labels_inputs = np.append(labels_inputs, np.load(y_train[file]), axis=0)\n",
    "        lstm_inputs = np.append(lstm_inputs, np.load(c_lstm[file]), axis=0)\n",
    "        cnn_inputs = np.append(cnn_inputs, np.load(c_cnn[file]), axis=0)\n",
    "        labels_inputs = np.append(labels_inputs, np.load(c_y_train[file]), axis=0)\n",
    "    \n",
    "    lstm_inputs = lstm_inputs.astype(np.float16)\n",
    "    labels_inputs = labels_inputs.astype(np.float16)\n",
    "    return lstm_inputs, cnn_inputs, labels_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(optimizer='adam', loss='mean_squared_error', metrix='accuracy'):\n",
    "    #cnn\n",
    "    cnn_inputs = Input(shape=(height, width, 1), name='cnn_inputs')\n",
    "    layers = Conv2D(32, (3,3), padding=\"same\", activation=\"relu\")(cnn_inputs)\n",
    "    layers = BatchNormalization()(layers)\n",
    "    layers = MaxPooling2D(2,2, border_mode='same')(layers)\n",
    "    layers = Conv2D(32, (3,3), activation='relu')(layers)\n",
    "    layers = BatchNormalization()(layers)\n",
    "    layers = MaxPooling2D(2,2, border_mode='same')(layers)\n",
    "    layers = Conv2D(64, (3,3), activation='relu')(layers)\n",
    "    layers = BatchNormalization()(layers)\n",
    "    layers = MaxPooling2D(2,2, border_mode='same')(layers)\n",
    "    layers = Conv2D(64, (3,3), activation='relu')(layers)\n",
    "    layers = BatchNormalization()(layers)\n",
    "    layers = MaxPooling2D(2,2, border_mode='same')(layers)\n",
    "    layers = Flatten()(layers)\n",
    "    layers = Dropout(0.3)(layers)\n",
    "    layers = Dense(32, activation='relu')(layers)\n",
    "    \n",
    "    #lstm\n",
    "    lstm_inputs = Input(shape=(steps, 24), name='lstm_inputs')\n",
    "    lstm_layers = LSTM(16, return_sequences=True)(lstm_inputs)\n",
    "    lstm_layers = LSTM(32)(lstm_layers)\n",
    "    lstm_layers = Dense(32, activation='relu')(lstm_layers)\n",
    "    lstm_layers = Dense(24, activation='relu')(lstm_layers)\n",
    "    \n",
    "    #main route\n",
    "    main_ = concatenate([layers, lstm_layers])\n",
    "    main_ = BatchNormalization()(main_)\n",
    "    main_ = Dense(64, activation='relu')(main_)\n",
    "    main_ = Dense(64, activation='relu')(main_)\n",
    "    output = Dense(24, activation='sigmoid')(main_)\n",
    "    \n",
    "    model = Model(inputs=[cnn_inputs, lstm_inputs], outputs=[output])\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[metrix])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_2(optimizer, loss):\n",
    "    #cnn\n",
    "    cnn_inputs = Input(shape=(height, width, 1), name='cnn_inputs')\n",
    "    layers = Conv2D(32, (3,3), padding=\"same\", activation=\"relu\")(cnn_inputs)\n",
    "    layers = BatchNormalization()(layers)\n",
    "    layers = Dropout(0.3)(layers)\n",
    "    layers = MaxPooling2D(2,2, border_mode='same')(layers)\n",
    "    layers = Conv2D(32, (3,3), activation='relu')(layers)\n",
    "    layers = Conv2D(64, (3,3), activation='relu')(layers)\n",
    "    layers = BatchNormalization()(layers)\n",
    "    layers = MaxPooling2D(2,2, border_mode='same')(layers)\n",
    "    layers = Conv2D(64, (3,3), activation='relu')(layers)\n",
    "    layers = Conv2D(32, (3,3), activation='relu')(layers)\n",
    "    layers = BatchNormalization()(layers)\n",
    "    layers = MaxPooling2D(2,2, border_mode='same')(layers)\n",
    "    layers = Conv2D(32, (3,3), activation='relu')(layers)\n",
    "    layers = BatchNormalization()(layers)\n",
    "    layers = Flatten()(layers)\n",
    "    layers = Dense(32, activation='relu')(layers)\n",
    "    \n",
    "    #lstm\n",
    "    lstm_inputs = Input(shape=(steps, 24), name='lstm_inputs')\n",
    "    lstm_layers = LSTM(16, return_sequences=True)(lstm_inputs)\n",
    "    lstm_layers = LSTM(32)(lstm_layers)\n",
    "    lstm_layers = Dense(32, activation='relu')(lstm_layers)\n",
    "    lstm_layers = Dense(24, activation='relu')(lstm_layers)\n",
    "    \n",
    "    #main route\n",
    "    main_ = concatenate([layers, lstm_layers])\n",
    "    main_ = BatchNormalization()(main_)\n",
    "    main_ = Dense(64, activation='relu')(main_)\n",
    "    main_ = Dense(64, activation='relu')(main_)\n",
    "    output = Dense(24, activation='sigmoid')(main_)\n",
    "    \n",
    "    model = Model(inputs=[cnn_inputs, lstm_inputs], outputs=[output])\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_3(optimizer='adam', loss='mean_squared_error', metrix='accuracy'):\n",
    "    #cnn\n",
    "    cnn_inputs = Input(shape=(height, width, 1), name='cnn_inputs')\n",
    "    layers = Conv2D(32, (3,3), padding=\"same\", activation=\"relu\")(cnn_inputs)\n",
    "    layers = BatchNormalization()(layers)\n",
    "    layers = MaxPooling2D(2,2, border_mode='same')(layers)\n",
    "    layers = Conv2D(32, (3,3), activation='relu')(layers)\n",
    "    layers = BatchNormalization()(layers)\n",
    "    layers = MaxPooling2D(2,2, border_mode='same')(layers)\n",
    "    layers = Flatten()(layers)\n",
    "    layers = Dropout(0.3)(layers)\n",
    "    layers = Dense(32, activation='relu')(layers)\n",
    "    \n",
    "    #lstm\n",
    "    lstm_inputs = Input(shape=(steps, 24), name='lstm_inputs')\n",
    "    lstm_layers = LSTM(16, return_sequences=True)(lstm_inputs)\n",
    "    lstm_layers = LSTM(16)(lstm_layers)\n",
    "    lstm_layers = Dense(24, activation='relu')(lstm_layers)\n",
    "    \n",
    "    #main route\n",
    "    main_ = concatenate([layers, lstm_layers])\n",
    "    main_ = BatchNormalization()(main_)\n",
    "    main_ = Dense(64, activation='relu')(main_)\n",
    "    output = Dense(24, activation='sigmoid')(main_)\n",
    "    \n",
    "    model = Model(inputs=[cnn_inputs, lstm_inputs], outputs=[output])\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[metrix])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_whole(classifier, num, name, epochs=100, batchsize=32):\n",
    "    lstm_inputs, cnn_inputs, labels_inputs = inputs_to_model(train_order, num, 0, train_order[num].shape[0]+1)\n",
    "    \n",
    "    classifier.fit([cnn_inputs, lstm_inputs], labels_inputs, epochs=epochs, batch_size=batchsize, verbose=1,\n",
    "                  validation_data = ([cnn_test, lstm_test], labels_test))\n",
    "    \n",
    "    #CV_loss, accuracy = load_test_score(classifier, cnn_test, lstm_test, y_test)\n",
    "    #print('val_loss: ', CV_loss)\n",
    "    #print('val_accuarcy', accuracy)\n",
    "    \n",
    "    classifier.save_weights(name, overwrite=True)   \n",
    "    del classifier\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Generate the CV sets\n",
    "kf = KFold(n_splits=3, random_state=7)\n",
    "\n",
    "train_order = []\n",
    "test_order = []\n",
    "length = np.arange(len(lstm))\n",
    "\n",
    "for x,y in kf.split(length):\n",
    "    train_order.append(length[x])\n",
    "    test_order.append(length[y])\n",
    "train_order = np.array(train_order)\n",
    "test_order = np.array(test_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "lstm_test, cnn_test, labels_test = inputs_to_model(test_order, 0, 0, test_order[0].shape[0]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(2, 2, padding=\"same\")`\n",
      "  \n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(2, 2, padding=\"same\")`\n",
      "  if __name__ == '__main__':\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(2, 2, padding=\"same\")`\n",
      "  if sys.path[0] == '':\n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(2, 2, padding=\"same\")`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "100\n",
      "Train on 145313 samples, validate on 73497 samples\n",
      "Epoch 1/50\n",
      "145313/145313 [==============================] - 264s 2ms/step - loss: 0.0574 - acc: 0.9792 - val_loss: 0.0205 - val_acc: 0.9936\n",
      "Epoch 2/50\n",
      "145313/145313 [==============================] - 259s 2ms/step - loss: 0.0208 - acc: 0.9933 - val_loss: 0.0143 - val_acc: 0.9955\n",
      "Epoch 3/50\n",
      "145313/145313 [==============================] - 254s 2ms/step - loss: 0.0161 - acc: 0.9950 - val_loss: 0.0185 - val_acc: 0.9947\n",
      "Epoch 4/50\n",
      "145313/145313 [==============================] - 259s 2ms/step - loss: 0.0143 - acc: 0.9956 - val_loss: 0.0344 - val_acc: 0.9882\n",
      "Epoch 5/50\n",
      "145313/145313 [==============================] - 259s 2ms/step - loss: 0.0124 - acc: 0.9962 - val_loss: 0.0113 - val_acc: 0.9966\n",
      "Epoch 6/50\n",
      "145313/145313 [==============================] - 258s 2ms/step - loss: 0.0118 - acc: 0.9965 - val_loss: 0.0076 - val_acc: 0.9978\n",
      "Epoch 7/50\n",
      "145313/145313 [==============================] - 255s 2ms/step - loss: 0.0109 - acc: 0.9967 - val_loss: 0.0078 - val_acc: 0.9979\n",
      "Epoch 8/50\n",
      "145313/145313 [==============================] - 252s 2ms/step - loss: 0.0102 - acc: 0.9970 - val_loss: 0.0081 - val_acc: 0.9978\n",
      "Epoch 9/50\n",
      "145313/145313 [==============================] - 252s 2ms/step - loss: 0.0097 - acc: 0.9971 - val_loss: 0.0202 - val_acc: 0.9949\n",
      "Epoch 10/50\n",
      "145313/145313 [==============================] - 252s 2ms/step - loss: 0.0092 - acc: 0.9973 - val_loss: 0.0073 - val_acc: 0.9980\n",
      "Epoch 11/50\n",
      "145313/145313 [==============================] - 252s 2ms/step - loss: 0.0089 - acc: 0.9974 - val_loss: 0.0094 - val_acc: 0.9972\n",
      "Epoch 12/50\n",
      "145313/145313 [==============================] - 252s 2ms/step - loss: 0.0088 - acc: 0.9974 - val_loss: 0.0140 - val_acc: 0.9955\n",
      "Epoch 13/50\n",
      "145313/145313 [==============================] - 252s 2ms/step - loss: 0.0084 - acc: 0.9975 - val_loss: 0.0167 - val_acc: 0.9946\n",
      "Epoch 14/50\n",
      "145313/145313 [==============================] - 252s 2ms/step - loss: 0.0082 - acc: 0.9976 - val_loss: 0.0072 - val_acc: 0.9981\n",
      "Epoch 15/50\n",
      "145313/145313 [==============================] - 252s 2ms/step - loss: 0.0078 - acc: 0.9976 - val_loss: 0.0886 - val_acc: 0.9758\n",
      "Epoch 16/50\n",
      "145313/145313 [==============================] - 259s 2ms/step - loss: 0.0077 - acc: 0.9977 - val_loss: 0.0151 - val_acc: 0.9962\n",
      "Epoch 17/50\n",
      "145313/145313 [==============================] - 260s 2ms/step - loss: 0.0075 - acc: 0.9978 - val_loss: 0.0091 - val_acc: 0.9975\n",
      "Epoch 18/50\n",
      "145313/145313 [==============================] - 260s 2ms/step - loss: 0.0073 - acc: 0.9978 - val_loss: 0.0117 - val_acc: 0.9971\n",
      "Epoch 19/50\n",
      "145313/145313 [==============================] - 256s 2ms/step - loss: 0.0071 - acc: 0.9979 - val_loss: 0.0127 - val_acc: 0.9969\n",
      "Epoch 20/50\n",
      "145313/145313 [==============================] - 253s 2ms/step - loss: 0.0071 - acc: 0.9979 - val_loss: 0.0150 - val_acc: 0.9961\n",
      "Epoch 21/50\n",
      "145313/145313 [==============================] - 253s 2ms/step - loss: 0.0069 - acc: 0.9979 - val_loss: 0.0565 - val_acc: 0.9808\n",
      "Epoch 22/50\n",
      "145313/145313 [==============================] - 254s 2ms/step - loss: 0.0070 - acc: 0.9980 - val_loss: 0.0079 - val_acc: 0.9980\n",
      "Epoch 23/50\n",
      "145313/145313 [==============================] - 253s 2ms/step - loss: 0.0067 - acc: 0.9980 - val_loss: 0.0061 - val_acc: 0.9983\n",
      "Epoch 24/50\n",
      "145313/145313 [==============================] - 252s 2ms/step - loss: 0.0066 - acc: 0.9980 - val_loss: 0.0067 - val_acc: 0.9984\n",
      "Epoch 25/50\n",
      "145313/145313 [==============================] - 253s 2ms/step - loss: 0.0064 - acc: 0.9981 - val_loss: 0.0088 - val_acc: 0.9977\n",
      "Epoch 26/50\n",
      "145313/145313 [==============================] - 266s 2ms/step - loss: 0.0066 - acc: 0.9981 - val_loss: 0.0204 - val_acc: 0.9931\n",
      "Epoch 27/50\n",
      "145313/145313 [==============================] - 273s 2ms/step - loss: 0.0063 - acc: 0.9981 - val_loss: 0.0057 - val_acc: 0.9986\n",
      "Epoch 28/50\n",
      "145313/145313 [==============================] - 271s 2ms/step - loss: 0.0060 - acc: 0.9982 - val_loss: 0.0065 - val_acc: 0.9983\n",
      "Epoch 29/50\n",
      "145313/145313 [==============================] - 271s 2ms/step - loss: 0.0060 - acc: 0.9982 - val_loss: 0.0169 - val_acc: 0.9947\n",
      "Epoch 30/50\n",
      "145313/145313 [==============================] - 272s 2ms/step - loss: 0.0059 - acc: 0.9982 - val_loss: 0.0067 - val_acc: 0.9982\n",
      "Epoch 31/50\n",
      "145313/145313 [==============================] - 276s 2ms/step - loss: 0.0060 - acc: 0.9982 - val_loss: 0.0074 - val_acc: 0.9982\n",
      "Epoch 32/50\n",
      "145313/145313 [==============================] - 270s 2ms/step - loss: 0.0057 - acc: 0.9983 - val_loss: 0.0171 - val_acc: 0.9945\n",
      "Epoch 33/50\n",
      "145313/145313 [==============================] - 268s 2ms/step - loss: 0.0058 - acc: 0.9983 - val_loss: 0.0057 - val_acc: 0.9986\n",
      "Epoch 34/50\n",
      "145313/145313 [==============================] - 267s 2ms/step - loss: 0.0060 - acc: 0.9983 - val_loss: 0.0065 - val_acc: 0.9985\n",
      "Epoch 35/50\n",
      "145313/145313 [==============================] - 265s 2ms/step - loss: 0.0056 - acc: 0.9984 - val_loss: 0.0079 - val_acc: 0.9981\n",
      "Epoch 36/50\n",
      "145313/145313 [==============================] - 273s 2ms/step - loss: 0.0054 - acc: 0.9984 - val_loss: 0.0063 - val_acc: 0.9985\n",
      "Epoch 37/50\n",
      "145313/145313 [==============================] - 267s 2ms/step - loss: 0.0058 - acc: 0.9984 - val_loss: 0.0344 - val_acc: 0.9896\n",
      "Epoch 38/50\n",
      "145313/145313 [==============================] - 267s 2ms/step - loss: 0.0053 - acc: 0.9984 - val_loss: 0.0060 - val_acc: 0.9985\n",
      "Epoch 39/50\n",
      "145313/145313 [==============================] - 266s 2ms/step - loss: 0.0053 - acc: 0.9984 - val_loss: 0.0104 - val_acc: 0.9975\n",
      "Epoch 40/50\n",
      "145313/145313 [==============================] - 270s 2ms/step - loss: 0.0053 - acc: 0.9984 - val_loss: 0.0072 - val_acc: 0.9983\n",
      "Epoch 41/50\n",
      "145313/145313 [==============================] - 286s 2ms/step - loss: 0.0052 - acc: 0.9984 - val_loss: 0.0058 - val_acc: 0.9987\n",
      "Epoch 42/50\n",
      "145313/145313 [==============================] - 285s 2ms/step - loss: 0.0053 - acc: 0.9984 - val_loss: 0.0711 - val_acc: 0.9811\n",
      "Epoch 43/50\n",
      "145313/145313 [==============================] - 281s 2ms/step - loss: 0.0053 - acc: 0.9985 - val_loss: 0.0108 - val_acc: 0.9976\n",
      "Epoch 44/50\n",
      "145313/145313 [==============================] - 275s 2ms/step - loss: 0.0052 - acc: 0.9985 - val_loss: 0.0054 - val_acc: 0.9986\n",
      "Epoch 45/50\n",
      "145313/145313 [==============================] - 273s 2ms/step - loss: 0.0051 - acc: 0.9985 - val_loss: 0.0065 - val_acc: 0.9984\n",
      "Epoch 46/50\n",
      "145313/145313 [==============================] - 274s 2ms/step - loss: 0.0056 - acc: 0.9985 - val_loss: 0.0069 - val_acc: 0.9985\n",
      "Epoch 47/50\n",
      "145313/145313 [==============================] - 269s 2ms/step - loss: 0.0058 - acc: 0.9984 - val_loss: 0.0055 - val_acc: 0.9987\n",
      "Epoch 48/50\n",
      "145313/145313 [==============================] - 272s 2ms/step - loss: 0.0052 - acc: 0.9985 - val_loss: 0.0062 - val_acc: 0.9985\n",
      "Epoch 49/50\n",
      "145313/145313 [==============================] - 277s 2ms/step - loss: 0.0049 - acc: 0.9986 - val_loss: 0.0068 - val_acc: 0.9985\n",
      "Epoch 50/50\n",
      "145313/145313 [==============================] - 273s 2ms/step - loss: 0.0054 - acc: 0.9985 - val_loss: 0.0630 - val_acc: 0.9808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "39738"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam = keras.optimizers.Adam(lr=0.005)\n",
    "classifier = main(optimizer=adam, loss='binary_crossentropy')\n",
    "train_test_whole(classifier, 0, 'weight-75-overlap-model1.hdf5', epochs=50)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(2, 2, padding=\"same\")`\n",
      "  \n",
      "c:\\users\\zheng\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(2, 2, padding=\"same\")`\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "100\n",
      "Train on 145313 samples, validate on 73497 samples\n",
      "Epoch 1/50\n",
      "145313/145313 [==============================] - 260s 2ms/step - loss: 0.0292 - acc: 0.9901 - val_loss: 0.0137 - val_acc: 0.9957\n",
      "Epoch 2/50\n",
      "145313/145313 [==============================] - 258s 2ms/step - loss: 0.0141 - acc: 0.9956 - val_loss: 0.0108 - val_acc: 0.9967\n",
      "Epoch 3/50\n",
      "145313/145313 [==============================] - 257s 2ms/step - loss: 0.0113 - acc: 0.9964 - val_loss: 0.0092 - val_acc: 0.9972\n",
      "Epoch 4/50\n",
      "145313/145313 [==============================] - 255s 2ms/step - loss: 0.0097 - acc: 0.9969 - val_loss: 0.0092 - val_acc: 0.9972\n",
      "Epoch 5/50\n",
      "145313/145313 [==============================] - 256s 2ms/step - loss: 0.0086 - acc: 0.9973 - val_loss: 0.0101 - val_acc: 0.9971\n",
      "Epoch 6/50\n",
      "145313/145313 [==============================] - 255s 2ms/step - loss: 0.0088 - acc: 0.9973 - val_loss: 0.0106 - val_acc: 0.9972\n",
      "Epoch 7/50\n",
      "145313/145313 [==============================] - 252s 2ms/step - loss: 0.0072 - acc: 0.9977 - val_loss: 0.2343 - val_acc: 0.9653\n",
      "Epoch 8/50\n",
      "145313/145313 [==============================] - 254s 2ms/step - loss: 0.0068 - acc: 0.9978 - val_loss: 0.0076 - val_acc: 0.9980\n",
      "Epoch 9/50\n",
      "145313/145313 [==============================] - 257s 2ms/step - loss: 0.0063 - acc: 0.9980 - val_loss: 0.0096 - val_acc: 0.9973\n",
      "Epoch 10/50\n",
      "145313/145313 [==============================] - 256s 2ms/step - loss: 0.0060 - acc: 0.9981 - val_loss: 0.0070 - val_acc: 0.9980\n",
      "Epoch 11/50\n",
      "145313/145313 [==============================] - 257s 2ms/step - loss: 0.0056 - acc: 0.9982 - val_loss: 0.0071 - val_acc: 0.9981\n",
      "Epoch 12/50\n",
      "145313/145313 [==============================] - 258s 2ms/step - loss: 0.0053 - acc: 0.9983 - val_loss: 0.0079 - val_acc: 0.9979\n",
      "Epoch 13/50\n",
      "145313/145313 [==============================] - 260s 2ms/step - loss: 0.0052 - acc: 0.9983 - val_loss: 0.0071 - val_acc: 0.9980\n",
      "Epoch 14/50\n",
      "145313/145313 [==============================] - 258s 2ms/step - loss: 0.0049 - acc: 0.9984 - val_loss: 0.0070 - val_acc: 0.9982\n",
      "Epoch 15/50\n",
      "145313/145313 [==============================] - 251s 2ms/step - loss: 0.0048 - acc: 0.9984 - val_loss: 0.0107 - val_acc: 0.9973s: 0.\n",
      "Epoch 16/50\n",
      "145313/145313 [==============================] - 243s 2ms/step - loss: 0.0047 - acc: 0.9985 - val_loss: 0.0095 - val_acc: 0.9976\n",
      "Epoch 17/50\n",
      "145313/145313 [==============================] - 243s 2ms/step - loss: 0.0047 - acc: 0.9985 - val_loss: 0.0077 - val_acc: 0.9981\n",
      "Epoch 18/50\n",
      "145313/145313 [==============================] - 242s 2ms/step - loss: 0.0043 - acc: 0.9986 - val_loss: 0.0077 - val_acc: 0.9981\n",
      "Epoch 19/50\n",
      "145313/145313 [==============================] - 238s 2ms/step - loss: 0.0042 - acc: 0.9987 - val_loss: 0.0075 - val_acc: 0.9981\n",
      "Epoch 20/50\n",
      "145313/145313 [==============================] - 233s 2ms/step - loss: 0.0049 - acc: 0.9986 - val_loss: 0.0070 - val_acc: 0.9982\n",
      "Epoch 21/50\n",
      "145313/145313 [==============================] - 233s 2ms/step - loss: 0.0042 - acc: 0.9987 - val_loss: 0.0069 - val_acc: 0.9982\n",
      "Epoch 22/50\n",
      "145313/145313 [==============================] - 232s 2ms/step - loss: 0.0040 - acc: 0.9987 - val_loss: 0.0091 - val_acc: 0.9979\n",
      "Epoch 23/50\n",
      "145313/145313 [==============================] - 238s 2ms/step - loss: 0.0038 - acc: 0.9988 - val_loss: 0.0068 - val_acc: 0.9982\n",
      "Epoch 24/50\n",
      "145313/145313 [==============================] - 248s 2ms/step - loss: 0.0038 - acc: 0.9988 - val_loss: 0.0071 - val_acc: 0.9982\n",
      "Epoch 25/50\n",
      "145313/145313 [==============================] - 244s 2ms/step - loss: 0.0036 - acc: 0.9988 - val_loss: 0.0069 - val_acc: 0.9983\n",
      "Epoch 26/50\n",
      "145313/145313 [==============================] - 245s 2ms/step - loss: 0.0035 - acc: 0.9989 - val_loss: 0.0068 - val_acc: 0.9984\n",
      "Epoch 27/50\n",
      "145313/145313 [==============================] - 248s 2ms/step - loss: 0.0035 - acc: 0.9989 - val_loss: 0.0079 - val_acc: 0.9983\n",
      "Epoch 28/50\n",
      "145313/145313 [==============================] - 248s 2ms/step - loss: 0.0034 - acc: 0.9989 - val_loss: 0.0085 - val_acc: 0.9981\n",
      "Epoch 29/50\n",
      "145313/145313 [==============================] - 241s 2ms/step - loss: 0.0032 - acc: 0.9990 - val_loss: 0.0071 - val_acc: 0.9984\n",
      "Epoch 30/50\n",
      "145313/145313 [==============================] - 242s 2ms/step - loss: 0.0033 - acc: 0.9989 - val_loss: 0.0064 - val_acc: 0.9985\n",
      "Epoch 31/50\n",
      "145313/145313 [==============================] - 247s 2ms/step - loss: 0.0032 - acc: 0.9989 - val_loss: 0.0069 - val_acc: 0.9984\n",
      "Epoch 32/50\n",
      "145313/145313 [==============================] - 246s 2ms/step - loss: 0.0031 - acc: 0.9990 - val_loss: 0.0077 - val_acc: 0.9982\n",
      "Epoch 33/50\n",
      "145313/145313 [==============================] - 246s 2ms/step - loss: 0.0031 - acc: 0.9990 - val_loss: 0.0072 - val_acc: 0.9983\n",
      "Epoch 34/50\n",
      "145313/145313 [==============================] - 248s 2ms/step - loss: 0.0030 - acc: 0.9990 - val_loss: 0.0069 - val_acc: 0.9983\n",
      "Epoch 35/50\n",
      "145313/145313 [==============================] - 246s 2ms/step - loss: 0.0032 - acc: 0.9990 - val_loss: 0.0069 - val_acc: 0.9984\n",
      "Epoch 36/50\n",
      "145313/145313 [==============================] - 247s 2ms/step - loss: 0.0030 - acc: 0.9990 - val_loss: 0.0078 - val_acc: 0.9983\n",
      "Epoch 37/50\n",
      "145313/145313 [==============================] - 246s 2ms/step - loss: 0.0029 - acc: 0.9991 - val_loss: 0.0080 - val_acc: 0.9983\n",
      "Epoch 38/50\n",
      "145313/145313 [==============================] - 247s 2ms/step - loss: 0.0029 - acc: 0.9991 - val_loss: 0.0072 - val_acc: 0.9984\n",
      "Epoch 39/50\n",
      "145313/145313 [==============================] - 247s 2ms/step - loss: 0.0029 - acc: 0.9991 - val_loss: 0.0077 - val_acc: 0.9984\n",
      "Epoch 40/50\n",
      "145313/145313 [==============================] - 241s 2ms/step - loss: 0.0028 - acc: 0.9991 - val_loss: 0.0073 - val_acc: 0.9983\n",
      "Epoch 41/50\n",
      "145313/145313 [==============================] - 242s 2ms/step - loss: 0.0028 - acc: 0.9991 - val_loss: 0.0073 - val_acc: 0.9985\n",
      "Epoch 42/50\n",
      "145313/145313 [==============================] - 242s 2ms/step - loss: 0.0029 - acc: 0.9991 - val_loss: 0.0079 - val_acc: 0.9983\n",
      "Epoch 43/50\n",
      "145313/145313 [==============================] - 243s 2ms/step - loss: 0.0028 - acc: 0.9991 - val_loss: 0.0064 - val_acc: 0.9985\n",
      "Epoch 44/50\n",
      "145313/145313 [==============================] - 242s 2ms/step - loss: 0.0028 - acc: 0.9991 - val_loss: 0.0076 - val_acc: 0.9983\n",
      "Epoch 45/50\n",
      "145313/145313 [==============================] - 242s 2ms/step - loss: 0.0028 - acc: 0.9991 - val_loss: 0.0075 - val_acc: 0.9984\n",
      "Epoch 46/50\n",
      "145313/145313 [==============================] - 239s 2ms/step - loss: 0.0027 - acc: 0.9991 - val_loss: 0.0085 - val_acc: 0.9983\n",
      "Epoch 47/50\n",
      "145313/145313 [==============================] - 238s 2ms/step - loss: 0.0027 - acc: 0.9992 - val_loss: 0.0074 - val_acc: 0.9983\n",
      "Epoch 48/50\n",
      "145313/145313 [==============================] - 241s 2ms/step - loss: 0.0025 - acc: 0.9992 - val_loss: 0.0074 - val_acc: 0.9984\n",
      "Epoch 49/50\n",
      "145313/145313 [==============================] - 242s 2ms/step - loss: 0.0026 - acc: 0.9992 - val_loss: 0.0077 - val_acc: 0.9983\n",
      "Epoch 50/50\n",
      "145313/145313 [==============================] - 239s 2ms/step - loss: 0.0026 - acc: 0.9992 - val_loss: 0.0070 - val_acc: 0.9985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26486"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam = keras.optimizers.Adam(lr=0.005)\n",
    "classifier = main_3(optimizer=adam, loss='binary_crossentropy')\n",
    "train_test_whole(classifier, 0, 'weight-75-overlap-model2.hdf5', epochs=50)\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
